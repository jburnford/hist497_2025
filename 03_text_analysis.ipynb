{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# **Notebook 3: Text Analysis for Historians**\n\nWelcome to text analysis! In this notebook, you'll learn to analyze large text collections using modern computational tools. We'll explore how religious discourse has evolved in US Presidential inaugural addresses from 1789 to 2021.\n\n**What you'll learn:**\n- Modern text processing with SpaCy\n- Document comparison and analysis\n- N-gram analysis for phrase patterns\n- Temporal visualization of text trends\n- Professional text analysis workflows\n\n**Why this matters for historians:**\nThese skills let you analyze thousands of documents, track language changes over time, and discover patterns that would be impossible to see manually. You'll be able to ask questions like: \"How has presidential religious language changed since Washington?\"\n\n**Our research question:**\nHow has religious discourse in US Presidential inaugural addresses evolved from 1789 to 2021?",
   "metadata": {
    "id": "bkQKw8LbOaQq"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NbqXigQ9OVTm"
   },
   "outputs": [],
   "source": [
    "# Install and import our modern text analysis libraries\n",
    "# Run this cell once. It needs internet access to download packages/models.\n",
    "!pip install spacy scikit-learn matplotlib seaborn pandas --quiet\n",
    "!python -m spacy download en_core_web_sm --quiet\n",
    "\n",
    "print(\"\ud83d\udcda Installing modern text analysis libraries...\")\n",
    "print(\"\u2705 Installation complete!\")\n",
    "\n",
    "# Import the libraries we'll use\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from collections import Counter\n",
    "import re\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "# Load SpaCy's English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "print(\"\u2705 Libraries imported successfully!\")\n",
    "print(\"\ud83d\udd24 SpaCy: Modern text processing\")\n",
    "print(\"\ud83d\udcca Scikit-learn: Document analysis and comparison\") \n",
    "print(\"\ud83d\udcc8 Matplotlib/Seaborn: Data visualization\")\n",
    "print(\"\ud83d\udc3c Pandas: Data manipulation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1: From Scraped Pages to a Research Corpus\n",
    "\n",
    "In Notebook 2 you learned how to collect historical texts responsibly from the web. Now we'll move from raw HTML to a structured corpus we can analyze.\n",
    "\n",
    "We'll work with the U.S. Presidential Inaugural Address corpus\u2014a curated dataset that mirrors what you could build from your scraped sources. It includes text, year, president, and party so we can focus on analysis techniques.\n"
   ],
   "metadata": {
    "id": "_XVeWO4gOqJX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the inaugural address corpus\n",
    "# This sample mirrors a larger corpus you could assemble from scraped texts\n",
    "\n",
    "def load_inaugural_corpus():\n",
    "    \"\"\"Load US Presidential Inaugural Address corpus with metadata\"\"\"\n",
    "    \n",
    "    # Sample data structure - in a real implementation, this would load from CSV or API\n",
    "    corpus_data = [\n",
    "        {\n",
    "            'Year': 1789, 'President': 'Washington', 'FirstName': 'George', 'Party': 'Nonpartisan',\n",
    "            'text': 'Almighty Being who rules over the universe divine Providence has honored the American people divine Author of every good and perfect gift divine blessing divine guidance religious obligations'\n",
    "        },\n",
    "        {\n",
    "            'Year': 1861, 'President': 'Lincoln', 'FirstName': 'Abraham', 'Party': 'Republican',\n",
    "            'text': 'Almighty has His own purposes divine attributes justice of the Almighty that God gives to both North and South this terrible war religious duty under God'\n",
    "        },\n",
    "        {\n",
    "            'Year': 1933, 'President': 'Roosevelt', 'FirstName': 'Franklin D.', 'Party': 'Democratic',\n",
    "            'text': 'with the help of God nation asks for action under the guidance of Almighty God social justice divine providence blessed with natural resources'\n",
    "        },\n",
    "        {\n",
    "            'Year': 1961, 'President': 'Kennedy', 'FirstName': 'John F.', 'Party': 'Democratic',\n",
    "            'text': 'for God and country God willing responsibility to God and man divine power which has lighted the world God bless America almighty God'\n",
    "        },\n",
    "        {\n",
    "            'Year': 2021, 'President': 'Biden', 'FirstName': 'Joseph R.', 'Party': 'Democratic',\n",
    "            'text': 'may God bless America and may God protect our troops prayer for our country God willing we will overcome God bless you all'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # In practice, you would load the full corpus like this:\n",
    "    # corpus_url = \"https://raw.githubusercontent.com/quanteda/quanteda.corpora/master/data-raw/data_corpus_inaugural.csv\"\n",
    "    # df = pd.read_csv(corpus_url)\n",
    "    \n",
    "    df = pd.DataFrame(corpus_data)\n",
    "    print(f\"\ud83d\udcda Loaded {len(df)} inaugural addresses\")\n",
    "    print(f\"\ud83d\udcc5 Date range: {df['Year'].min()} to {df['Year'].max()}\")\n",
    "    print(f\"\ud83c\udfdb\ufe0f Presidents included: {', '.join(df['President'].tolist())}\")\n",
    "    print(\"\ud83d\udca1 Use your own scraped texts by replacing this loader with your dataset.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the corpus\n",
    "inaugural_df = load_inaugural_corpus()\n",
    "\n",
    "# Display basic information\n",
    "print(f\"\\n\ud83d\udcca Corpus Overview:\")\n",
    "print(inaugural_df[['Year', 'President', 'Party']].to_string(index=False))\n",
    "\n",
    "print(f\"\\n\ud83d\udca1 Note: This is a sample for demonstration. The full corpus contains 59 speeches!\")\n",
    "print(f\"   In practice, you'd load the complete dataset with all presidents.\")\n"
   ],
   "metadata": {
    "id": "DOjD4q2ZPIPa"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2: Modern Text Processing with SpaCy\n",
    "\n",
    "To analyze texts at scale we need clean tokens. SpaCy gives us a robust pipeline for tokenization, part-of-speech tagging, and lemmatization. We'll build a reusable helper that improves on the simple string-splitting from Notebook 2.\n"
   ],
   "metadata": {
    "id": "CcD0XeGhPIvN"
   }
  },
  {
   "cell_type": "code",
   "source": "def process_text_with_spacy(text, remove_stop_words=True, lemmatize=True):\n    \"\"\"\n    Process text using SpaCy for professional text analysis\n    \n    Parameters:\n    - text: Input text string\n    - remove_stop_words: Whether to filter out common words\n    - lemmatize: Whether to convert words to root forms\n    \n    Returns:\n    - List of processed tokens\n    \"\"\"\n    # Process with SpaCy\n    doc = nlp(text)\n    \n    processed_tokens = []\n    \n    for token in doc:\n        # Skip punctuation and whitespace\n        if token.is_punct or token.is_space:\n            continue\n            \n        # Skip stop words if requested\n        if remove_stop_words and token.is_stop:\n            continue\n            \n        # Skip very short tokens\n        if len(token.text) < 2:\n            continue\n            \n        # Use lemma (root form) if requested, otherwise use original text\n        if lemmatize:\n            word = token.lemma_.lower()\n        else:\n            word = token.text.lower()\n            \n        # Only keep alphabetic tokens\n        if word.isalpha():\n            processed_tokens.append(word)\n    \n    return processed_tokens\n\n# Test our function with Washington's sample text\nwashington_text = inaugural_df[inaugural_df['President'] == 'Washington']['text'].iloc[0]\n\nprint(\"\ud83d\udd0d Testing our SpaCy processing function:\")\nprint(\"=\" * 60)\nprint(f\"Original text: {washington_text}\")\nprint()\n\n# Process with different settings\ntokens_basic = process_text_with_spacy(washington_text, remove_stop_words=False, lemmatize=False)\ntokens_full = process_text_with_spacy(washington_text, remove_stop_words=True, lemmatize=True)\n\nprint(f\"Basic processing (no filtering): {len(tokens_basic)} tokens\")\nprint(f\"  {tokens_basic[:10]}...\")\n\nprint(f\"Full processing (stop words removed, lemmatized): {len(tokens_full)} tokens\")\nprint(f\"  {tokens_full[:10]}...\")\n\nprint(f\"\\n\u2705 Notice how SpaCy gives us much cleaner, more meaningful tokens!\")\nprint(f\"   - Removes common words like 'the', 'who', 'has'\")\nprint(f\"   - Converts words to root forms (e.g., 'honored' -> 'honor')\")\nprint(f\"   - Filters out punctuation automatically\")",
   "metadata": {
    "id": "FdKe5pabP13w"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 2b: Explore SpaCy Annotations\n",
    "\n",
    "Let's inspect how SpaCy labels tokens (part of speech, lemma, stop word status). Understanding these attributes will help you design meaningful filters for historical texts.\n"
   ],
   "metadata": {
    "id": "9GqX1zWkPT5A"
   }
  },
  {
   "cell_type": "code",
   "source": "# Demonstrate SpaCy's text processing capabilities\nsample_text = \"Almighty God has blessed America with divine providence and religious freedom\"\n\n# Process the text with SpaCy\ndoc = nlp(sample_text)\n\nprint(\"\ud83d\udd24 SpaCy Text Analysis Demonstration:\")\nprint(\"=\" * 50)\nprint(f\"Original text: {sample_text}\")\nprint()\n\n# Show what SpaCy extracts\nprint(\"\ud83d\udcdd Token Analysis:\")\nfor token in doc:\n    print(f\"  '{token.text}' -> Lemma: '{token.lemma_}', POS: {token.pos_}, Stop: {token.is_stop}\")\n\nprint(f\"\\n\ud83c\udff7\ufe0f Named Entities Found:\")\nfor ent in doc.ents:\n    print(f\"  '{ent.text}' -> {ent.label_} ({spacy.explain(ent.label_)})\")\n\nprint(f\"\\n\ud83d\udca1 Key SpaCy Features:\")\nprint(\"  - Lemmatization: Converts words to root form (blessed -> bless)\")\nprint(\"  - POS tagging: Identifies parts of speech\")\nprint(\"  - Stop word detection: Identifies common words to filter\")\nprint(\"  - Named entity recognition: Finds people, places, organizations\")\nprint(\"  - Much more accurate than simple .split() and .lower() approaches!\")",
   "metadata": {
    "id": "TKRtLBjRPTdZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3: Process the Entire Corpus\n",
    "\n",
    "Now apply the processing helper to every inaugural address. This mirrors taking the cleaned HTML you produced in Notebook 2 and standardizing it for analysis.\n",
    "\n",
    "Feel free to tweak the options (keep stop words, skip lemmatization) and compare the results.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Process all inaugural addresses with SpaCy\n",
    "print(\"\ud83d\udd04 Processing all inaugural addresses with SpaCy...\")\n",
    "\n",
    "# Add a column for processed tokens\n",
    "inaugural_df['processed_tokens'] = inaugural_df['text'].apply(\n",
    "    lambda text: process_text_with_spacy(text, remove_stop_words=True, lemmatize=True)\n",
    ")\n",
    "\n",
    "# Add a column for token count\n",
    "inaugural_df['token_count'] = inaugural_df['processed_tokens'].apply(len)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\n\ud83d\udcca Processing Results:\")\n",
    "print(\"=\" * 60)\n",
    "for idx, row in inaugural_df.iterrows():\n",
    "    print(f\"{row['Year']} {row['President']}: {row['token_count']} processed tokens\")\n",
    "    print(f\"  Sample tokens: {row['processed_tokens'][:8]}...\")\n",
    "    print()\n",
    "\n",
    "print(f\"\u2705 Successfully processed {len(inaugural_df)} inaugural addresses!\")\n",
    "print(f\"\ud83d\udcc8 Total unique vocabulary across all speeches: {len(set([token for tokens in inaugural_df['processed_tokens'] for token in tokens]))}\")\n",
    "\n",
    "# Try experimenting with different settings:\n",
    "# inaugural_df['tokens_no_lemma'] = inaugural_df['text'].apply(\n",
    "#     lambda text: process_text_with_spacy(text, remove_stop_words=True, lemmatize=False)\n",
    "# )\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 4: Build a Religious Vocabulary\n",
    "\n",
    "To study religious discourse we need a transparent list of terms. We'll group vocabulary into themes (divine references, actions, concepts, biblical language) so you can expand or adapt it for your project.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Define religious vocabulary categories\nreligious_vocabulary = {\n    'Divine References': ['god', 'almighty', 'divine', 'lord', 'providence', 'creator', 'heaven', 'holy'],\n    'Religious Actions': ['pray', 'prayer', 'bless', 'blessing', 'worship', 'faith', 'believe'],\n    'Religious Concepts': ['religious', 'sacred', 'holy', 'spiritual', 'righteous', 'moral', 'virtue'],\n    'Biblical/Christian': ['jesus', 'christ', 'christian', 'bible', 'scripture', 'gospel', 'salvation']\n}\n\n# Flatten the vocabulary for easy searching\nall_religious_words = []\nfor category, words in religious_vocabulary.items():\n    all_religious_words.extend(words)\n\nprint(\"\ud83d\ude4f Religious Vocabulary Analysis\")\nprint(\"=\" * 50)\nprint(\"\ud83d\udcd6 Religious word categories:\")\nfor category, words in religious_vocabulary.items():\n    print(f\"  {category}: {', '.join(words)}\")\n\nprint(f\"\\n\ud83d\udcca Total religious terms tracked: {len(all_religious_words)}\")\n\n# Function to count religious words in a text\ndef count_religious_words(tokens, religious_vocab=all_religious_words):\n    \"\"\"Count religious words in processed tokens\"\"\"\n    religious_count = 0\n    found_words = []\n    \n    for token in tokens:\n        if token in religious_vocab:\n            religious_count += 1\n            found_words.append(token)\n    \n    return religious_count, found_words\n\n# Test with Washington's speech\nwashington_tokens = inaugural_df[inaugural_df['President'] == 'Washington']['processed_tokens'].iloc[0]\nrel_count, rel_words = count_religious_words(washington_tokens)\n\nprint(f\"\\n\ud83d\udd0d Test with Washington (1789):\")\nprint(f\"  Religious words found: {rel_count}\")\nprint(f\"  Specific words: {', '.join(set(rel_words))}\")\nprint(f\"  Religious density: {rel_count/len(washington_tokens)*100:.1f}% of all words\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 5: Analyze Religious Discourse Across Speeches\n",
    "\n",
    "With processed tokens and a working vocabulary, we can count religious terms, inspect which words appear in each speech, and compute densities (religious words \u00f7 total words).\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Analyze religious discourse across all speeches\nprint(\"\ud83d\udd0d Analyzing religious discourse across all inaugurals...\")\n\n# Add religious analysis columns\ninaugural_df['religious_count'] = inaugural_df['processed_tokens'].apply(\n    lambda tokens: count_religious_words(tokens)[0]\n)\n\ninaugural_df['religious_words'] = inaugural_df['processed_tokens'].apply(\n    lambda tokens: count_religious_words(tokens)[1]\n)\n\ninaugural_df['religious_density'] = (\n    inaugural_df['religious_count'] / inaugural_df['token_count'] * 100\n).round(1)\n\n# Display results\nprint(f\"\\n\ud83d\udcca Religious Discourse Analysis Results:\")\nprint(\"=\" * 70)\nfor idx, row in inaugural_df.iterrows():\n    print(f\"{row['Year']} {row['President']:<12}: {row['religious_count']:2d} religious words \"\n          f\"({row['religious_density']:4.1f}% density)\")\n    \n    # Show specific religious words found\n    unique_religious = list(set(row['religious_words']))\n    if unique_religious:\n        print(f\"{'':26} Words: {', '.join(unique_religious[:6])}\")\n        if len(unique_religious) > 6:\n            print(f\"{'':26} + {len(unique_religious) - 6} more...\")\n    print()\n\n# Calculate summary statistics\navg_density = inaugural_df['religious_density'].mean()\nmax_religious = inaugural_df.loc[inaugural_df['religious_density'].idxmax()]\nmin_religious = inaugural_df.loc[inaugural_df['religious_density'].idxmin()]\n\nprint(f\"\ud83d\udcc8 Summary Statistics:\")\nprint(f\"  Average religious density: {avg_density:.1f}%\")\nprint(f\"  Highest religious content: {max_religious['President']} ({max_religious['Year']}) - {max_religious['religious_density']:.1f}%\")\nprint(f\"  Lowest religious content: {min_religious['President']} ({min_religious['Year']}) - {min_religious['religious_density']:.1f}%\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 6: N-gram Analysis \u2013 Finding Religious Phrases\n",
    "\n",
    "Individual words are helpful, but phrases like \"God bless America\" reveal richer storytelling. We'll extract n-grams (bi-grams, tri-grams) using scikit-learn's `CountVectorizer`.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# N-gram analysis for religious phrases\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef extract_ngrams(text, n=2):\n    \"\"\"Extract n-grams from text using scikit-learn\"\"\"\n    # Use CountVectorizer to extract n-grams\n    vectorizer = CountVectorizer(\n        ngram_range=(n, n),  # Only n-grams of length n\n        stop_words='english',\n        lowercase=True,\n        token_pattern=r'[a-zA-Z]+',  # Only alphabetic tokens\n        min_df=1  # Minimum document frequency\n    )\n    \n    # Fit and transform the text\n    ngram_matrix = vectorizer.fit_transform([text])\n    \n    # Get the n-grams and their counts\n    feature_names = vectorizer.get_feature_names_out()\n    counts = ngram_matrix.toarray()[0]\n    \n    # Create list of (ngram, count) tuples\n    ngrams_with_counts = list(zip(feature_names, counts))\n    \n    # Sort by count (descending)\n    ngrams_with_counts.sort(key=lambda x: x[1], reverse=True)\n    \n    return ngrams_with_counts\n\ndef find_religious_ngrams(ngrams_list, religious_vocab=all_religious_words):\n    \"\"\"Filter n-grams that contain religious vocabulary\"\"\"\n    religious_ngrams = []\n    \n    for ngram, count in ngrams_list:\n        # Check if any word in the n-gram is religious\n        words_in_ngram = ngram.split()\n        if any(word in religious_vocab for word in words_in_ngram):\n            religious_ngrams.append((ngram, count))\n    \n    return religious_ngrams\n\nprint(\"\ud83d\udcdd N-gram Analysis: Finding Religious Phrases\")\nprint(\"=\" * 60)\n\n# Analyze bigrams (2-word phrases) across all speeches\nprint(\"\ud83d\udd0d Analyzing 2-grams (bigrams)...\")\n\n# Combine all speech texts for corpus-wide analysis\nall_texts = ' '.join(inaugural_df['text'].tolist())\n\n# Extract bigrams\nbigrams = extract_ngrams(all_texts, n=2)\nreligious_bigrams = find_religious_ngrams(bigrams)\n\nprint(f\"\\nTop religious bigrams:\")\nfor bigram, count in religious_bigrams[:10]:\n    print(f\"  '{bigram}': {count} occurrences\")\n\n# Extract trigrams (3-word phrases)\nprint(f\"\\n\ud83d\udd0d Analyzing 3-grams (trigrams)...\")\ntrigrams = extract_ngrams(all_texts, n=3)\nreligious_trigrams = find_religious_ngrams(trigrams)\n\nprint(f\"\\nTop religious trigrams:\")\nfor trigram, count in religious_trigrams[:8]:\n    print(f\"  '{trigram}': {count} occurrences\")\n\nprint(f\"\\n\ud83d\udca1 N-gram insights:\")\nprint(f\"  - Bigrams reveal common religious phrases\")\nprint(f\"  - Trigrams show complete religious expressions\")\nprint(f\"  - Frequency indicates which phrases are most traditional\")\nprint(f\"  - Perfect for tracking phrase evolution over time!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 7: Track Specific Religious Phrases\n",
    "\n",
    "Choose a phrase and follow its usage across presidents. This helps connect linguistic trends to historical moments you care about.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Track specific religious phrases over time\n",
    "def track_phrase_over_time(df, phrase):\n",
    "    \"\"\"Track occurrences of a specific phrase across speeches\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        text_lower = row['text'].lower()\n",
    "        phrase_count = text_lower.count(phrase.lower())\n",
    "        \n",
    "        if phrase_count > 0:\n",
    "            results.append({\n",
    "                'Year': row['Year'],\n",
    "                'President': row['President'],\n",
    "                'Count': phrase_count,\n",
    "                'Context': phrase\n",
    "            })\n",
    "    \n",
    "    if results:\n",
    "        results_df = pd.DataFrame(results)\n",
    "        print(f\"\ud83d\udcc8 Phrase '{phrase}' appears in {len(results_df)} speeches.\")\n",
    "        print(results_df.to_string(index=False))\n",
    "    else:\n",
    "        print(f\"\u2139\ufe0f Phrase '{phrase}' does not appear in the current corpus.\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example: Track references to \"God bless\"\n",
    "example_phrase = \"God bless\"\n",
    "track_phrase_over_time(inaugural_df, example_phrase)\n",
    "\n",
    "print()\n",
    "print(\"\ud83d\udca1 Try it yourself:\")\n",
    "print(\"track_phrase_over_time(inaugural_df, 'divine providence')\")\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 8: Visualize Temporal Trends\n",
    "\n",
    "Charts help us see patterns that can get lost in tables. We'll compare densities over time, look at counts, relate speech length to religious content, and summarize by era.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create visualizations of religious discourse trends\nplt.style.use('default')  # Clean, professional style\nplt.figure(figsize=(12, 8))\n\n# Plot 1: Religious density over time\nplt.subplot(2, 2, 1)\nplt.plot(inaugural_df['Year'], inaugural_df['religious_density'], 'o-', linewidth=2, markersize=6)\nplt.title('Religious Discourse Density Over Time', fontsize=12, fontweight='bold')\nplt.xlabel('Year')\nplt.ylabel('Religious Density (%)')\nplt.grid(True, alpha=0.3)\n\n# Annotate some key points\nfor idx, row in inaugural_df.iterrows():\n    if row['religious_density'] > inaugural_df['religious_density'].mean() + 5:  # High points\n        plt.annotate(f\"{row['President']}\\n{row['religious_density']:.1f}%\", \n                    (row['Year'], row['religious_density']),\n                    xytext=(10, 10), textcoords='offset points',\n                    fontsize=8, ha='left')\n\n# Plot 2: Raw religious word counts\nplt.subplot(2, 2, 2)\nplt.bar(inaugural_df['Year'], inaugural_df['religious_count'], alpha=0.7)\nplt.title('Number of Religious Words per Speech', fontsize=12, fontweight='bold')\nplt.xlabel('Year')\nplt.ylabel('Religious Word Count')\nplt.xticks(rotation=45)\n\n# Plot 3: Religious words vs. speech length\nplt.subplot(2, 2, 3)\nplt.scatter(inaugural_df['token_count'], inaugural_df['religious_count'], \n           s=60, alpha=0.7, c=inaugural_df['Year'], cmap='viridis')\nplt.title('Religious Words vs. Speech Length', fontsize=12, fontweight='bold')\nplt.xlabel('Total Words in Speech')\nplt.ylabel('Religious Words')\nplt.colorbar(label='Year')\n\n# Add trend line\nz = np.polyfit(inaugural_df['token_count'], inaugural_df['religious_count'], 1)\np = np.poly1d(z)\nplt.plot(inaugural_df['token_count'], p(inaugural_df['token_count']), \"r--\", alpha=0.8)\n\n# Plot 4: Historical periods comparison\nplt.subplot(2, 2, 4)\n# Create era categories\ninaugural_df['Era'] = pd.cut(inaugural_df['Year'], \n                            bins=[1780, 1850, 1900, 1950, 2030],\n                            labels=['Early Republic\\n(1789-1850)', 'Civil War Era\\n(1851-1900)', \n                                   'Modern Era\\n(1901-1950)', 'Contemporary\\n(1951-2021)'])\n\nera_avg = inaugural_df.groupby('Era')['religious_density'].mean()\nplt.bar(range(len(era_avg)), era_avg.values, alpha=0.7)\nplt.title('Average Religious Density by Era', fontsize=12, fontweight='bold')\nplt.xlabel('Historical Era')\nplt.ylabel('Average Religious Density (%)')\nplt.xticks(range(len(era_avg)), era_avg.index, rotation=45)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\ud83d\udcca Visualization Insights:\")\nprint(\"=\" * 40)\nprint(\"\ud83d\udcc8 Top plot: Shows religious density trends over time\")\nprint(\"\ud83d\udcca Bar chart: Raw counts of religious words per speech\")\nprint(\"\ud83d\udd0d Scatter plot: Relationship between speech length and religious content\")\nprint(\"\ud83d\udcc5 Bottom plot: Comparison across historical eras\")\nprint(f\"\\n\ud83d\udca1 Key patterns to notice:\")\nprint(f\"  - Do religious references increase or decrease over time?\")\nprint(f\"  - Are longer speeches more religious?\")\nprint(f\"  - Which historical eras had the most religious rhetoric?\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 9: Final Challenge \u2013 Comparative Historical Analysis\n",
    "\n",
    "Bring everything together. Pose a research question, run targeted analyses, and interpret what the numbers suggest about religious discourse in presidential inaugurals.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Final Challenge: Your Historical Research Project\n\nprint(\"\ud83d\udd2c Final Challenge: Comparative Historical Analysis\")\nprint(\"=\" * 60)\n\n# Research question suggestions\nresearch_questions = [\n    \"How does religious language differ between Republican and Democratic presidents?\",\n    \"Do crisis periods (wars, depressions) correlate with increased religious rhetoric?\",\n    \"Which religious concepts (divine, God, blessing) are most common across eras?\",\n    \"How has the formality of religious language changed over time?\",\n    \"Do longer inaugurals contain proportionally more religious content?\"\n]\n\nprint(\"\ud83c\udfaf Suggested Research Questions:\")\nfor i, question in enumerate(research_questions, 1):\n    print(f\"  {i}. {question}\")\n\nprint(f\"\\n\ud83d\udccb Your Task:\")\nprint(\"1. Choose a research question (or create your own)\")\nprint(\"2. Use the analysis techniques from this notebook\")\nprint(\"3. Create visualizations to support your findings\")\nprint(\"4. Write a brief historical interpretation\")\n\n# Example analysis: Party comparison\nprint(f\"\\n\ud83d\udcca Example Analysis: Religious Language by Political Party\")\nprint(\"=\" * 50)\n\n# Filter for speeches with party data (excluding Washington who was nonpartisan)\nparty_data = inaugural_df[inaugural_df['Party'] != 'Nonpartisan'].copy()\n\nif len(party_data) > 0:\n    party_comparison = party_data.groupby('Party').agg({\n        'religious_density': ['mean', 'std', 'count'],\n        'religious_count': 'mean'\n    }).round(2)\n    \n    print(\"Religious density by party:\")\n    print(party_comparison)\n    \n    # Simple party comparison visualization\n    plt.figure(figsize=(10, 6))\n    \n    # Box plot comparing parties\n    plt.subplot(1, 2, 1)\n    party_groups = [group['religious_density'].values for name, group in party_data.groupby('Party')]\n    party_names = list(party_data.groupby('Party').groups.keys())\n    \n    plt.boxplot(party_groups, labels=party_names)\n    plt.title('Religious Density Distribution by Party')\n    plt.ylabel('Religious Density (%)')\n    plt.xticks(rotation=45)\n    \n    # Time series by party\n    plt.subplot(1, 2, 2)\n    for party in party_names:\n        party_subset = party_data[party_data['Party'] == party]\n        plt.plot(party_subset['Year'], party_subset['religious_density'], \n                'o-', label=party, alpha=0.7)\n    \n    plt.title('Religious Density Over Time by Party')\n    plt.xlabel('Year')\n    plt.ylabel('Religious Density (%)')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n\n# Your turn - add your own analysis here!\nprint(f\"\\n\ud83d\ude80 Your Analysis Space:\")\nprint(\"=\" * 30)\nprint(\"# Customize this section for your research question\")\nprint(\"# Use the functions and techniques from earlier cells\")\nprint(\"# Examples:\")\nprint(\"# - Compare different time periods\")\nprint(\"# - Analyze specific religious themes\")\nprint(\"# - Track evolution of particular phrases\")\nprint(\"# - Examine correlation with historical events\")\n\n# Template for student analysis\nyour_research_question = \"Your research question here\"\nprint(f\"\\n\ud83d\udcdd Research Question: {your_research_question}\")\n\n# Add your analysis code here:\n# your_data = inaugural_df[some_filter]\n# your_results = some_analysis(your_data)  \n# create_your_visualization(your_results)\n\nprint(f\"\\n\ud83d\udcda Research Findings:\")\nprint(\"1. [Your first finding]\")\nprint(\"2. [Your second finding]\") \nprint(\"3. [Your interpretation of the historical significance]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summary: Your Text Analysis Toolkit\n",
    "\n",
    "\ud83c\udf89 **Great work!** You now have an end-to-end workflow for historical text analysis: clean your corpus, design vocabularies, count and compare language, mine phrases, and visualize trends.\n",
    "\n",
    "**Skills unlocked:**\n",
    "- \u2705 Structured corpora from scraped sources\n",
    "- \u2705 Reusable SpaCy preprocessing pipeline\n",
    "- \u2705 Targeted vocabulary design and counting\n",
    "- \u2705 Phrase mining with scikit-learn n-grams\n",
    "- \u2705 Temporal visualizations for interpretation\n",
    "\n",
    "Keep iterating on these notebooks as you expand your own historical datasets.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## \ud83c\udf41 Bonus Challenge: Building a Canadian Corpus\n",
    "\n",
    "Ready for a stretch goal? Design a Canadian Throne Speech corpus and run the same analyses. Use the scrape \u2192 clean \u2192 analyze pipeline you've built in Notebooks 2 and 3.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Bonus Challenge: Planning a Canadian Throne Speech Corpus\n\nprint(\"\ud83c\udf41 Bonus Project: Canadian Throne Speech Corpus\")\nprint(\"=\" * 60)\n\nprint(\"\ud83c\udfaf Project Goal:\")\nprint(\"Create a corpus of Canadian Throne Speeches to compare with US Presidential inaugurals\")\n\nprint(f\"\\n\ud83d\udcda Data Sources to Explore:\")\nsources = [\n    {\n        'name': 'Poltext Canadian Throne Speeches',\n        'url': 'https://www.poltext.org/en/part-1-electronic-political-texts/canadian-throne-speeches',\n        'description': 'Academic corpus with structured data',\n        'advantages': ['Professional curation', 'Standardized format', 'Metadata included'],\n        'approach': 'Download CSV/XML files, parse with pandas'\n    },\n    {\n        'name': 'Parliament of Canada Archives',\n        'url': 'https://www.parl.ca/DocumentViewer/en/house/sitting-hansard',\n        'description': 'Official government transcripts',\n        'advantages': ['Authoritative source', 'Complete coverage', 'Multiple formats'],\n        'approach': 'Web scraping with Beautiful Soup + requests'\n    },\n    {\n        'name': 'Library and Archives Canada',\n        'url': 'https://www.bac-lac.gc.ca/',\n        'description': 'National archives with digitized documents',\n        'advantages': ['Historical depth', 'Original documents', 'Rich metadata'],\n        'approach': 'API access or Internet Archive integration'\n    }\n]\n\nfor i, source in enumerate(sources, 1):\n    print(f\"\\n{i}. {source['name']}\")\n    print(f\"   URL: {source['url']}\")\n    print(f\"   Description: {source['description']}\")\n    print(f\"   Advantages: {', '.join(source['advantages'])}\")\n    print(f\"   Technical approach: {source['approach']}\")\n\nprint(f\"\\n\ud83d\udd27 Technical Implementation Plan:\")\nimplementation_steps = [\n    \"1. Data Collection: Use web scraping or API to gather throne speeches\",\n    \"2. Data Cleaning: Extract text, dates, and metadata using SpaCy\",\n    \"3. Corpus Structure: Create pandas DataFrame similar to inaugural corpus\", \n    \"4. Analysis Pipeline: Apply same religious discourse analysis techniques\",\n    \"5. Comparative Study: Compare Canadian vs. US religious political rhetoric\",\n    \"6. Visualization: Create charts showing differences and similarities\",\n    \"7. Historical Context: Interpret findings in light of different political systems\"\n]\n\nfor step in implementation_steps:\n    print(f\"  {step}\")\n\nprint(f\"\\n\ud83d\udd0d Research Questions for Canadian Analysis:\")\ncanadian_questions = [\n    \"How does religious language in Throne Speeches compare to US inaugurals?\",\n    \"Do Canadian speeches show different temporal patterns?\",\n    \"How do different Prime Ministers vary in religious rhetoric?\",\n    \"Does the Westminster system influence religious language differently?\",\n    \"How do Quebec/French Canadian influences affect religious discourse?\"\n]\n\nfor i, question in enumerate(canadian_questions, 1):\n    print(f\"  {i}. {question}\")\n\nprint(f\"\\n\ud83d\udcbb Code Template for Canadian Corpus:\")\nprint(\"=\" * 40)\n\n# Template code structure\ntemplate_code = '''\n# Step 1: Data collection function\ndef collect_throne_speeches():\n    \"\"\"Collect Canadian throne speeches from online sources\"\"\"\n    # Your web scraping or API code here\n    pass\n\n# Step 2: Process Canadian texts  \ndef process_canadian_text(text):\n    \"\"\"Process Canadian political text with SpaCy\"\"\"\n    # Apply same processing as US inaugurals\n    # Consider bilingual content (English/French)\n    pass\n\n# Step 3: Comparative analysis\ndef compare_us_canada_discourse(us_data, canadian_data):\n    \"\"\"Compare religious discourse between countries\"\"\"\n    # Statistical comparison\n    # Visualization of differences\n    # Historical interpretation\n    pass\n\n# Step 4: Bilingual analysis (advanced)\ndef analyze_french_english_differences():\n    \"\"\"Analyze differences between French and English throne speeches\"\"\"\n    # Requires French SpaCy model: python -m spacy download fr_core_news_sm\n    pass\n'''\n\nprint(template_code)\n\nprint(f\"\\n\ud83d\ude80 Next Steps for Ambitious Students:\")\nnext_steps = [\n    \"1. Choose a data source and examine its structure\",\n    \"2. Write a simple web scraper or data downloader\", \n    \"3. Adapt the US inaugural analysis code for Canadian data\",\n    \"4. Create comparative visualizations\",\n    \"5. Write up findings as a research paper or blog post\",\n    \"6. Share your corpus with other digital historians!\"\n]\n\nfor step in next_steps:\n    print(f\"  {step}\")\n\nprint(f\"\\n\ud83d\udca1 This project combines:\")\nprint(\"  \u2705 All the web scraping skills from Notebook 2\")\nprint(\"  \u2705 All the text analysis skills from Notebook 3\") \nprint(\"  \u2705 Original historical research\")\nprint(\"  \u2705 Cross-national comparative analysis\")\nprint(\"  \ud83c\udde8\ud83c\udde6 Contributing to Canadian digital humanities!\")\n\nprint(f\"\\n\ud83d\udce7 If you build this corpus, consider sharing it with:\")\nprint(\"  - Canadian political science researchers\")\nprint(\"  - Digital humanities communities\") \nprint(\"  - The Programming Historian\")\nprint(\"  - Government of Canada open data initiatives\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}