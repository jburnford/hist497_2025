{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# **Notebook 3: Text Analysis for Historians**\n\nWelcome to text analysis! In this notebook, you'll learn to analyze large text collections using modern computational tools. We'll explore how religious discourse has evolved in US Presidential inaugural addresses from 1789 to 2021.\n\n**What you'll learn:**\n- Modern text processing with SpaCy\n- Document comparison and analysis\n- N-gram analysis for phrase patterns\n- Temporal visualization of text trends\n- Professional text analysis workflows\n\n**Why this matters for historians:**\nThese skills let you analyze thousands of documents, track language changes over time, and discover patterns that would be impossible to see manually. You'll be able to ask questions like: \"How has presidential religious language changed since Washington?\"\n\n**Our research question:**\nHow has religious discourse in US Presidential inaugural addresses evolved from 1789 to 2021?",
   "metadata": {
    "id": "bkQKw8LbOaQq"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NbqXigQ9OVTm"
   },
   "outputs": [],
   "source": "# Install and import our modern text analysis libraries\n!pip install spacy scikit-learn matplotlib seaborn pandas --quiet\n!python -m spacy download en_core_web_sm --quiet\n\nprint(\"üìö Installing modern text analysis libraries...\")\nprint(\"‚úÖ Installation complete!\")\n\n# Import the libraries we'll use\nimport spacy\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom collections import Counter\nimport re\nimport requests\nfrom io import StringIO\n\n# Load SpaCy's English model\nnlp = spacy.load(\"en_core_web_sm\")\n\nprint(\"‚úÖ Libraries imported successfully!\")\nprint(\"üî§ SpaCy: Modern text processing\")\nprint(\"üìä Scikit-learn: Document analysis and comparison\") \nprint(\"üìà Matplotlib/Seaborn: Data visualization\")\nprint(\"üêº Pandas: Data manipulation\")"
  },
  {
   "cell_type": "markdown",
   "source": "## Step 1: Loading the Presidential Inaugural Corpus\n\nWe'll use the US Presidential Inaugural Address corpus, which contains 59 speeches from Washington (1789) to Biden (2021). This is perfect for analyzing how language has changed over more than two centuries.\n\n**Step 1a: Download the corpus data**\n\nFirst, let's get the inaugural address corpus. We'll load it from a reliable source that includes both the text and metadata.",
   "metadata": {
    "id": "_XVeWO4gOqJX"
   }
  },
  {
   "cell_type": "code",
   "source": "# Load the inaugural address corpus\n# We'll create a sample dataset that matches the quanteda corpus structure\n\ndef load_inaugural_corpus():\n    \"\"\"Load US Presidential Inaugural Address corpus with metadata\"\"\"\n    \n    # Sample data structure - in a real implementation, this would load from CSV or API\n    # This represents the key speeches across different eras for demonstration\n    corpus_data = [\n        {\n            'Year': 1789, 'President': 'Washington', 'FirstName': 'George', 'Party': 'Nonpartisan',\n            'text': 'Almighty Being who rules over the universe divine Providence has honored the American people divine Author of every good and perfect gift divine blessing divine guidance religious obligations'\n        },\n        {\n            'Year': 1861, 'President': 'Lincoln', 'FirstName': 'Abraham', 'Party': 'Republican',\n            'text': 'Almighty has His own purposes divine attributes justice of the Almighty that God gives to both North and South this terrible war religious duty under God'\n        },\n        {\n            'Year': 1933, 'President': 'Roosevelt', 'FirstName': 'Franklin D.', 'Party': 'Democratic',\n            'text': 'with the help of God nation asks for action under the guidance of Almighty God social justice divine providence blessed with natural resources'\n        },\n        {\n            'Year': 1961, 'President': 'Kennedy', 'FirstName': 'John F.', 'Party': 'Democratic',\n            'text': 'for God and country God willing responsibility to God and man divine power which has lighted the world God bless America almighty God'\n        },\n        {\n            'Year': 2021, 'President': 'Biden', 'FirstName': 'Joseph R.', 'Party': 'Democratic',\n            'text': 'may God bless America and may God protect our troops prayer for our country God willing we will overcome God bless you all'\n        }\n    ]\n    \n    # In practice, you would load the full corpus like this:\n    # corpus_url = \"https://raw.githubusercontent.com/quanteda/quanteda.corpora/master/data-raw/data_corpus_inaugural.csv\"\n    # df = pd.read_csv(corpus_url)\n    \n    # For this demo, we'll use our sample data\n    df = pd.DataFrame(corpus_data)\n    print(f\"üìö Loaded {len(df)} inaugural addresses\")\n    print(f\"üìÖ Date range: {df['Year'].min()} to {df['Year'].max()}\")\n    print(f\"üèõÔ∏è Presidents included: {', '.join(df['President'].tolist())}\")\n    \n    return df\n\n# Load the corpus\ninaugural_df = load_inaugural_corpus()\n\n# Display basic information\nprint(f\"\\nüìä Corpus Overview:\")\nprint(inaugural_df[['Year', 'President', 'Party']].to_string(index=False))\n\nprint(f\"\\nüí° Note: This is a sample for demonstration. The full corpus contains 59 speeches!\")\nprint(f\"   In practice, you'd load the complete dataset with all presidents.\")",
   "metadata": {
    "id": "DOjD4q2ZPIPa"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 2: Modern Text Processing with SpaCy\n\nSpaCy is the leading library for natural language processing in 2025. It provides industrial-strength text processing that's much more sophisticated than simple word splitting.\n\n**Step 2a: Understanding SpaCy's capabilities**\n\nLet's see what SpaCy can tell us about a presidential text:",
   "metadata": {
    "id": "CcD0XeGhPIvN"
   }
  },
  {
   "cell_type": "code",
   "source": "# Demonstrate SpaCy's text processing capabilities\nsample_text = \"Almighty God has blessed America with divine providence and religious freedom\"\n\n# Process the text with SpaCy\ndoc = nlp(sample_text)\n\nprint(\"üî§ SpaCy Text Analysis Demonstration:\")\nprint(\"=\" * 50)\nprint(f\"Original text: {sample_text}\")\nprint()\n\n# Show what SpaCy extracts\nprint(\"üìù Token Analysis:\")\nfor token in doc:\n    print(f\"  '{token.text}' -> Lemma: '{token.lemma_}', POS: {token.pos_}, Stop: {token.is_stop}\")\n\nprint(f\"\\nüè∑Ô∏è Named Entities Found:\")\nfor ent in doc.ents:\n    print(f\"  '{ent.text}' -> {ent.label_} ({spacy.explain(ent.label_)})\")\n\nprint(f\"\\nüí° Key SpaCy Features:\")\nprint(\"  - Lemmatization: Converts words to root form (blessed -> bless)\")\nprint(\"  - POS tagging: Identifies parts of speech\")\nprint(\"  - Stop word detection: Identifies common words to filter\")\nprint(\"  - Named entity recognition: Finds people, places, organizations\")\nprint(\"  - Much more accurate than simple .split() and .lower() approaches!\")",
   "metadata": {
    "id": "TKRtLBjRPTdZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**Step 2b: Create a text processing function**\n\nNow let's create a professional text processing function using SpaCy:",
   "metadata": {
    "id": "9GqX1zWkPT5A"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Summary: Your Text Analysis Toolkit\n\nüéâ **Congratulations!** You've mastered modern computational text analysis for historical research. You now have professional-level skills that can handle thousands of documents and reveal patterns invisible to traditional methods.\n\n**Technical Skills Mastered:**\n- ‚úÖ **SpaCy processing**: Industrial-strength text preprocessing\n- ‚úÖ **Scikit-learn analysis**: Document comparison and n-gram extraction  \n- ‚úÖ **Data visualization**: Temporal trend analysis with matplotlib/seaborn\n- ‚úÖ **Corpus analysis**: Large-scale text collection processing\n- ‚úÖ **Statistical analysis**: Quantitative historical research methods\n\n**Historical Research Skills:**\n- ‚úÖ **Targeted vocabulary analysis**: Tracking specific themes over time\n- ‚úÖ **N-gram analysis**: Finding phrase patterns and evolution\n- ‚úÖ **Temporal analysis**: Understanding how language changes across periods\n- ‚úÖ **Comparative methods**: Analyzing differences between groups/eras\n- ‚úÖ **Quantitative interpretation**: Drawing historical conclusions from data\n\n**Research Applications:**\n- ‚úÖ **Religious discourse analysis**: Ready methodology for similar studies\n- ‚úÖ **Political rhetoric evolution**: Framework for analyzing political language\n- ‚úÖ **Cross-temporal comparison**: Skills for studying long-term changes\n- ‚úÖ **Cross-national analysis**: Foundation for comparative historical studies\n\n**Next Steps for Advanced Research:**\n1. **Scale up**: Apply these methods to larger corpora (thousands of documents)\n2. **Specialize**: Focus on specific historical themes (nationalism, democracy, etc.)\n3. **Compare**: Build comparative studies across countries or institutions\n4. **Innovate**: Develop new metrics and visualizations for historical questions\n5. **Publish**: Share your findings with digital humanities communities\n\n**Key Takeaway:**\nYou've learned to ask questions that would be impossible without computational methods: \"How has religious language evolved across 250+ years of American political rhetoric?\" These skills let you discover patterns, test hypotheses, and generate insights that transform historical understanding.\n\n**The Digital Historian's Advantage:**\n- Process vast amounts of text systematically\n- Identify subtle patterns across long time periods  \n- Quantify changes that seem intuitive but need proof\n- Compare multiple dimensions simultaneously\n- Generate reproducible, evidence-based conclusions\n\nYou're now equipped to tackle original digital history research projects!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Bonus Challenge: Planning a Canadian Throne Speech Corpus\n\nprint(\"üçÅ Bonus Project: Canadian Throne Speech Corpus\")\nprint(\"=\" * 60)\n\nprint(\"üéØ Project Goal:\")\nprint(\"Create a corpus of Canadian Throne Speeches to compare with US Presidential inaugurals\")\n\nprint(f\"\\nüìö Data Sources to Explore:\")\nsources = [\n    {\n        'name': 'Poltext Canadian Throne Speeches',\n        'url': 'https://www.poltext.org/en/part-1-electronic-political-texts/canadian-throne-speeches',\n        'description': 'Academic corpus with structured data',\n        'advantages': ['Professional curation', 'Standardized format', 'Metadata included'],\n        'approach': 'Download CSV/XML files, parse with pandas'\n    },\n    {\n        'name': 'Parliament of Canada Archives',\n        'url': 'https://www.parl.ca/DocumentViewer/en/house/sitting-hansard',\n        'description': 'Official government transcripts',\n        'advantages': ['Authoritative source', 'Complete coverage', 'Multiple formats'],\n        'approach': 'Web scraping with Beautiful Soup + requests'\n    },\n    {\n        'name': 'Library and Archives Canada',\n        'url': 'https://www.bac-lac.gc.ca/',\n        'description': 'National archives with digitized documents',\n        'advantages': ['Historical depth', 'Original documents', 'Rich metadata'],\n        'approach': 'API access or Internet Archive integration'\n    }\n]\n\nfor i, source in enumerate(sources, 1):\n    print(f\"\\n{i}. {source['name']}\")\n    print(f\"   URL: {source['url']}\")\n    print(f\"   Description: {source['description']}\")\n    print(f\"   Advantages: {', '.join(source['advantages'])}\")\n    print(f\"   Technical approach: {source['approach']}\")\n\nprint(f\"\\nüîß Technical Implementation Plan:\")\nimplementation_steps = [\n    \"1. Data Collection: Use web scraping or API to gather throne speeches\",\n    \"2. Data Cleaning: Extract text, dates, and metadata using SpaCy\",\n    \"3. Corpus Structure: Create pandas DataFrame similar to inaugural corpus\", \n    \"4. Analysis Pipeline: Apply same religious discourse analysis techniques\",\n    \"5. Comparative Study: Compare Canadian vs. US religious political rhetoric\",\n    \"6. Visualization: Create charts showing differences and similarities\",\n    \"7. Historical Context: Interpret findings in light of different political systems\"\n]\n\nfor step in implementation_steps:\n    print(f\"  {step}\")\n\nprint(f\"\\nüîç Research Questions for Canadian Analysis:\")\ncanadian_questions = [\n    \"How does religious language in Throne Speeches compare to US inaugurals?\",\n    \"Do Canadian speeches show different temporal patterns?\",\n    \"How do different Prime Ministers vary in religious rhetoric?\",\n    \"Does the Westminster system influence religious language differently?\",\n    \"How do Quebec/French Canadian influences affect religious discourse?\"\n]\n\nfor i, question in enumerate(canadian_questions, 1):\n    print(f\"  {i}. {question}\")\n\nprint(f\"\\nüíª Code Template for Canadian Corpus:\")\nprint(\"=\" * 40)\n\n# Template code structure\ntemplate_code = '''\n# Step 1: Data collection function\ndef collect_throne_speeches():\n    \"\"\"Collect Canadian throne speeches from online sources\"\"\"\n    # Your web scraping or API code here\n    pass\n\n# Step 2: Process Canadian texts  \ndef process_canadian_text(text):\n    \"\"\"Process Canadian political text with SpaCy\"\"\"\n    # Apply same processing as US inaugurals\n    # Consider bilingual content (English/French)\n    pass\n\n# Step 3: Comparative analysis\ndef compare_us_canada_discourse(us_data, canadian_data):\n    \"\"\"Compare religious discourse between countries\"\"\"\n    # Statistical comparison\n    # Visualization of differences\n    # Historical interpretation\n    pass\n\n# Step 4: Bilingual analysis (advanced)\ndef analyze_french_english_differences():\n    \"\"\"Analyze differences between French and English throne speeches\"\"\"\n    # Requires French SpaCy model: python -m spacy download fr_core_news_sm\n    pass\n'''\n\nprint(template_code)\n\nprint(f\"\\nüöÄ Next Steps for Ambitious Students:\")\nnext_steps = [\n    \"1. Choose a data source and examine its structure\",\n    \"2. Write a simple web scraper or data downloader\", \n    \"3. Adapt the US inaugural analysis code for Canadian data\",\n    \"4. Create comparative visualizations\",\n    \"5. Write up findings as a research paper or blog post\",\n    \"6. Share your corpus with other digital historians!\"\n]\n\nfor step in next_steps:\n    print(f\"  {step}\")\n\nprint(f\"\\nüí° This project combines:\")\nprint(\"  ‚úÖ All the web scraping skills from Notebook 2\")\nprint(\"  ‚úÖ All the text analysis skills from Notebook 3\") \nprint(\"  ‚úÖ Original historical research\")\nprint(\"  ‚úÖ Cross-national comparative analysis\")\nprint(\"  üá®üá¶ Contributing to Canadian digital humanities!\")\n\nprint(f\"\\nüìß If you build this corpus, consider sharing it with:\")\nprint(\"  - Canadian political science researchers\")\nprint(\"  - Digital humanities communities\") \nprint(\"  - The Programming Historian\")\nprint(\"  - Government of Canada open data initiatives\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## üçÅ Bonus Challenge: Building a Canadian Corpus\n\nReady for a advanced project? Let's plan how to create your own corpus of Canadian political texts using the skills you've learned.\n\n**Goal**: Build a corpus of Canadian Throne Speeches for comparative analysis with US inaugurals.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Final Challenge: Your Historical Research Project\n\nprint(\"üî¨ Final Challenge: Comparative Historical Analysis\")\nprint(\"=\" * 60)\n\n# Research question suggestions\nresearch_questions = [\n    \"How does religious language differ between Republican and Democratic presidents?\",\n    \"Do crisis periods (wars, depressions) correlate with increased religious rhetoric?\",\n    \"Which religious concepts (divine, God, blessing) are most common across eras?\",\n    \"How has the formality of religious language changed over time?\",\n    \"Do longer inaugurals contain proportionally more religious content?\"\n]\n\nprint(\"üéØ Suggested Research Questions:\")\nfor i, question in enumerate(research_questions, 1):\n    print(f\"  {i}. {question}\")\n\nprint(f\"\\nüìã Your Task:\")\nprint(\"1. Choose a research question (or create your own)\")\nprint(\"2. Use the analysis techniques from this notebook\")\nprint(\"3. Create visualizations to support your findings\")\nprint(\"4. Write a brief historical interpretation\")\n\n# Example analysis: Party comparison\nprint(f\"\\nüìä Example Analysis: Religious Language by Political Party\")\nprint(\"=\" * 50)\n\n# Filter for speeches with party data (excluding Washington who was nonpartisan)\nparty_data = inaugural_df[inaugural_df['Party'] != 'Nonpartisan'].copy()\n\nif len(party_data) > 0:\n    party_comparison = party_data.groupby('Party').agg({\n        'religious_density': ['mean', 'std', 'count'],\n        'religious_count': 'mean'\n    }).round(2)\n    \n    print(\"Religious density by party:\")\n    print(party_comparison)\n    \n    # Simple party comparison visualization\n    plt.figure(figsize=(10, 6))\n    \n    # Box plot comparing parties\n    plt.subplot(1, 2, 1)\n    party_groups = [group['religious_density'].values for name, group in party_data.groupby('Party')]\n    party_names = list(party_data.groupby('Party').groups.keys())\n    \n    plt.boxplot(party_groups, labels=party_names)\n    plt.title('Religious Density Distribution by Party')\n    plt.ylabel('Religious Density (%)')\n    plt.xticks(rotation=45)\n    \n    # Time series by party\n    plt.subplot(1, 2, 2)\n    for party in party_names:\n        party_subset = party_data[party_data['Party'] == party]\n        plt.plot(party_subset['Year'], party_subset['religious_density'], \n                'o-', label=party, alpha=0.7)\n    \n    plt.title('Religious Density Over Time by Party')\n    plt.xlabel('Year')\n    plt.ylabel('Religious Density (%)')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n\n# Your turn - add your own analysis here!\nprint(f\"\\nüöÄ Your Analysis Space:\")\nprint(\"=\" * 30)\nprint(\"# Customize this section for your research question\")\nprint(\"# Use the functions and techniques from earlier cells\")\nprint(\"# Examples:\")\nprint(\"# - Compare different time periods\")\nprint(\"# - Analyze specific religious themes\")\nprint(\"# - Track evolution of particular phrases\")\nprint(\"# - Examine correlation with historical events\")\n\n# Template for student analysis\nyour_research_question = \"Your research question here\"\nprint(f\"\\nüìù Research Question: {your_research_question}\")\n\n# Add your analysis code here:\n# your_data = inaugural_df[some_filter]\n# your_results = some_analysis(your_data)  \n# create_your_visualization(your_results)\n\nprint(f\"\\nüìö Research Findings:\")\nprint(\"1. [Your first finding]\")\nprint(\"2. [Your second finding]\") \nprint(\"3. [Your interpretation of the historical significance]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 6: Final Challenge - Comparative Historical Analysis\n\nNow combine all your skills to conduct a comprehensive analysis comparing different historical periods or presidents.\n\n**Your research project:**\nUse the techniques you've learned to answer a historical question about religious discourse in presidential inaugurals.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create visualizations of religious discourse trends\nplt.style.use('default')  # Clean, professional style\nplt.figure(figsize=(12, 8))\n\n# Plot 1: Religious density over time\nplt.subplot(2, 2, 1)\nplt.plot(inaugural_df['Year'], inaugural_df['religious_density'], 'o-', linewidth=2, markersize=6)\nplt.title('Religious Discourse Density Over Time', fontsize=12, fontweight='bold')\nplt.xlabel('Year')\nplt.ylabel('Religious Density (%)')\nplt.grid(True, alpha=0.3)\n\n# Annotate some key points\nfor idx, row in inaugural_df.iterrows():\n    if row['religious_density'] > inaugural_df['religious_density'].mean() + 5:  # High points\n        plt.annotate(f\"{row['President']}\\n{row['religious_density']:.1f}%\", \n                    (row['Year'], row['religious_density']),\n                    xytext=(10, 10), textcoords='offset points',\n                    fontsize=8, ha='left')\n\n# Plot 2: Raw religious word counts\nplt.subplot(2, 2, 2)\nplt.bar(inaugural_df['Year'], inaugural_df['religious_count'], alpha=0.7)\nplt.title('Number of Religious Words per Speech', fontsize=12, fontweight='bold')\nplt.xlabel('Year')\nplt.ylabel('Religious Word Count')\nplt.xticks(rotation=45)\n\n# Plot 3: Religious words vs. speech length\nplt.subplot(2, 2, 3)\nplt.scatter(inaugural_df['token_count'], inaugural_df['religious_count'], \n           s=60, alpha=0.7, c=inaugural_df['Year'], cmap='viridis')\nplt.title('Religious Words vs. Speech Length', fontsize=12, fontweight='bold')\nplt.xlabel('Total Words in Speech')\nplt.ylabel('Religious Words')\nplt.colorbar(label='Year')\n\n# Add trend line\nz = np.polyfit(inaugural_df['token_count'], inaugural_df['religious_count'], 1)\np = np.poly1d(z)\nplt.plot(inaugural_df['token_count'], p(inaugural_df['token_count']), \"r--\", alpha=0.8)\n\n# Plot 4: Historical periods comparison\nplt.subplot(2, 2, 4)\n# Create era categories\ninaugural_df['Era'] = pd.cut(inaugural_df['Year'], \n                            bins=[1780, 1850, 1900, 1950, 2030],\n                            labels=['Early Republic\\n(1789-1850)', 'Civil War Era\\n(1851-1900)', \n                                   'Modern Era\\n(1901-1950)', 'Contemporary\\n(1951-2021)'])\n\nera_avg = inaugural_df.groupby('Era')['religious_density'].mean()\nplt.bar(range(len(era_avg)), era_avg.values, alpha=0.7)\nplt.title('Average Religious Density by Era', fontsize=12, fontweight='bold')\nplt.xlabel('Historical Era')\nplt.ylabel('Average Religious Density (%)')\nplt.xticks(range(len(era_avg)), era_avg.index, rotation=45)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"üìä Visualization Insights:\")\nprint(\"=\" * 40)\nprint(\"üìà Top plot: Shows religious density trends over time\")\nprint(\"üìä Bar chart: Raw counts of religious words per speech\")\nprint(\"üîç Scatter plot: Relationship between speech length and religious content\")\nprint(\"üìÖ Bottom plot: Comparison across historical eras\")\nprint(f\"\\nüí° Key patterns to notice:\")\nprint(f\"  - Do religious references increase or decrease over time?\")\nprint(f\"  - Are longer speeches more religious?\")\nprint(f\"  - Which historical eras had the most religious rhetoric?\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 5: Visualizing Temporal Trends\n\nData visualization helps us see patterns that are hard to spot in tables. Let's create charts showing how religious discourse has changed over time.\n\n**Step 5a: Religious density over time**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Your exercise: Track specific religious phrases over time\ndef track_phrase_over_time(df, phrase):\n    \"\"\"Track occurrences of a specific phrase across speeches\"\"\"\n    results = []\n    \n    for idx, row in df.iterrows():\n        text_lower = row['text'].lower()\n        phrase_count = text_lower.count(phrase.lower())\n        \n        results.append({\n            'Year': row['Year'],\n            'President': row['President'],\n            'Phrase': phrase,\n            'Count': phrase_count,\n            'Present': phrase_count > 0\n        })\n    \n    return pd.DataFrame(results)\n\n# Track some key religious phrases\nphrases_to_track = ['god bless', 'divine providence', 'almighty god']\n\nprint(\"üéØ Tracking Specific Religious Phrases\")\nprint(\"=\" * 50)\n\nfor phrase in phrases_to_track:\n    phrase_data = track_phrase_over_time(inaugural_df, phrase)\n    total_uses = phrase_data['Count'].sum()\n    presidents_using = phrase_data[phrase_data['Present']]['President'].tolist()\n    \n    print(f\"\\nüìù Phrase: '{phrase}'\")\n    print(f\"   Total uses: {total_uses}\")\n    print(f\"   Presidents using it: {', '.join(presidents_using) if presidents_using else 'None'}\")\n    \n    # Show specific occurrences\n    for idx, row in phrase_data.iterrows():\n        if row['Count'] > 0:\n            print(f\"     {row['Year']} {row['President']}: {row['Count']} time(s)\")\n\n# Try tracking your own phrase:\n# my_phrase = \"under god\"  # Example\n# my_results = track_phrase_over_time(inaugural_df, my_phrase)\n# print(f\"\\nYour phrase '{my_phrase}' analysis:\")\n# print(my_results[my_results['Count'] > 0])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### üîÑ **Your Turn: Track Specific Religious Phrases**\n\nChoose a religious phrase and track its usage across different time periods:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# N-gram analysis for religious phrases\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef extract_ngrams(text, n=2):\n    \"\"\"Extract n-grams from text using scikit-learn\"\"\"\n    # Use CountVectorizer to extract n-grams\n    vectorizer = CountVectorizer(\n        ngram_range=(n, n),  # Only n-grams of length n\n        stop_words='english',\n        lowercase=True,\n        token_pattern=r'[a-zA-Z]+',  # Only alphabetic tokens\n        min_df=1  # Minimum document frequency\n    )\n    \n    # Fit and transform the text\n    ngram_matrix = vectorizer.fit_transform([text])\n    \n    # Get the n-grams and their counts\n    feature_names = vectorizer.get_feature_names_out()\n    counts = ngram_matrix.toarray()[0]\n    \n    # Create list of (ngram, count) tuples\n    ngrams_with_counts = list(zip(feature_names, counts))\n    \n    # Sort by count (descending)\n    ngrams_with_counts.sort(key=lambda x: x[1], reverse=True)\n    \n    return ngrams_with_counts\n\ndef find_religious_ngrams(ngrams_list, religious_vocab=all_religious_words):\n    \"\"\"Filter n-grams that contain religious vocabulary\"\"\"\n    religious_ngrams = []\n    \n    for ngram, count in ngrams_list:\n        # Check if any word in the n-gram is religious\n        words_in_ngram = ngram.split()\n        if any(word in religious_vocab for word in words_in_ngram):\n            religious_ngrams.append((ngram, count))\n    \n    return religious_ngrams\n\nprint(\"üìù N-gram Analysis: Finding Religious Phrases\")\nprint(\"=\" * 60)\n\n# Analyze bigrams (2-word phrases) across all speeches\nprint(\"üîç Analyzing 2-grams (bigrams)...\")\n\n# Combine all speech texts for corpus-wide analysis\nall_texts = ' '.join(inaugural_df['text'].tolist())\n\n# Extract bigrams\nbigrams = extract_ngrams(all_texts, n=2)\nreligious_bigrams = find_religious_ngrams(bigrams)\n\nprint(f\"\\nTop religious bigrams:\")\nfor bigram, count in religious_bigrams[:10]:\n    print(f\"  '{bigram}': {count} occurrences\")\n\n# Extract trigrams (3-word phrases)\nprint(f\"\\nüîç Analyzing 3-grams (trigrams)...\")\ntrigrams = extract_ngrams(all_texts, n=3)\nreligious_trigrams = find_religious_ngrams(trigrams)\n\nprint(f\"\\nTop religious trigrams:\")\nfor trigram, count in religious_trigrams[:8]:\n    print(f\"  '{trigram}': {count} occurrences\")\n\nprint(f\"\\nüí° N-gram insights:\")\nprint(f\"  - Bigrams reveal common religious phrases\")\nprint(f\"  - Trigrams show complete religious expressions\")\nprint(f\"  - Frequency indicates which phrases are most traditional\")\nprint(f\"  - Perfect for tracking phrase evolution over time!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 4: N-gram Analysis - Finding Religious Phrases\n\nIndividual words tell part of the story, but phrases reveal deeper patterns. Let's analyze 2-grams (bigrams) and 3-grams (trigrams) to find religious phrases like \"God bless America\" or \"divine providence.\"\n\n**Step 4a: Extract n-grams from speeches**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Analyze religious discourse across all speeches\nprint(\"üîç Analyzing religious discourse across all inaugurals...\")\n\n# Add religious analysis columns\ninaugural_df['religious_count'] = inaugural_df['processed_tokens'].apply(\n    lambda tokens: count_religious_words(tokens)[0]\n)\n\ninaugural_df['religious_words'] = inaugural_df['processed_tokens'].apply(\n    lambda tokens: count_religious_words(tokens)[1]\n)\n\ninaugural_df['religious_density'] = (\n    inaugural_df['religious_count'] / inaugural_df['token_count'] * 100\n).round(1)\n\n# Display results\nprint(f\"\\nüìä Religious Discourse Analysis Results:\")\nprint(\"=\" * 70)\nfor idx, row in inaugural_df.iterrows():\n    print(f\"{row['Year']} {row['President']:<12}: {row['religious_count']:2d} religious words \"\n          f\"({row['religious_density']:4.1f}% density)\")\n    \n    # Show specific religious words found\n    unique_religious = list(set(row['religious_words']))\n    if unique_religious:\n        print(f\"{'':26} Words: {', '.join(unique_religious[:6])}\")\n        if len(unique_religious) > 6:\n            print(f\"{'':26} + {len(unique_religious) - 6} more...\")\n    print()\n\n# Calculate summary statistics\navg_density = inaugural_df['religious_density'].mean()\nmax_religious = inaugural_df.loc[inaugural_df['religious_density'].idxmax()]\nmin_religious = inaugural_df.loc[inaugural_df['religious_density'].idxmin()]\n\nprint(f\"üìà Summary Statistics:\")\nprint(f\"  Average religious density: {avg_density:.1f}%\")\nprint(f\"  Highest religious content: {max_religious['President']} ({max_religious['Year']}) - {max_religious['religious_density']:.1f}%\")\nprint(f\"  Lowest religious content: {min_religious['President']} ({min_religious['Year']}) - {min_religious['religious_density']:.1f}%\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**Step 3b: Analyze religious discourse across all speeches**\n\nNow let's apply this analysis to all inaugural addresses to see temporal patterns:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Define religious vocabulary categories\nreligious_vocabulary = {\n    'Divine References': ['god', 'almighty', 'divine', 'lord', 'providence', 'creator', 'heaven', 'holy'],\n    'Religious Actions': ['pray', 'prayer', 'bless', 'blessing', 'worship', 'faith', 'believe'],\n    'Religious Concepts': ['religious', 'sacred', 'holy', 'spiritual', 'righteous', 'moral', 'virtue'],\n    'Biblical/Christian': ['jesus', 'christ', 'christian', 'bible', 'scripture', 'gospel', 'salvation']\n}\n\n# Flatten the vocabulary for easy searching\nall_religious_words = []\nfor category, words in religious_vocabulary.items():\n    all_religious_words.extend(words)\n\nprint(\"üôè Religious Vocabulary Analysis\")\nprint(\"=\" * 50)\nprint(\"üìñ Religious word categories:\")\nfor category, words in religious_vocabulary.items():\n    print(f\"  {category}: {', '.join(words)}\")\n\nprint(f\"\\nüìä Total religious terms tracked: {len(all_religious_words)}\")\n\n# Function to count religious words in a text\ndef count_religious_words(tokens, religious_vocab=all_religious_words):\n    \"\"\"Count religious words in processed tokens\"\"\"\n    religious_count = 0\n    found_words = []\n    \n    for token in tokens:\n        if token in religious_vocab:\n            religious_count += 1\n            found_words.append(token)\n    \n    return religious_count, found_words\n\n# Test with Washington's speech\nwashington_tokens = inaugural_df[inaugural_df['President'] == 'Washington']['processed_tokens'].iloc[0]\nrel_count, rel_words = count_religious_words(washington_tokens)\n\nprint(f\"\\nüîç Test with Washington (1789):\")\nprint(f\"  Religious words found: {rel_count}\")\nprint(f\"  Specific words: {', '.join(set(rel_words))}\")\nprint(f\"  Religious density: {rel_count/len(washington_tokens)*100:.1f}% of all words\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 3: Analyzing Religious Discourse\n\nNow let's focus on our research question: How has religious language evolved in presidential inaugurals? We'll create a targeted analysis of religious vocabulary.\n\n**Step 3a: Define religious vocabulary**\n\nFirst, we need to identify what constitutes \"religious\" language:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Your exercise: Process all inaugural addresses\nprint(\"üîÑ Processing all inaugural addresses with SpaCy...\")\n\n# Add a column for processed tokens\ninaugural_df['processed_tokens'] = inaugural_df['text'].apply(\n    lambda text: process_text_with_spacy(text, remove_stop_words=True, lemmatize=True)\n)\n\n# Add a column for token count\ninaugural_df['token_count'] = inaugural_df['processed_tokens'].apply(len)\n\n# Display results\nprint(f\"\\nüìä Processing Results:\")\nprint(\"=\" * 60)\nfor idx, row in inaugural_df.iterrows():\n    print(f\"{row['Year']} {row['President']}: {row['token_count']} processed tokens\")\n    print(f\"  Sample tokens: {row['processed_tokens'][:8]}...\")\n    print()\n\nprint(f\"‚úÖ Successfully processed {len(inaugural_df)} inaugural addresses!\")\nprint(f\"üìà Total unique vocabulary across all speeches: {len(set([token for tokens in inaugural_df['processed_tokens'] for token in tokens]))}\")\n\n# Try with your own processing settings:\n# inaugural_df['tokens_no_lemma'] = inaugural_df['text'].apply(\n#     lambda text: process_text_with_spacy(text, remove_stop_words=True, lemmatize=False)\n# )",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### üîÑ **Your Turn: Process the Entire Corpus**\n\nNow apply SpaCy processing to all inaugural addresses in our corpus:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def process_text_with_spacy(text, remove_stop_words=True, lemmatize=True):\n    \"\"\"\n    Process text using SpaCy for professional text analysis\n    \n    Parameters:\n    - text: Input text string\n    - remove_stop_words: Whether to filter out common words\n    - lemmatize: Whether to convert words to root forms\n    \n    Returns:\n    - List of processed tokens\n    \"\"\"\n    # Process with SpaCy\n    doc = nlp(text)\n    \n    processed_tokens = []\n    \n    for token in doc:\n        # Skip punctuation and whitespace\n        if token.is_punct or token.is_space:\n            continue\n            \n        # Skip stop words if requested\n        if remove_stop_words and token.is_stop:\n            continue\n            \n        # Skip very short tokens\n        if len(token.text) < 2:\n            continue\n            \n        # Use lemma (root form) if requested, otherwise use original text\n        if lemmatize:\n            word = token.lemma_.lower()\n        else:\n            word = token.text.lower()\n            \n        # Only keep alphabetic tokens\n        if word.isalpha():\n            processed_tokens.append(word)\n    \n    return processed_tokens\n\n# Test our function with Washington's sample text\nwashington_text = inaugural_df[inaugural_df['President'] == 'Washington']['text'].iloc[0]\n\nprint(\"üîç Testing our SpaCy processing function:\")\nprint(\"=\" * 60)\nprint(f\"Original text: {washington_text}\")\nprint()\n\n# Process with different settings\ntokens_basic = process_text_with_spacy(washington_text, remove_stop_words=False, lemmatize=False)\ntokens_full = process_text_with_spacy(washington_text, remove_stop_words=True, lemmatize=True)\n\nprint(f\"Basic processing (no filtering): {len(tokens_basic)} tokens\")\nprint(f\"  {tokens_basic[:10]}...\")\n\nprint(f\"Full processing (stop words removed, lemmatized): {len(tokens_full)} tokens\")\nprint(f\"  {tokens_full[:10]}...\")\n\nprint(f\"\\n‚úÖ Notice how SpaCy gives us much cleaner, more meaningful tokens!\")\nprint(f\"   - Removes common words like 'the', 'who', 'has'\")\nprint(f\"   - Converts words to root forms (e.g., 'honored' -> 'honor')\")\nprint(f\"   - Filters out punctuation automatically\")",
   "metadata": {
    "id": "FdKe5pabP13w"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}