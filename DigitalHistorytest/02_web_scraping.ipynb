{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Notebook 2: Web Scraping for Historians**\n",
    "\n",
    "Welcome to web scraping! In this notebook, you'll learn to extract historical data from websites using Python. We'll work with real Canadian historical sources and build your skills step by step.\n",
    "\n",
    "**What you'll learn:**\n",
    "- How web pages are structured (HTML basics)\n",
    "- Using Beautiful Soup to extract specific content\n",
    "- Working with Canadian historical archives\n",
    "- Building reusable code for research\n",
    "\n",
    "**Ethics first:** Always respect websites. Check robots.txt files, don't overload servers, and respect copyright."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setting Up Our Tools\n",
    "\n",
    "Before we can scrape websites, we need to install and import the right libraries. Let's do this step by step.\n",
    "\n",
    "**First, install the libraries:**"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Example: How to add delays between requests\nimport time\n\ndef respectful_get(url, delay=1):\n    \"\"\"\n    Make a web request with built-in delay for respectful scraping\n    \"\"\"\n    print(f\"‚è≥ Waiting {delay} second(s) before request...\")\n    time.sleep(delay)  # Wait before making request\n    \n    try:\n        response = requests.get(url, timeout=10)\n        response.raise_for_status()\n        print(f\"‚úÖ Request successful\")\n        return response\n    except requests.exceptions.RequestException as e:\n        print(f\"‚ùå Request failed: {e}\")\n        return None\n\n# We'll use this function for respectful scraping throughout the notebook\nprint(\"Respectful scraping function defined!\")\nprint(\"üí° This adds delays between requests to be considerate to servers.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## ü§ù Ethical Web Scraping Guidelines\n\nBefore we start scraping, let's understand the ethics and best practices:\n\n**‚úÖ Always:**\n- Check robots.txt (add /robots.txt to any website URL)\n- Add delays between requests (don't overwhelm servers)\n- Use reasonable timeouts\n- Respect copyright and terms of service\n- Identify yourself with User-Agent headers when appropriate\n\n**‚ùå Never:**\n- Scrape faster than a human could browse\n- Ignore error messages or blocks\n- Scrape personal/private information\n- Violate website terms of service\n- Overload servers with rapid requests\n\n**üìñ For historical research:**\n- Many archives encourage responsible academic use\n- Always cite your digital sources properly\n- Consider contacting archives for bulk data access\n- Respect cultural sensitivities in historical materials",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This installs the libraries we need\n",
    "!pip install requests beautifulsoup4 --quiet\n",
    "print(\"Libraries installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, import them so we can use them:**\n",
    "\n",
    "Copy this code into the cell below:\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "```\n",
    "If you don't get an error and nothing happens after you click the run button, it worked. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import the libraries we need for web scraping\nimport requests\nfrom bs4 import BeautifulSoup\n\nprint(\"Libraries imported successfully!\")\nprint(\"‚úÖ requests: Downloads web pages\")\nprint(\"‚úÖ BeautifulSoup: Parses HTML content\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Your First Web Request\n",
    "\n",
    "Let's start by downloading a web page. We'll use a Canadian historical source - a Toronto Public Library blog post about Jesuit Relations.\n",
    "\n",
    "**Step 2a: Define the URL**\n",
    "\n",
    "Copy this code:\n",
    "```python\n",
    "url = \"https://torontopubliclibrary.typepad.com/local-history-genealogy/2020/01/sainte-marie-among-the-hurons-selections-from-the-jesuit-relations-and-allied-documents.html\"\n",
    "print(f\"We're going to scrape: {url}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define the URL for our Canadian historical source\nurl = \"https://torontopubliclibrary.typepad.com/local-history-genealogy/2020/01/sainte-marie-among-the-hurons-selections-from-the-jesuit-relations-and-allied-documents.html\"\nprint(f\"We're going to scrape: {url}\")\nprint(\"\\nüìö This is a Toronto Public Library blog post about Jesuit Relations\")\nprint(\"üá®üá¶ Perfect for learning Canadian historical web scraping!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2b: Download the page**\n",
    "\n",
    "Now let's actually download the web page. The `requests.get()` function fetches the page for us.\n",
    "\n",
    "Copy this code:\n",
    "```python\n",
    "response = requests.get(url)\n",
    "print(f\"Status Code: {response.status_code}\")  # 200 means success\n",
    "print(f\"Page downloaded! It contains {len(response.text)} characters.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Download the page with error handling\ntry:\n    response = requests.get(url, timeout=10)  # 10 second timeout\n    response.raise_for_status()  # Raises an exception for bad status codes\n    print(f\"‚úÖ Success! Status Code: {response.status_code}\")\n    print(f\"üìÑ Page downloaded! It contains {len(response.text):,} characters.\")\nexcept requests.exceptions.RequestException as e:\n    print(f\"‚ùå Error downloading page: {e}\")\n    print(\"üí° This could be due to:\")\n    print(\"   - No internet connection\")\n    print(\"   - Website is down\")\n    print(\"   - URL has changed\")\n    print(\"   - Server is blocking requests\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2c: Look at the raw HTML**\n",
    "\n",
    "Let's see what we actually downloaded. Warning: it's going to look messy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Your adaptation exercise: Saskatchewan Internet Archive document\nsaskatchewan_url = \"https://archive.org/details/saskatchewan00sask\"\n\nprint(\"üçÅ Trying Saskatchewan historical document...\")\nprint(f\"URL: {saskatchewan_url}\")\n\ntry:\n    # Use our respectful function\n    sask_response = respectful_get(saskatchewan_url)\n    \n    if sask_response:\n        print(f\"‚úÖ Status Code: {sask_response.status_code}\")\n        print(f\"üìÑ Downloaded {len(sask_response.text):,} characters\")\n        \n        # Show first 500 characters\n        print(\"\\nFirst 500 characters of raw HTML:\")\n        print(\"-\" * 50)\n        print(sask_response.text[:500])\n        print(\"-\" * 50)\n        print(\"üí° Notice how different this Internet Archive page looks compared to the blog!\")\n    else:\n        print(\"‚ùå Failed to download Saskatchewan document\")\n        \nexcept Exception as e:\n    print(f\"‚ùå Error: {e}\")\n    print(\"üí° If this fails, it might be due to network issues or site changes\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ **Your Turn: Adaptation Exercise**\n",
    "\n",
    "Now try the same steps with a different Canadian source. Copy the code from above and modify it to use this Internet Archive document about Saskatchewan:\n",
    "\n",
    "```python\n",
    "url = \"https://archive.org/details/saskatchewan00sask\"\n",
    "```\n",
    "\n",
    "Follow the same steps: define the URL, download the page, check the status code, and look at the first 500 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your adaptation exercise here\n",
    "# 1. Define the Saskatchewan URL\n",
    "# 2. Download the page with requests.get()\n",
    "# 3. Print the status code and character count\n",
    "# 4. Show first 500 characters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Making Sense of HTML with Beautiful Soup\n",
    "\n",
    "Raw HTML is hard to work with. Beautiful Soup parses it and makes it easy to extract what we need.\n",
    "\n",
    "**Step 3a: Create your first \"soup\"**\n",
    "\n",
    "Let's go back to our TPL blog post and parse it properly:\n",
    "\n",
    "Copy this code:\n",
    "```python\n",
    "# Go back to the TPL blog post\n",
    "url = \"https://torontopubliclibrary.typepad.com/local-history-genealogy/2020/01/sainte-marie-among-the-hurons-selections-from-the-jesuit-relations-and-allied-documents.html\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create the soup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "print(\"Soup object created! Now we can easily extract content.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create soup object with validation\ntry:\n    # Use our respectful function\n    response = respectful_get(url)\n    \n    if response:\n        # Create the soup object\n        soup = BeautifulSoup(response.text, 'html.parser')\n        print(\"‚úÖ Soup object created successfully!\")\n        \n        # Validate we got HTML content\n        if soup.find('html'):\n            print(\"‚úÖ Valid HTML structure detected\")\n        else:\n            print(\"‚ö†Ô∏è  Warning: No HTML structure found - might be plain text\")\n            \n        # Check if we got the expected content\n        if len(soup.get_text().strip()) > 100:\n            print(f\"‚úÖ Content validation: {len(soup.get_text().strip()):,} characters of text\")\n        else:\n            print(\"‚ö†Ô∏è  Warning: Very little text content found\")\n    else:\n        print(\"‚ùå Could not create soup - request failed\")\n        \nexcept Exception as e:\n    print(f\"‚ùå Error creating soup: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3b: Extract the page title**\n",
    "\n",
    "Let's start simple by getting the page title. In HTML, the title is in `<title>` tags.\n",
    "\n",
    "Copy this code:\n",
    "```python\n",
    "title = soup.find('title')\n",
    "print(f\"Page title: {title.get_text()}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Extract title with validation\ntry:\n    title_tag = soup.find('title')\n    \n    if title_tag:\n        title_text = title_tag.get_text().strip()\n        print(f\"‚úÖ Page title found: {title_text}\")\n        \n        # Validate the title makes sense\n        if len(title_text) > 5:\n            print(\"‚úÖ Title validation: Reasonable length\")\n        else:\n            print(\"‚ö†Ô∏è  Warning: Title seems very short\")\n            \n        # Check if it's related to our expected content\n        if any(keyword in title_text.lower() for keyword in ['jesuit', 'sainte-marie', 'huron', 'toronto']):\n            print(\"‚úÖ Content validation: Title matches expected historical topic\")\n        else:\n            print(\"‚ö†Ô∏è  Warning: Title doesn't match expected content - check URL\")\n    else:\n        print(\"‚ùå No title tag found\")\n        print(\"üí° This might mean:\")\n        print(\"   - Page structure is different than expected\")\n        print(\"   - We got redirected to a different page\")\n        print(\"   - Page failed to load properly\")\n        \nexcept Exception as e:\n    print(f\"‚ùå Error extracting title: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3c: Get clean text (no HTML tags)**\n",
    "\n",
    "The `.get_text()` method removes all HTML tags and gives us just the readable content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all text content without HTML tags\n",
    "clean_text = soup.get_text()\n",
    "\n",
    "print(f\"Clean text length: {len(clean_text)} characters\")\n",
    "print(\"\\nFirst 500 characters of clean text:\")\n",
    "print(clean_text[:500])\n",
    "print(\"\\nMuch better! Now we can read the actual content.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ **Your Turn: Practice with Internet Archive**\n",
    "\n",
    "Now apply the same Beautiful Soup steps to the Saskatchewan document. Copy and adapt the code above:\n",
    "\n",
    "1. Create a soup object from the Saskatchewan URL\n",
    "2. Extract and print the title\n",
    "3. Get the clean text and show the first 500 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Your practice: Apply Beautiful Soup to Saskatchewan document\nprint(\"üçÅ Analyzing Saskatchewan document with Beautiful Soup...\")\n\ntry:\n    # Create soup object for Saskatchewan document\n    sask_soup = BeautifulSoup(sask_response.text, 'html.parser')\n    print(\"‚úÖ Saskatchewan soup created!\")\n    \n    # Extract and print the title\n    sask_title = sask_soup.find('title')\n    if sask_title:\n        title_text = sask_title.get_text().strip()\n        print(f\"üìö Title: {title_text}\")\n        \n        # Validate it's an Internet Archive page\n        if 'archive.org' in title_text.lower() or 'saskatchewan' in title_text.lower():\n            print(\"‚úÖ Confirmed: This is the Saskatchewan document\")\n        else:\n            print(\"‚ö†Ô∏è  Title doesn't match expectations\")\n    else:\n        print(\"‚ùå No title found\")\n    \n    # Get clean text and show first 500 characters\n    clean_text = sask_soup.get_text()\n    print(f\"\\nüìÑ Clean text length: {len(clean_text):,} characters\")\n    print(\"\\nFirst 500 characters of clean text:\")\n    print(\"-\" * 50)\n    print(clean_text[:500])\n    print(\"-\" * 50)\n    print(\"üí° Much more readable than raw HTML!\")\n    \nexcept NameError:\n    print(\"‚ùå Saskatchewan response not available - run the previous exercise first\")\nexcept Exception as e:\n    print(f\"‚ùå Error: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Targeting Specific Content\n",
    "\n",
    "Getting all the text is useful, but often we want specific parts. Let's learn to target particular HTML elements.\n",
    "\n",
    "**Step 4a: Find all paragraphs**\n",
    "\n",
    "Blog posts organize content in paragraphs (`<p>` tags). Let's find them:\n",
    "\n",
    "Copy this code:\n",
    "```python\n",
    "# Go back to our TPL blog soup\n",
    "url = \"https://torontopubliclibrary.typepad.com/local-history-genealogy/2020/01/sainte-marie-among-the-hurons-selections-from-the-jesuit-relations-and-allied-documents.html\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all paragraph tags\n",
    "paragraphs = soup.find_all('p')\n",
    "print(f\"Found {len(paragraphs)} paragraphs\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Find all paragraphs with validation\ntry:\n    # Make sure we have our soup object\n    if 'soup' not in locals():\n        print(\"‚ùå Soup object not found - please run the previous Beautiful Soup cells first\")\n    else:\n        # Find all paragraph tags\n        paragraphs = soup.find_all('p')\n        print(f\"üìù Found {len(paragraphs)} paragraph tags\")\n        \n        # Validate we found reasonable content\n        if len(paragraphs) > 0:\n            print(\"‚úÖ Paragraph extraction successful\")\n            \n            # Show some examples\n            print(f\"\\nAnalyzing first 3 paragraphs:\")\n            for i in range(min(3, len(paragraphs))):\n                para_text = paragraphs[i].get_text().strip()\n                if para_text:  # Only show non-empty paragraphs\n                    print(f\"\\nParagraph {i+1} ({len(para_text)} chars):\")\n                    print(f\"'{para_text[:100]}{'...' if len(para_text) > 100 else ''}'\")\n                else:\n                    print(f\"\\nParagraph {i+1}: (empty)\")\n        else:\n            print(\"‚ö†Ô∏è  Warning: No paragraphs found\")\n            print(\"üí° This might mean:\")\n            print(\"   - Page uses different HTML structure\")\n            print(\"   - Content is in different tags (div, article, etc.)\")\n            print(\"   - Page didn't load properly\")\n            \nexcept Exception as e:\n    print(f\"‚ùå Error finding paragraphs: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4b: Look at individual paragraphs**\n",
    "\n",
    "Let's examine a few paragraphs to see what we're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the first few paragraphs\n",
    "print(\"First 3 paragraphs:\")\n",
    "for i in range(3):\n",
    "    if i < len(paragraphs):\n",
    "        para_text = paragraphs[i].get_text().strip()\n",
    "        print(f\"\\nParagraph {i+1}: {para_text[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4c: Filter for substantial paragraphs**\n",
    "\n",
    "Many paragraphs are short or empty. Let's filter for substantial ones (more than 50 characters):\n",
    "\n",
    "Copy this code:\n",
    "```python\n",
    "# Filter for paragraphs with meaningful content\n",
    "substantial_paras = []\n",
    "for para in paragraphs:\n",
    "    text = para.get_text().strip()\n",
    "    if len(text) > 50:  # Only paragraphs with more than 50 characters\n",
    "        substantial_paras.append(text)\n",
    "\n",
    "print(f\"Substantial paragraphs: {len(substantial_paras)}\")\n",
    "print(f\"\\nFirst substantial paragraph:\")\n",
    "print(substantial_paras[0])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Filter for substantial paragraphs with validation\ntry:\n    if 'paragraphs' not in locals():\n        print(\"‚ùå Paragraphs not found - run the previous cell first\")\n    else:\n        # Filter for paragraphs with meaningful content\n        substantial_paras = []\n        empty_count = 0\n        \n        for para in paragraphs:\n            text = para.get_text().strip()\n            if len(text) > 50:  # Only paragraphs with more than 50 characters\n                substantial_paras.append(text)\n            elif len(text) == 0:\n                empty_count += 1\n        \n        print(f\"üìä Paragraph Analysis:\")\n        print(f\"   Total paragraphs found: {len(paragraphs)}\")\n        print(f\"   Substantial paragraphs (>50 chars): {len(substantial_paras)}\")\n        print(f\"   Empty paragraphs: {empty_count}\")\n        print(f\"   Short paragraphs (<50 chars): {len(paragraphs) - len(substantial_paras) - empty_count}\")\n        \n        if substantial_paras:\n            print(f\"\\nüìñ First substantial paragraph:\")\n            print(\"-\" * 60)\n            print(substantial_paras[0])\n            print(\"-\" * 60)\n            \n            # Validate content quality\n            first_para = substantial_paras[0].lower()\n            if any(word in first_para for word in ['jesuit', 'huron', 'sainte-marie', 'canada', 'history']):\n                print(\"‚úÖ Content validation: Found expected historical keywords\")\n            else:\n                print(\"‚ö†Ô∏è  Content note: No obvious historical keywords found\")\n        else:\n            print(\"‚ùå No substantial paragraphs found\")\n            print(\"üí° This might mean the content is structured differently\")\n            \nexcept Exception as e:\n    print(f\"‚ùå Error filtering paragraphs: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ **Your Turn: Find Historical Quotes**\n",
    "\n",
    "Historical blog posts often include quotes in special `<blockquote>` tags. Adapt the code above to:\n",
    "\n",
    "1. Find all blockquote elements using `soup.find_all('blockquote')`\n",
    "2. Extract their text and print them\n",
    "3. Count how many historical quotes you found\n",
    "\n",
    "Use the same TPL blog post soup object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Your exercise: Find historical quotes in blockquotes\ntry:\n    if 'soup' not in locals():\n        print(\"‚ùå Soup object not found - run the Beautiful Soup creation cells first\")\n    else:\n        # Find all blockquote elements\n        blockquotes = soup.find_all('blockquote')\n        print(f\"üìú Found {len(blockquotes)} blockquote elements\")\n        \n        if blockquotes:\n            print(\"‚úÖ Historical quotes found!\")\n            print(\"\\nüîç Analyzing each quote:\")\n            \n            valid_quotes = []\n            for i, quote in enumerate(blockquotes, 1):\n                quote_text = quote.get_text().strip()\n                \n                if quote_text and len(quote_text) > 10:  # Filter out very short quotes\n                    valid_quotes.append(quote_text)\n                    print(f\"\\nQuote {i} ({len(quote_text)} characters):\")\n                    print(\"-\" * 50)\n                    # Show first 200 characters of quote\n                    display_text = quote_text[:200] + \"...\" if len(quote_text) > 200 else quote_text\n                    print(display_text)\n                    print(\"-\" * 50)\n                    \n                    # Check for historical indicators\n                    quote_lower = quote_text.lower()\n                    historical_indicators = ['jesuit', 'huron', 'savage', 'father', 'lord', 'god', '16', '17']\n                    found_indicators = [ind for ind in historical_indicators if ind in quote_lower]\n                    \n                    if found_indicators:\n                        print(f\"üìö Historical indicators found: {', '.join(found_indicators)}\")\n                    else:\n                        print(\"üìù No obvious historical indicators\")\n                else:\n                    print(f\"Quote {i}: (too short or empty)\")\n            \n            print(f\"\\nüìä Summary:\")\n            print(f\"   Total blockquotes: {len(blockquotes)}\")\n            print(f\"   Valid historical quotes: {len(valid_quotes)}\")\n            \n            if valid_quotes:\n                avg_length = sum(len(q) for q in valid_quotes) / len(valid_quotes)\n                print(f\"   Average quote length: {avg_length:.0f} characters\")\n            \n        else:\n            print(\"‚ùå No blockquotes found on this page\")\n            print(\"üí° This might mean:\")\n            print(\"   - This blog doesn't use blockquotes for quotes\")\n            print(\"   - Quotes might be in other tags (div, p with special classes)\")\n            print(\"   - Page structure is different than expected\")\n            \nexcept Exception as e:\n    print(f\"‚ùå Error finding quotes: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Working with Links\n",
    "\n",
    "Historical sources often link to primary documents. Let's learn to extract and analyze links.\n",
    "\n",
    "**Step 5a: Find all links**\n",
    "\n",
    "Links are in `<a>` tags with `href` attributes. Let's find them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Find all links with validation\ntry:\n    if 'soup' not in locals():\n        print(\"‚ùå Soup object not found - run the Beautiful Soup creation cells first\")\n    else:\n        # Find all links with href attributes\n        all_links = soup.find_all('a', href=True)\n        print(f\"üîó Found {len(all_links)} links with href attributes\")\n        \n        if all_links:\n            print(\"‚úÖ Link extraction successful\")\n            \n            # Analyze link types\n            internal_links = 0\n            external_links = 0\n            archive_links = 0\n            \n            print(\"\\nüîç Analyzing first 5 links:\")\n            for i, link in enumerate(all_links[:5]):\n                link_text = link.get_text().strip()\n                link_url = link.get('href')\n                \n                # Classify link type\n                if link_url.startswith('http'):\n                    if 'torontopubliclibrary' in link_url:\n                        link_type = \"Internal\"\n                        internal_links += 1\n                    elif 'archive.org' in link_url:\n                        link_type = \"Archive\"\n                        archive_links += 1\n                    else:\n                        link_type = \"External\"\n                        external_links += 1\n                else:\n                    link_type = \"Relative\"\n                    internal_links += 1\n                \n                print(f\"\\n{i+1}. [{link_type}] '{link_text[:50]}{'...' if len(link_text) > 50 else ''}'\")\n                print(f\"    URL: {link_url[:80]}{'...' if len(link_url) > 80 else ''}\")\n            \n            print(f\"\\nüìä Link Classification (first 5):\")\n            print(f\"   Internal/Relative: {internal_links}\")\n            print(f\"   External: {external_links}\")\n            print(f\"   Archive links: {archive_links}\")\n            \n        else:\n            print(\"‚ùå No links with href found\")\n            print(\"üí° This might mean:\")\n            print(\"   - Page has no links\")\n            print(\"   - Links use different attributes\")\n            print(\"   - Page didn't load properly\")\n            \nexcept Exception as e:\n    print(f\"‚ùå Error finding links: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5b: Filter for historical document links**\n",
    "\n",
    "Let's find links that point to historical archives or documents:\n",
    "\n",
    "Copy this code:\n",
    "```python\n",
    "# Define domains that often contain historical documents\n",
    "historical_domains = ['archive.org', 'canadiana.ca', 'gutenberg.org', 'biographi.ca']\n",
    "\n",
    "# Filter links\n",
    "document_links = []\n",
    "for link in all_links:\n",
    "    href = link.get('href', '')\n",
    "    # Check if any historical domain is in the URL\n",
    "    for domain in historical_domains:\n",
    "        if domain in href:\n",
    "            document_links.append({\n",
    "                'text': link.get_text().strip(),\n",
    "                'url': href\n",
    "            })\n",
    "            break  # Don't add the same link twice\n",
    "\n",
    "print(f\"Historical document links found: {len(document_links)}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Filter for historical document links with validation\ntry:\n    if 'all_links' not in locals():\n        print(\"‚ùå Links not found - run the previous link extraction cell first\")\n    else:\n        # Define domains that often contain historical documents\n        historical_domains = ['archive.org', 'canadiana.ca', 'gutenberg.org', 'biographi.ca', 'bac-lac.gc.ca']\n        \n        # Filter links\n        document_links = []\n        domain_counts = {}\n        \n        for link in all_links:\n            href = link.get('href', '')\n            link_text = link.get_text().strip()\n            \n            # Check if any historical domain is in the URL\n            for domain in historical_domains:\n                if domain in href:\n                    document_links.append({\n                        'text': link_text,\n                        'url': href,\n                        'domain': domain\n                    })\n                    \n                    # Count by domain\n                    domain_counts[domain] = domain_counts.get(domain, 0) + 1\n                    break  # Don't add the same link twice\n        \n        print(f\"üèõÔ∏è Historical document analysis:\")\n        print(f\"   Total links checked: {len(all_links)}\")\n        print(f\"   Historical document links found: {len(document_links)}\")\n        \n        if document_links:\n            print(f\"\\nüìä By domain:\")\n            for domain, count in domain_counts.items():\n                print(f\"   {domain}: {count} links\")\n            \n            print(f\"\\nüìö Historical document links:\")\n            for i, link in enumerate(document_links[:5], 1):  # Show first 5\n                print(f\"\\n{i}. {link['text'][:60]}{'...' if len(link['text']) > 60 else ''}\")\n                print(f\"   Domain: {link['domain']}\")\n                print(f\"   URL: {link['url'][:80]}{'...' if len(link['url']) > 80 else ''}\")\n            \n            if len(document_links) > 5:\n                print(f\"\\n... and {len(document_links) - 5} more historical links\")\n                \n            # Validate link quality\n            valid_links = [link for link in document_links if link['text'] and len(link['text']) > 3]\n            print(f\"\\n‚úÖ Quality check: {len(valid_links)}/{len(document_links)} links have meaningful text\")\n            \n        else:\n            print(\"‚ùå No historical document links found\")\n            print(f\"üí° Searched for these domains: {', '.join(historical_domains)}\")\n            print(\"   This might mean:\")\n            print(\"   - This page doesn't link to major historical archives\")\n            print(\"   - Links use different URL structures\")\n            print(\"   - Need to add more domain patterns\")\n            \nexcept Exception as e:\n    print(f\"‚ùå Error filtering historical links: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5c: Display the historical links**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the historical document links we found\n",
    "print(\"Historical Document Links:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, link in enumerate(document_links, 1):\n",
    "    print(f\"{i}. {link['text']}\")\n",
    "    print(f\"   URL: {link['url']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ **Your Turn: Find PDF Links**\n",
    "\n",
    "Many historical documents are available as PDFs. Adapt the link-finding code to:\n",
    "\n",
    "1. Find all links that contain \".pdf\" in their href\n",
    "2. Store them in a list called `pdf_links`\n",
    "3. Print how many PDF links you found\n",
    "4. Display the first 3 PDF links\n",
    "\n",
    "Hint: Use `if '.pdf' in href:` to check for PDF links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Your exercise: Find PDF links\ntry:\n    if 'all_links' not in locals():\n        print(\"‚ùå Links not found - run the previous link extraction cell first\")\n    else:\n        # Find all links that contain \".pdf\" in their href\n        pdf_links = []\n        \n        for link in all_links:\n            href = link.get('href', '')\n            link_text = link.get_text().strip()\n            \n            if '.pdf' in href.lower():\n                pdf_links.append({\n                    'text': link_text,\n                    'url': href\n                })\n        \n        print(f\"üìÑ PDF Link Analysis:\")\n        print(f\"   Total links checked: {len(all_links)}\")\n        print(f\"   PDF links found: {len(pdf_links)}\")\n        \n        if pdf_links:\n            print(f\"\\n‚úÖ Found {len(pdf_links)} PDF documents!\")\n            \n            print(f\"\\nüìã First 3 PDF links:\")\n            for i, pdf in enumerate(pdf_links[:3], 1):\n                print(f\"\\n{i}. '{pdf['text'][:60]}{'...' if len(pdf['text']) > 60 else ''}'\")\n                print(f\"   URL: {pdf['url']}\")\n                \n                # Analyze URL for file type validation\n                if pdf['url'].lower().endswith('.pdf'):\n                    print(\"   ‚úÖ Direct PDF link\")\n                else:\n                    print(\"   ‚ö†Ô∏è  URL contains .pdf but doesn't end with .pdf\")\n            \n            if len(pdf_links) > 3:\n                print(f\"\\n... and {len(pdf_links) - 3} more PDF links\")\n                \n            # Quality validation\n            valid_pdfs = [pdf for pdf in pdf_links if pdf['text'] and len(pdf['text']) > 3]\n            print(f\"\\nüìä Quality check: {len(valid_pdfs)}/{len(pdf_links)} PDF links have meaningful text\")\n            \n        else:\n            print(\"‚ùå No PDF links found on this page\")\n            print(\"üí° This might mean:\")\n            print(\"   - Page doesn't link to PDF documents\")\n            print(\"   - PDFs are embedded differently\")\n            print(\"   - Links use different file extensions\")\n            print(\"   - Try looking for links with 'download', 'document', or 'file' in text\")\n            \n            # Alternative search\n            print(\"\\nüîç Searching for potential document links...\")\n            doc_keywords = ['download', 'document', 'file', 'report', 'manuscript']\n            potential_docs = []\n            \n            for link in all_links:\n                link_text = link.get_text().lower()\n                if any(keyword in link_text for keyword in doc_keywords):\n                    potential_docs.append(link.get_text().strip())\n            \n            if potential_docs:\n                print(f\"   Found {len(potential_docs)} links with document keywords:\")\n                for doc in potential_docs[:3]:\n                    print(f\"     - {doc[:60]}{'...' if len(doc) > 60 else ''}\")\n            else:\n                print(\"   No obvious document links found\")\n                \nexcept Exception as e:\n    print(f\"‚ùå Error finding PDF links: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Internet Archive Metadata\n",
    "\n",
    "Internet Archive documents have rich metadata. Let's learn to extract it systematically.\n",
    "\n",
    "**Step 6a: Get an Internet Archive page**\n",
    "\n",
    "Let's work with our Saskatchewan document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get Internet Archive page with validation\nia_url = \"https://archive.org/details/saskatchewan00sask\"\n\nprint(\"üèõÔ∏è Working with Internet Archive metadata...\")\nprint(f\"URL: {ia_url}\")\n\ntry:\n    # Use our respectful function with a bit longer delay for IA\n    ia_response = respectful_get(ia_url, delay=2)\n    \n    if ia_response:\n        print(f\"‚úÖ Status Code: {ia_response.status_code}\")\n        print(\"‚úÖ Internet Archive page loaded successfully!\")\n        \n        # Basic validation\n        if 'archive.org' in ia_response.url:\n            print(\"‚úÖ Confirmed: This is an Internet Archive page\")\n        else:\n            print(\"‚ö†Ô∏è  Warning: Response URL doesn't match Internet Archive\")\n            \n        # Check content size\n        content_size = len(ia_response.text)\n        print(f\"üìÑ Content size: {content_size:,} characters\")\n        \n        if content_size > 10000:\n            print(\"‚úÖ Substantial content received\")\n        else:\n            print(\"‚ö†Ô∏è  Warning: Less content than expected\")\n            \n    else:\n        print(\"‚ùå Failed to load Internet Archive page\")\n        print(\"üí° Possible issues:\")\n        print(\"   - Internet Archive servers busy\")\n        print(\"   - Network connectivity issues\")\n        print(\"   - Document no longer available\")\n        \nexcept Exception as e:\n    print(f\"‚ùå Error loading Internet Archive page: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6b: Extract the document title**\n",
    "\n",
    "Internet Archive puts document titles in `<h1>` tags:\n",
    "\n",
    "Copy this code:\n",
    "```python\n",
    "title = ia_soup.find('h1')\n",
    "if title:\n",
    "    document_title = title.get_text().strip()\n",
    "    print(f\"Document Title: {document_title}\")\n",
    "else:\n",
    "    print(\"No title found\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Extract Internet Archive title with validation\ntry:\n    if 'ia_response' not in locals() or not ia_response:\n        print(\"‚ùå Internet Archive response not available - run the previous cell first\")\n    else:\n        # Create soup for Internet Archive page\n        ia_soup = BeautifulSoup(ia_response.text, 'html.parser')\n        print(\"‚úÖ Internet Archive soup created\")\n        \n        # Find title - IA uses h1 for main document title\n        title_tag = ia_soup.find('h1')\n        \n        if title_tag:\n            document_title = title_tag.get_text().strip()\n            print(f\"üìö Document Title: {document_title}\")\n            \n            # Validate title quality\n            if len(document_title) > 5:\n                print(\"‚úÖ Title validation: Reasonable length\")\n            else:\n                print(\"‚ö†Ô∏è  Warning: Title seems very short\")\n                \n            # Check for expected keywords\n            title_lower = document_title.lower()\n            if 'saskatchewan' in title_lower:\n                print(\"‚úÖ Content validation: Saskatchewan document confirmed\")\n            else:\n                print(\"‚ö†Ô∏è  Content note: Title doesn't contain 'Saskatchewan'\")\n                print(f\"   This might be normal - could be a more specific title\")\n                \n            # Check for historical time indicators\n            historical_indicators = ['history', 'historical', '19', '18', 'century']\n            found_indicators = [ind for ind in historical_indicators if ind in title_lower]\n            if found_indicators:\n                print(f\"üìñ Historical indicators: {', '.join(found_indicators)}\")\n            \n        else:\n            print(\"‚ùå No h1 title tag found\")\n            print(\"üí° Trying alternative title methods...\")\n            \n            # Try alternative title extraction\n            title_alternatives = [\n                ia_soup.find('title'),  # HTML title tag\n                ia_soup.find('h2'),     # Secondary heading\n                ia_soup.find('div', class_='item-title')  # IA specific class\n            ]\n            \n            for alt_title in title_alternatives:\n                if alt_title:\n                    alt_text = alt_title.get_text().strip()\n                    if alt_text and len(alt_text) > 5:\n                        print(f\"üìö Alternative title found: {alt_text}\")\n                        break\n            else:\n                print(\"‚ùå No alternative titles found\")\n                \nexcept Exception as e:\n    print(f\"‚ùå Error extracting title: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6c: Find metadata fields**\n",
    "\n",
    "Internet Archive uses `<dt>` (definition term) and `<dd>` (definition description) tags for metadata:\n",
    "\n",
    "Copy this code:\n",
    "```python\n",
    "# Find metadata terms and values\n",
    "metadata_terms = ia_soup.find_all('dt')\n",
    "metadata_values = ia_soup.find_all('dd')\n",
    "\n",
    "print(f\"Metadata fields found: {len(metadata_terms)}\")\n",
    "print(f\"Metadata values found: {len(metadata_values)}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6d: Extract and display metadata**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store metadata\n",
    "metadata = {}\n",
    "\n",
    "# Pair up terms and values\n",
    "for term, value in zip(metadata_terms, metadata_values):\n",
    "    term_text = term.get_text().strip()\n",
    "    value_text = value.get_text().strip()\n",
    "    metadata[term_text] = value_text\n",
    "\n",
    "# Display key metadata\n",
    "print(\"Document Metadata:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Show specific metadata fields we care about\n",
    "important_fields = ['by', 'Publication date', 'Topics', 'Language']\n",
    "\n",
    "for field in important_fields:\n",
    "    if field in metadata:\n",
    "        value = metadata[field]\n",
    "        # Truncate very long values\n",
    "        if len(value) > 100:\n",
    "            value = value[:100] + \"...\"\n",
    "        print(f\"{field}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ **Your Turn: Create a Metadata Function**\n",
    "\n",
    "Now create a reusable function that extracts metadata from any Internet Archive document. Fill in the missing parts:\n",
    "\n",
    "```python\n",
    "def extract_ia_metadata(url):\n",
    "    \"\"\"Extract title and metadata from an Internet Archive document\"\"\"\n",
    "    # 1. Get the page with requests.get()\n",
    "    # 2. Create soup with BeautifulSoup\n",
    "    # 3. Extract title from h1 tag\n",
    "    # 4. Extract metadata from dt/dd tags\n",
    "    # 5. Return a dictionary with title and metadata\n",
    "```\n",
    "\n",
    "Test it with the University of Toronto annual report: `https://archive.org/details/annualreport191920nivuoft`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create a reusable Internet Archive metadata function\ndef extract_ia_metadata(url):\n    \"\"\"Extract title and metadata from an Internet Archive document\"\"\"\n    try:\n        print(f\"üîç Analyzing Internet Archive URL: {url}\")\n        \n        # Extract item ID from URL\n        if '/details/' in url:\n            item_id = url.split('/details/')[-1]\n            print(f\"üìã Item ID extracted: {item_id}\")\n        else:\n            return {'error': 'Invalid Internet Archive URL format'}\n        \n        # Method 1: Try Internet Archive Python library (preferred)\n        try:\n            import internetarchive as ia\n            item = ia.get_item(item_id)\n            \n            # Extract key metadata\n            metadata = {\n                'method': 'IA Python Library',\n                'title': item.metadata.get('title', 'No title'),\n                'creator': item.metadata.get('creator', 'No creator'),\n                'date': item.metadata.get('date', 'No date'),\n                'subject': item.metadata.get('subject', 'No subject'),\n                'description': item.metadata.get('description', 'No description')[:200] + '...' if item.metadata.get('description') else 'No description',\n                'language': item.metadata.get('language', 'No language'),\n                'files_count': len(list(item.files))\n            }\n            \n            print(\"‚úÖ Successfully extracted metadata using IA library\")\n            return metadata\n            \n        except ImportError:\n            print(\"‚ö†Ô∏è  IA library not available, falling back to web scraping...\")\n        except Exception as e:\n            print(f\"‚ö†Ô∏è  IA library failed ({e}), falling back to web scraping...\")\n        \n        # Method 2: Fallback to web scraping\n        response = respectful_get(url, delay=2)\n        if not response:\n            return {'error': 'Could not download page'}\n            \n        soup = BeautifulSoup(response.text, 'html.parser')\n        \n        # Extract metadata from HTML\n        title_tag = soup.find('h1')\n        title = title_tag.get_text().strip() if title_tag else 'No title found'\n        \n        # Try to find metadata fields\n        metadata_dict = {'method': 'Web Scraping', 'title': title}\n        \n        # Look for dt/dd metadata pairs\n        dt_tags = soup.find_all('dt')\n        dd_tags = soup.find_all('dd')\n        \n        for dt, dd in zip(dt_tags, dd_tags):\n            key = dt.get_text().strip().lower()\n            value = dd.get_text().strip()\n            \n            # Map common fields\n            if 'by' in key or 'creator' in key:\n                metadata_dict['creator'] = value\n            elif 'date' in key:\n                metadata_dict['date'] = value\n            elif 'topic' in key or 'subject' in key:\n                metadata_dict['subject'] = value\n            elif 'language' in key:\n                metadata_dict['language'] = value\n        \n        print(\"‚úÖ Successfully extracted metadata using web scraping\")\n        return metadata_dict\n        \n    except Exception as e:\n        print(f\"‚ùå Error extracting metadata: {e}\")\n        return {'error': str(e)}\n\n# Test the function with University of Toronto annual report\nuoft_url = \"https://archive.org/details/annualreport191920nivuoft\"\nprint(\"üéì Testing with University of Toronto Annual Report 1919-20...\")\n\nresult = extract_ia_metadata(uoft_url)\n\nprint(f\"\\nüìä Extracted Metadata:\")\nprint(\"=\" * 50)\nfor key, value in result.items():\n    print(f\"{key.title()}: {value}\")\n\nprint(f\"\\nüí° This function demonstrates two approaches:\")\nprint(f\"   1. Internet Archive Python library (preferred)\")\nprint(f\"   2. Web scraping with Beautiful Soup (fallback)\")\nprint(f\"   Real research projects should use both for robustness!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Final Challenge - Your Research Project\n",
    "\n",
    "Time to put it all together! Choose a Canadian historical source and conduct your own analysis.\n",
    "\n",
    "**Available sources:**\n",
    "- TPL Jesuit Relations blog: Research the historical quotes and themes\n",
    "- Saskatchewan History: Analyze the metadata and publication context\n",
    "- UofT Annual Report 1919-20: Extract institutional information\n",
    "\n",
    "**Your research steps:**\n",
    "1. Choose a source and research question\n",
    "2. Use the scraping techniques you've learned\n",
    "3. Extract specific information related to your question\n",
    "4. Present your findings"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Using Internet Archive library - much more efficient!\nprint(\"üèõÔ∏è Accessing Internet Archive with the Python library...\")\n\ntry:\n    # Get the Saskatchewan document using its identifier\n    item_id = \"saskatchewan00sask\"  # The ID from the URL\n    \n    # Get the item (this is much faster than web scraping)\n    item = ia.get_item(item_id)\n    \n    print(f\"‚úÖ Successfully retrieved item: {item_id}\")\n    print(f\"üìö Title: {item.metadata.get('title', 'No title')}\")\n    print(f\"üìÖ Date: {item.metadata.get('date', 'No date')}\")\n    print(f\"üë§ Creator: {item.metadata.get('creator', 'No creator')}\")\n    print(f\"üìñ Subject: {item.metadata.get('subject', 'No subject')}\")\n    \n    # Show available files\n    files = list(item.files)\n    print(f\"\\nüìÅ Available files: {len(files)}\")\n    \n    # Show first few files\n    for i, file in enumerate(files[:5]):\n        file_name = file.get('name', 'Unknown')\n        file_format = file.get('format', 'Unknown')\n        file_size = file.get('size', 'Unknown')\n        print(f\"   {i+1}. {file_name} ({file_format}) - {file_size} bytes\")\n    \n    if len(files) > 5:\n        print(f\"   ... and {len(files) - 5} more files\")\n        \n    print(f\"\\nüöÄ Much easier than HTML scraping!\")\n    print(f\"üí° The IA library gives us clean, structured data instantly\")\n    \nexcept Exception as e:\n    print(f\"‚ùå Error accessing Internet Archive: {e}\")\n    print(\"üí° This might be due to:\")\n    print(\"   - Network connectivity issues\")\n    print(\"   - Item ID changed or removed\")\n    print(\"   - Internet Archive servers busy\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Install and import Internet Archive library\n# First install the library (run this once)\n!pip install internetarchive --quiet\n\nprint(\"üìö Installing Internet Archive Python library...\")\nprint(\"‚úÖ Installation complete!\")\n\n# Import the library\ntry:\n    import internetarchive as ia\n    print(\"‚úÖ Internet Archive library imported successfully!\")\n    print(\"üîó This library provides direct, efficient access to IA collections\")\nexcept ImportError as e:\n    print(f\"‚ùå Error importing Internet Archive library: {e}\")\n    print(\"üí° Try running: pip install internetarchive\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 8: Introduction to Internet Archive Python Library\n\nWhile Beautiful Soup is excellent for scraping web pages, the Internet Archive provides a specialized Python library that makes accessing their collections much more efficient. This is perfect for large-scale historical research projects.\n\n**Why use the Internet Archive library?**\n- Direct access to metadata without scraping HTML\n- Faster downloads and better error handling\n- Access to full-text search capabilities\n- Bulk processing of large collections\n- Respects Internet Archive's preferred access methods",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Final Challenge: Your Historical Research Project\n\n# Available Canadian historical sources:\nsources = {\n    \"TPL Jesuit Relations\": {\n        \"url\": \"https://torontopubliclibrary.typepad.com/local-history-genealogy/2020/01/sainte-marie-among-the-hurons-selections-from-the-jesuit-relations-and-allied-documents.html\",\n        \"type\": \"Blog post with historical quotes\",\n        \"research_questions\": [\n            \"What themes appear in historical quotes?\",\n            \"How many external historical links are provided?\",\n            \"What time periods are mentioned?\"\n        ]\n    },\n    \"Saskatchewan History\": {\n        \"url\": \"https://archive.org/details/saskatchewan00sask\",\n        \"type\": \"Internet Archive document\",\n        \"research_questions\": [\n            \"What metadata is available about publication?\",\n            \"What file formats are provided?\",\n            \"Who was the creator/publisher?\"\n        ]\n    },\n    \"UofT Annual Report 1919-20\": {\n        \"url\": \"https://archive.org/details/annualreport191920nivuoft\",\n        \"type\": \"Institutional document\",\n        \"research_questions\": [\n            \"What institutional information is captured?\",\n            \"How is the document structured?\",\n            \"What historical context does it provide?\"\n        ]\n    }\n}\n\n# Choose your research project\nprint(\"üî¨ Historical Web Scraping Research Project\")\nprint(\"=\" * 50)\n\nprint(\"üìö Available sources:\")\nfor i, (name, info) in enumerate(sources.items(), 1):\n    print(f\"\\n{i}. {name}\")\n    print(f\"   Type: {info['type']}\")\n    print(f\"   URL: {info['url'][:60]}...\")\n    print(f\"   Sample questions:\")\n    for q in info['research_questions']:\n        print(f\"     - {q}\")\n\nprint(f\"\\nüéØ Your research steps:\")\nprint(\"1. Choose a source and research question\")\nprint(\"2. Use appropriate scraping techniques\")\nprint(\"3. Extract and validate data\") \nprint(\"4. Analyze and present findings\")\n\n# Example research project - customize this!\nchosen_source = \"TPL Jesuit Relations\"  # Change this to your choice\nmy_url = sources[chosen_source][\"url\"]\nmy_question = \"What historical themes appear in the quoted text?\"\n\nprint(f\"\\nüìã Example Research Project:\")\nprint(f\"Source: {chosen_source}\")\nprint(f\"Question: {my_question}\")\nprint(f\"URL: {my_url}\")\n\n# Conduct the research\nprint(f\"\\nüîç Conducting research...\")\n\ntry:\n    # Step 1: Get the page\n    response = respectful_get(my_url)\n    \n    if response:\n        soup = BeautifulSoup(response.text, 'html.parser')\n        print(\"‚úÖ Page successfully scraped\")\n        \n        # Step 2: Extract relevant content (customize based on your question)\n        if \"themes\" in my_question.lower():\n            # Look for quotes and analyze themes\n            blockquotes = soup.find_all('blockquote')\n            paragraphs = soup.find_all('p')\n            \n            print(f\"\\nüìä Content Analysis:\")\n            print(f\"   Blockquotes found: {len(blockquotes)}\")\n            print(f\"   Paragraphs found: {len(paragraphs)}\")\n            \n            # Analyze themes in text\n            all_text = soup.get_text().lower()\n            \n            # Historical themes to look for\n            themes = {\n                'Religious': ['god', 'lord', 'jesus', 'prayer', 'church', 'faith'],\n                'Indigenous peoples': ['huron', 'savage', 'indian', 'native', 'tribe'],\n                'French colonial': ['french', 'france', 'jesuit', 'missionary'],\n                'Geographic': ['canada', 'new france', 'sainte-marie', 'ontario'],\n                'Temporal': ['1600', '1640', '1650', '17th century', 'century']\n            }\n            \n            theme_counts = {}\n            for theme_name, keywords in themes.items():\n                count = sum(all_text.count(keyword) for keyword in keywords)\n                if count > 0:\n                    theme_counts[theme_name] = count\n            \n            print(f\"\\nüé≠ Historical Themes Found:\")\n            for theme, count in sorted(theme_counts.items(), key=lambda x: x[1], reverse=True):\n                print(f\"   {theme}: {count} mentions\")\n        \n        # Step 3: Present findings\n        print(f\"\\nüìù Research Findings:\")\n        print(\"1. Successfully scraped Canadian historical content\")\n        print(\"2. Identified multiple historical themes in source material\")\n        print(\"3. Demonstrated effective web scraping for historical research\")\n        \n        if theme_counts:\n            most_common = max(theme_counts.items(), key=lambda x: x[1])\n            print(f\"4. Most prominent theme: {most_common[0]} ({most_common[1]} mentions)\")\n        \n    else:\n        print(\"‚ùå Could not complete research - page unavailable\")\n        \nexcept Exception as e:\n    print(f\"‚ùå Research error: {e}\")\n\n# Your turn!\nprint(f\"\\nüöÄ Now it's your turn!\")\nprint(\"Customize this code for your own research question:\")\nprint(\"1. Change 'chosen_source' to your preferred source\")\nprint(\"2. Modify 'my_question' to your research interest\") \nprint(\"3. Adapt the analysis code for your specific question\")\nprint(\"4. Add your own themes, keywords, or analysis methods\")\n\nprint(f\"\\nüí° Remember to:\")\nprint(\"- Validate your scraped data\")\nprint(\"- Handle errors gracefully\")\nprint(\"- Respect website terms of service\")\nprint(\"- Cite your digital sources properly\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: What You've Learned\n",
    "\n",
    "üéâ **Congratulations!** You've mastered the fundamentals of web scraping for historical research:\n",
    "\n",
    "**Technical Skills:**\n",
    "- ‚úÖ Making web requests with `requests.get()`\n",
    "- ‚úÖ Parsing HTML with Beautiful Soup\n",
    "- ‚úÖ Targeting specific elements (`find`, `find_all`)\n",
    "- ‚úÖ Extracting text, links, and metadata\n",
    "- ‚úÖ Building reusable functions for research\n",
    "\n",
    "**Historical Sources:**\n",
    "- ‚úÖ Blog posts with embedded historical content\n",
    "- ‚úÖ Internet Archive documents with metadata\n",
    "- ‚úÖ Academic indexes and structured data\n",
    "\n",
    "**Research Methods:**\n",
    "- ‚úÖ Systematic content extraction\n",
    "- ‚úÖ Metadata analysis for document context\n",
    "- ‚úÖ Building reproducible research workflows\n",
    "\n",
    "**Next Steps:**\n",
    "In Notebook 3, you'll learn advanced text analysis techniques to find patterns in the historical data you've scraped. We'll also explore the Internet Archive Python library for more efficient access to large collections.\n",
    "\n",
    "**Remember:**\n",
    "- Always respect robots.txt and website terms of service\n",
    "- Cite your digital sources properly\n",
    "- Consider the limitations and context of digitized materials\n",
    "- Use these skills responsibly for legitimate research purposes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}