{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tjp-E2UxtKdX"
      },
      "source": [
        "# **Notebook 2: Web Scraping for Historians**\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jburnford/hist497_2025/blob/main/02_web_scraping.ipynb)\n",
        "\n",
        "Welcome to web scraping! In this notebook, you'll learn to extract historical data from websites using Python. We'll work with real Canadian historical sources and build your skills step by step.\n",
        "\n",
        "**What you'll learn:**\n",
        "- How web pages are structured (HTML basics)\n",
        "- Using Beautiful Soup to extract specific content\n",
        "- Working with Canadian historical archives\n",
        "- Building reusable code for research\n",
        "\n",
        "**Ethics first:** Always respect websites. Check robots.txt files, don't overload servers, and respect copyright.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELhjjAF8tKdZ"
      },
      "source": [
        "## Step 1: Setting Up Our Tools\n",
        "\n",
        "Before we can scrape websites, we need to install and import the right libraries. Let's do this step by step.\n",
        "\n",
        "**First, install the libraries:**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: How to add delays between requests\n",
        "import time\n",
        "\n",
        "# Friendly User-Agent so cultural institutions know who is visiting\n",
        "DEFAULT_HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (compatible; HIST497-Student/1.0; +https://github.com/jburnford/hist497_2025)\"\n",
        "}\n",
        "\n",
        "def respectful_get(url, delay=1, headers=None):\n",
        "    \"\"\"\n",
        "    Make a web request with built-in delay for respectful scraping\n",
        "    \"\"\"\n",
        "    print(f\"‚è≥ Waiting {delay} second(s) before request...\")\n",
        "    time.sleep(delay)  # Wait before making request\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers or DEFAULT_HEADERS, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        print(f\"‚úÖ Request successful\")\n",
        "        return response\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"‚ùå Request failed: {e}\")\n",
        "        return None\n",
        "\n",
        "# We'll use this function for respectful scraping throughout the notebook\n",
        "print(\"Respectful scraping function defined!\")\n",
        "print(\"üí° This adds delays between requests to be considerate to servers and sends a helpful User-Agent.\")\n"
      ],
      "metadata": {
        "id": "GE4P4FP8tKdZ",
        "outputId": "65a9fcb9-7c9d-42a2-c27f-8386944bf7c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Respectful scraping function defined!\n",
            "üí° This adds delays between requests to be considerate to servers and sends a helpful User-Agent.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ü§ù Ethical Web Scraping Guidelines\n",
        "\n",
        "Before we start scraping, let's understand the ethics and best practices:\n",
        "\n",
        "**‚úÖ Always:**\n",
        "- Check robots.txt (add /robots.txt to any website URL)\n",
        "- Add delays between requests (don't overwhelm servers)\n",
        "- Use reasonable timeouts\n",
        "- Respect copyright and terms of service\n",
        "- Identify yourself with User-Agent headers when appropriate\n",
        "\n",
        "**‚ùå Never:**\n",
        "- Scrape faster than a human could browse\n",
        "- Ignore error messages or blocks\n",
        "- Scrape personal/private information\n",
        "- Violate website terms of service\n",
        "- Overload servers with rapid requests\n",
        "\n",
        "**üìñ For historical research:**\n",
        "- Many archives encourage responsible academic use\n",
        "- Always cite your digital sources properly\n",
        "- Consider contacting archives for bulk data access\n",
        "- Respect cultural sensitivities in historical materials"
      ],
      "metadata": {
        "id": "zmNRUYvatKda"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "qjIWIdZAtKdb",
        "outputId": "fe566064-3f5e-4090-f825-47c67491bbfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries installed successfully!\n"
          ]
        }
      ],
      "source": [
        "# This installs the libraries we need\n",
        "!pip install requests beautifulsoup4 --quiet\n",
        "print(\"Libraries installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JWFkFH3tKdb"
      },
      "source": [
        "**Now, import them so we can use them:**\n",
        "\n",
        "Copy this code into the cell below:\n",
        "```python\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "```\n",
        "If you don't get an error and nothing happens after you click the run button, it worked."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "nRS-pfRLtKdc",
        "outputId": "e279808e-9472-4922-ec61-4bb2958d3fa4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported successfully!\n",
            "‚úÖ requests: Downloads web pages\n",
            "‚úÖ BeautifulSoup: Parses HTML content\n"
          ]
        }
      ],
      "source": [
        "# Import the libraries we need for web scraping\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(\"‚úÖ requests: Downloads web pages\")\n",
        "print(\"‚úÖ BeautifulSoup: Parses HTML content\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6N-MBvRtKdc"
      },
      "source": [
        "## Step 2: Your First Web Request\n",
        "\n",
        "Let's start by downloading a web page. We'll use a Saskatchewan encyclopedia article about the Rupert's Land purchase.\n",
        "\n",
        "Libraries sometimes block anonymous scraping, so we send a friendly `User-Agent` string that identifies this class.\n",
        "\n",
        "**Step 2a: Define the URL**\n",
        "\n",
        "Copy this code:\n",
        "```python\n",
        "url = \"https://esask.uregina.ca/entry/ruperts_land_purchase.html\"\n",
        "print(f\"We're going to scrape: {url}\")\n",
        "print(\"üìö This is an Encyclopedia of Saskatchewan article about the Rupert's Land purchase\")\n",
        "print(\"üèîÔ∏è Great practice for prairie-focused historical research!\")\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "RDmpyX-ItKdd",
        "outputId": "78ea58fd-13ab-447b-88da-d8d1b0db832b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We're going to scrape: https://esask.uregina.ca/entry/ruperts_land_purchase.html\n",
            "üìö This is an Encyclopedia of Saskatchewan article about the Rupert's Land purchase\n",
            "üèîÔ∏è Great practice for prairie-focused historical research!\n"
          ]
        }
      ],
      "source": [
        "# Define the URL for our Canadian historical source\n",
        "url = \"https://esask.uregina.ca/entry/ruperts_land_purchase.html\"\n",
        "print(f\"We're going to scrape: {url}\")\n",
        "print(\"üìö This is an Encyclopedia of Saskatchewan article about the Rupert's Land purchase\")\n",
        "print(\"üèîÔ∏è Great practice for prairie-focused historical research!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2elztToztKde"
      },
      "source": [
        "**Step 2b: Download the page**\n",
        "\n",
        "Now let's actually download the web page. The `requests.get()` function fetches the page for us. We include the same `DEFAULT_HEADERS` so the site recognizes us as a respectful browser.\n",
        "\n",
        "Copy this code:\n",
        "```python\n",
        "response = requests.get(url, headers=DEFAULT_HEADERS)\n",
        "print(f\"Status Code: {response.status_code}\")  # 200 means success\n",
        "print(f\"Page downloaded! It contains {len(response.text)} characters.\")\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzQcDssotKde"
      },
      "outputs": [],
      "source": [
        "# Download the page with error handling\n",
        "try:\n",
        "    response = requests.get(url, headers=DEFAULT_HEADERS, timeout=10)  # 10 second timeout\n",
        "    response.raise_for_status()  # Raises an exception for bad status codes\n",
        "    print(f\"‚úÖ Success! Status Code: {response.status_code}\")\n",
        "    print(f\"üìÑ Page downloaded! It contains {len(response.text):,} characters.\")\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"‚ùå Error downloading page: {e}\")\n",
        "    print(\"üí° This could be due to:\")\n",
        "    print(\"   - No internet connection\")\n",
        "    print(\"   - Website is down\")\n",
        "    print(\"   - URL has changed\")\n",
        "    print(\"   - Server is blocking requests\")\n",
        "    print(\"   - Try sending headers=DEFAULT_HEADERS to share a friendly User-Agent\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqJRsfnBtKde"
      },
      "source": [
        "**Step 2c: Compare with an Internet Archive Page**\n",
        "\n",
        "Let's try a different kind of site. We'll grab the Saskatchewan Internet Archive item and peek at its raw HTML the same way.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUf3R1ittKde"
      },
      "outputs": [],
      "source": [
        "# Show the first 500 characters so we can inspect the HTML\n",
        "print(\"üìÑ Previewing Encyclopedia of Saskatchewan HTML...\")\n",
        "print(\"-\" * 50)\n",
        "print(response.text[:500])\n",
        "print(\"-\" * 50)\n",
        "print(\"üí° Notice header navigation, metadata, and the main article content.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4NvbBoQtKdf"
      },
      "source": [
        "### üîÑ **Your Turn: Adaptation Exercise**\n",
        "\n",
        "Now try the same steps with a different Canadian source. Copy the code from above and modify it to use this Internet Archive document about Saskatchewan:\n",
        "\n",
        "```python\n",
        "url = \"https://archive.org/details/saskatchewan00sask\"\n",
        "```\n",
        "\n",
        "Follow the same steps: define the URL, download the page with `headers=DEFAULT_HEADERS`, check the status code, and look at the first 500 characters using `print(response.text[:500])`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7R1rSr7ZtKdf"
      },
      "outputs": [],
      "source": [
        "# Your adaptation exercise here\n",
        "# 1. Define the Saskatchewan URL\n",
        "# 2. Download the page with requests.get()\n",
        "# 3. Print the status code and character count\n",
        "# 4. Show first 500 characters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wyU3Nr4tKdf"
      },
      "source": [
        "## Step 3: Making Sense of HTML with Beautiful Soup\n",
        "\n",
        "Raw HTML is hard to work with. Beautiful Soup parses it and makes it easy to extract what we need.\n",
        "\n",
        "**Step 3a: Create your first \"soup\"**\n",
        "\n",
        "Let's go back to our Encyclopedia article and parse it properly:\n",
        "\n",
        "Copy this code:\n",
        "```python\n",
        "# Go back to the Encyclopedia article\n",
        "url = \"https://torontopubliclibrary.typepad.com/local-history-genealogy/2020/01/sainte-marie-among-the-hurons-selections-from-the-jesuit-relations-and-allied-documents.html\"\n",
        "response = requests.get(url, headers=DEFAULT_HEADERS)\n",
        "\n",
        "# Create the soup object\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "print(\"Soup object created! Now we can easily extract content.\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEHLKgNNtKdf"
      },
      "outputs": [],
      "source": [
        "# Create soup object with validation\n",
        "try:\n",
        "    # Use our respectful function\n",
        "    response = respectful_get(url)\n",
        "\n",
        "    if response:\n",
        "        # Create the soup object\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        print(\"‚úÖ Soup object created successfully!\")\n",
        "\n",
        "        # Validate we got HTML content\n",
        "        if soup.find('html'):\n",
        "            print(\"‚úÖ Valid HTML structure detected\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è  Warning: No HTML structure found - might be plain text\")\n",
        "\n",
        "        # Check if we got the expected content\n",
        "        if len(soup.get_text().strip()) > 100:\n",
        "            print(f\"‚úÖ Content validation: {len(soup.get_text().strip()):,} characters of text\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è  Warning: Very little text content found\")\n",
        "    else:\n",
        "        print(\"‚ùå Could not create soup - request failed\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error creating soup: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWBiqZgmtKdf"
      },
      "source": [
        "**Step 3b: Extract the page title**\n",
        "\n",
        "Let's start simple by getting the page title. In HTML, the title is in `<title>` tags.\n",
        "\n",
        "Copy this code:\n",
        "```python\n",
        "title = soup.find('title')\n",
        "print(f\"Page title: {title.get_text()}\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyIJGfjstKdg"
      },
      "outputs": [],
      "source": [
        "# Extract title with validation\n",
        "try:\n",
        "    title_tag = soup.find('title')\n",
        "\n",
        "    if title_tag:\n",
        "        title_text = title_tag.get_text().strip()\n",
        "        print(f\"‚úÖ Page title found: {title_text}\")\n",
        "\n",
        "        # Validate the title makes sense\n",
        "        if len(title_text) > 5:\n",
        "            print(\"‚úÖ Title validation: Reasonable length\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è  Warning: Title seems very short\")\n",
        "\n",
        "        # Check if it's related to our expected content\n",
        "        if any(keyword in title_text.lower() for keyword in ['rupert', 'hudson', 'company', 'purchase', 'saskatchewan']):\n",
        "            print(\"‚úÖ Content validation: Title matches expected historical topic\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è  Warning: Title doesn't match expected content - check URL\")\n",
        "    else:\n",
        "        print(\"‚ùå No title tag found\")\n",
        "        print(\"üí° This might mean:\")\n",
        "        print(\"   - Page structure is different than expected\")\n",
        "        print(\"   - We got redirected to a different page\")\n",
        "        print(\"   - Page failed to load properly\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error extracting title: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCqqwEy1tKdg"
      },
      "source": [
        "**Step 3c: Get clean text (no HTML tags)**\n",
        "\n",
        "The `.get_text()` method removes all HTML tags and gives us just the readable content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1raLFnAtKdg"
      },
      "outputs": [],
      "source": [
        "# Extract all text content without HTML tags\n",
        "try:\n",
        "    if 'soup' not in locals():\n",
        "        print(\"‚ùå Soup object not found - run the previous Beautiful Soup cell first\")\n",
        "    else:\n",
        "        clean_text = soup.get_text()\n",
        "        print(f\"Clean text length: {len(clean_text)} characters\")\n",
        "        print()\n",
        "        print(\"First 500 characters of clean text:\")\n",
        "        print(clean_text[:500])\n",
        "        print()\n",
        "        print(\"Much better! Now we can read the actual content.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error extracting clean text: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYTddMDvtKdg"
      },
      "source": [
        "### üîÑ **Your Turn: Practice with Internet Archive**\n",
        "\n",
        "Now apply the same Beautiful Soup steps to the Saskatchewan document. Copy and adapt the code above:\n",
        "\n",
        "1. Create a soup object from the Saskatchewan URL\n",
        "2. Extract and print the title\n",
        "3. Get the clean text and show the first 500 characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWZkp0kHtKdg"
      },
      "outputs": [],
      "source": [
        "# Your practice: Apply Beautiful Soup to Saskatchewan document\n",
        "print(\"üçÅ Analyzing Saskatchewan document with Beautiful Soup...\")\n",
        "\n",
        "try:\n",
        "    # Create soup object for Saskatchewan document\n",
        "    sask_soup = BeautifulSoup(sask_response.text, 'html.parser')\n",
        "    print(\"‚úÖ Saskatchewan soup created!\")\n",
        "\n",
        "    # Extract and print the title\n",
        "    sask_title = sask_soup.find('title')\n",
        "    if sask_title:\n",
        "        title_text = sask_title.get_text().strip()\n",
        "        print(f\"üìö Title: {title_text}\")\n",
        "\n",
        "        # Validate it's an Internet Archive page\n",
        "        if 'archive.org' in title_text.lower() or 'saskatchewan' in title_text.lower():\n",
        "            print(\"‚úÖ Confirmed: This is the Saskatchewan document\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è  Title doesn't match expectations\")\n",
        "    else:\n",
        "        print(\"‚ùå No title found\")\n",
        "\n",
        "    # Get clean text and show first 500 characters\n",
        "    clean_text = sask_soup.get_text()\n",
        "    print(f\"\\nüìÑ Clean text length: {len(clean_text):,} characters\")\n",
        "    print(\"\\nFirst 500 characters of clean text:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(clean_text[:500])\n",
        "    print(\"-\" * 50)\n",
        "    print(\"üí° Much more readable than raw HTML!\")\n",
        "\n",
        "except NameError:\n",
        "    print(\"‚ùå Saskatchewan response not available - run the previous exercise first\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dw6u9m8ctKdh"
      },
      "source": [
        "## Step 4: Targeting Specific Content\n",
        "\n",
        "Getting all the text is useful, but often we want specific parts. Let's learn to target particular HTML elements.\n",
        "\n",
        "**Step 4a: Find all paragraphs**\n",
        "\n",
        "Blog posts organize content in paragraphs (`<p>` tags). Let's find them:\n",
        "\n",
        "Copy this code:\n",
        "```python\n",
        "# Go back to our Encyclopedia article soup\n",
        "url = \"https://torontopubliclibrary.typepad.com/local-history-genealogy/2020/01/sainte-marie-among-the-hurons-selections-from-the-jesuit-relations-and-allied-documents.html\"\n",
        "response = requests.get(url, headers=DEFAULT_HEADERS)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Find all paragraph tags\n",
        "paragraphs = soup.find_all('p')\n",
        "print(f\"Found {len(paragraphs)} paragraphs\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRbl_KsZtKdh"
      },
      "outputs": [],
      "source": [
        "# Find all paragraphs with validation\n",
        "try:\n",
        "    # Make sure we have our soup object\n",
        "    if 'soup' not in locals():\n",
        "        print(\"‚ùå Soup object not found - please run the previous Beautiful Soup cells first\")\n",
        "    else:\n",
        "        # Find all paragraph tags\n",
        "        paragraphs = soup.find_all('p')\n",
        "        print(f\"üìù Found {len(paragraphs)} paragraph tags\")\n",
        "\n",
        "        # Validate we found reasonable content\n",
        "        if len(paragraphs) > 0:\n",
        "            print(\"‚úÖ Paragraph extraction successful\")\n",
        "\n",
        "            # Show some examples\n",
        "            print(f\"\\nAnalyzing first 3 paragraphs:\")\n",
        "            for i in range(min(3, len(paragraphs))):\n",
        "                para_text = paragraphs[i].get_text().strip()\n",
        "                if para_text:  # Only show non-empty paragraphs\n",
        "                    print(f\"\\nParagraph {i+1} ({len(para_text)} chars):\")\n",
        "                    print(f\"'{para_text[:100]}{'...' if len(para_text) > 100 else ''}'\")\n",
        "                else:\n",
        "                    print(f\"\\nParagraph {i+1}: (empty)\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è  Warning: No paragraphs found\")\n",
        "            print(\"üí° This might mean:\")\n",
        "            print(\"   - Page uses different HTML structure\")\n",
        "            print(\"   - Content is in different tags (div, article, etc.)\")\n",
        "            print(\"   - Page didn't load properly\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error finding paragraphs: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJBPWwC_tKdh"
      },
      "source": [
        "**Step 4b: Look at individual paragraphs**\n",
        "\n",
        "Let's examine a few paragraphs to see what we're working with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9HpLhMWtKdh"
      },
      "outputs": [],
      "source": [
        "# Look at the first few paragraphs with validation\n",
        "try:\n",
        "    if 'paragraphs' not in locals():\n",
        "        print(\"‚ùå Paragraphs not found - run the paragraph extraction cell first\")\n",
        "    elif not paragraphs:\n",
        "        print(\"‚ö†Ô∏è  No paragraph tags available to display\")\n",
        "    else:\n",
        "        print(\"First 3 paragraphs:\")\n",
        "        for i in range(min(3, len(paragraphs))):\n",
        "            para_text = paragraphs[i].get_text().strip()\n",
        "            preview = para_text[:100] + ('...' if len(para_text) > 100 else '')\n",
        "            print()\n",
        "            print(f\"Paragraph {i+1}: {preview}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error previewing paragraphs: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eq-dfTrgtKdh"
      },
      "source": [
        "**Step 4c: Filter for substantial paragraphs**\n",
        "\n",
        "Many paragraphs are short or empty. Let's filter for substantial ones (more than 50 characters):\n",
        "\n",
        "Copy this code:\n",
        "```python\n",
        "# Filter for paragraphs with meaningful content\n",
        "substantial_paras = []\n",
        "for para in paragraphs:\n",
        "    text = para.get_text().strip()\n",
        "    if len(text) > 50:  # Only paragraphs with more than 50 characters\n",
        "        substantial_paras.append(text)\n",
        "\n",
        "print(f\"Substantial paragraphs: {len(substantial_paras)}\")\n",
        "print(f\"\\nFirst substantial paragraph:\")\n",
        "print(substantial_paras[0])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2Gk6T3ttKdi"
      },
      "outputs": [],
      "source": [
        "# Filter for substantial paragraphs with validation\n",
        "try:\n",
        "    if 'paragraphs' not in locals():\n",
        "        print(\"‚ùå Paragraphs not found - run the previous cell first\")\n",
        "    else:\n",
        "        # Filter for paragraphs with meaningful content\n",
        "        substantial_paras = []\n",
        "        empty_count = 0\n",
        "\n",
        "        for para in paragraphs:\n",
        "            text = para.get_text().strip()\n",
        "            if len(text) > 50:  # Only paragraphs with more than 50 characters\n",
        "                substantial_paras.append(text)\n",
        "            elif len(text) == 0:\n",
        "                empty_count += 1\n",
        "\n",
        "        print(f\"üìä Paragraph Analysis:\")\n",
        "        print(f\"   Total paragraphs found: {len(paragraphs)}\")\n",
        "        print(f\"   Substantial paragraphs (>50 chars): {len(substantial_paras)}\")\n",
        "        print(f\"   Empty paragraphs: {empty_count}\")\n",
        "        print(f\"   Short paragraphs (<50 chars): {len(paragraphs) - len(substantial_paras) - empty_count}\")\n",
        "\n",
        "        if substantial_paras:\n",
        "            print(f\"\\nüìñ First substantial paragraph:\")\n",
        "            print(\"-\" * 60)\n",
        "            print(substantial_paras[0])\n",
        "            print(\"-\" * 60)\n",
        "\n",
        "            # Validate content quality\n",
        "            first_para = substantial_paras[0].lower()\n",
        "            if any(word in first_para for word in ['jesuit', 'huron', 'sainte-marie', 'canada', 'history']):\n",
        "                print(\"‚úÖ Content validation: Found expected historical keywords\")\n",
        "            else:\n",
        "                print(\"‚ö†Ô∏è  Content note: No obvious historical keywords found\")\n",
        "        else:\n",
        "            print(\"‚ùå No substantial paragraphs found\")\n",
        "            print(\"üí° This might mean the content is structured differently\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error filtering paragraphs: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K44911JjtKdi"
      },
      "source": [
        "### üîÑ **Your Turn: Find Historical Quotes**\n",
        "\n",
        "Historical articles often include quotes in special `<blockquote>` tags. Adapt the code above to:\n",
        "\n",
        "1. Find all blockquote elements using `soup.find_all('blockquote')`\n",
        "2. Extract their text and print them\n",
        "3. Count how many historical quotes you found\n",
        "\n",
        "Use the same Encyclopedia article soup object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "se3t4ukOtKdi"
      },
      "outputs": [],
      "source": [
        "# Your exercise: Find historical quotes in blockquotes\n",
        "try:\n",
        "    if 'soup' not in locals():\n",
        "        print(\"‚ùå Soup object not found - run the Beautiful Soup creation cells first\")\n",
        "    else:\n",
        "        # Find all blockquote elements\n",
        "        blockquotes = soup.find_all('blockquote')\n",
        "        print(f\"üìú Found {len(blockquotes)} blockquote elements\")\n",
        "\n",
        "        if blockquotes:\n",
        "            print(\"‚úÖ Historical quotes found!\")\n",
        "            print(\"\\nüîç Analyzing each quote:\")\n",
        "\n",
        "            valid_quotes = []\n",
        "            for i, quote in enumerate(blockquotes, 1):\n",
        "                quote_text = quote.get_text().strip()\n",
        "\n",
        "                if quote_text and len(quote_text) > 10:  # Filter out very short quotes\n",
        "                    valid_quotes.append(quote_text)\n",
        "                    print(f\"\\nQuote {i} ({len(quote_text)} characters):\")\n",
        "                    print(\"-\" * 50)\n",
        "                    # Show first 200 characters of quote\n",
        "                    display_text = quote_text[:200] + \"...\" if len(quote_text) > 200 else quote_text\n",
        "                    print(display_text)\n",
        "                    print(\"-\" * 50)\n",
        "\n",
        "                    # Check for historical indicators\n",
        "                    quote_lower = quote_text.lower()\n",
        "                    historical_indicators = ['rupert', 'hudson', 'company', 'indigenous', 'metis', 'cree', 'treaty', '1869', '1870']\n",
        "                    found_indicators = [ind for ind in historical_indicators if ind in quote_lower]\n",
        "\n",
        "                    if found_indicators:\n",
        "                        print(f\"üìö Historical indicators found: {', '.join(found_indicators)}\")\n",
        "                    else:\n",
        "                        print(\"üìù No obvious historical indicators\")\n",
        "                else:\n",
        "                    print(f\"Quote {i}: (too short or empty)\")\n",
        "\n",
        "            print(f\"\\nüìä Summary:\")\n",
        "            print(f\"   Total blockquotes: {len(blockquotes)}\")\n",
        "            print(f\"   Valid historical quotes: {len(valid_quotes)}\")\n",
        "\n",
        "            if valid_quotes:\n",
        "                avg_length = sum(len(q) for q in valid_quotes) / len(valid_quotes)\n",
        "                print(f\"   Average quote length: {avg_length:.0f} characters\")\n",
        "\n",
        "        else:\n",
        "            print(\"‚ùå No blockquotes found on this page\")\n",
        "            print(\"üí° This might mean:\")\n",
        "            print(\"   - This site doesn't use blockquotes for quotes\")\n",
        "            print(\"   - Quotes might be in other tags (div, p with special classes)\")\n",
        "            print(\"   - Page structure is different than expected\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error finding quotes: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoqpO6OotKdi"
      },
      "source": [
        "## Step 5: Working with Links\n",
        "\n",
        "Historical sources often link to primary documents. Let's learn to extract and analyze links.\n",
        "\n",
        "**Step 5a: Find all links**\n",
        "\n",
        "Links are in `<a>` tags with `href` attributes. Let's find them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52XKQVzntKdi"
      },
      "outputs": [],
      "source": [
        "# Find all links with validation\n",
        "try:\n",
        "    if 'soup' not in locals():\n",
        "        print(\"‚ùå Soup object not found - run the Beautiful Soup creation cells first\")\n",
        "    else:\n",
        "        # Find all links with href attributes\n",
        "        all_links = soup.find_all('a', href=True)\n",
        "        print(f\"üîó Found {len(all_links)} links with href attributes\")\n",
        "\n",
        "        if all_links:\n",
        "            print(\"‚úÖ Link extraction successful\")\n",
        "\n",
        "            # Analyze link types\n",
        "            internal_links = 0\n",
        "            external_links = 0\n",
        "            archive_links = 0\n",
        "\n",
        "            print(\"\\nüîç Analyzing first 5 links:\")\n",
        "            for i, link in enumerate(all_links[:5]):\n",
        "                link_text = link.get_text().strip()\n",
        "                link_url = link.get('href')\n",
        "\n",
        "                # Classify link type\n",
        "                if link_url.startswith('http'):\n",
        "                    if 'esask.uregina.ca' in link_url:\n",
        "                        link_type = \"Internal\"\n",
        "                        internal_links += 1\n",
        "                    elif 'archive.org' in link_url:\n",
        "                        link_type = \"Archive\"\n",
        "                        archive_links += 1\n",
        "                    else:\n",
        "                        link_type = \"External\"\n",
        "                        external_links += 1\n",
        "                else:\n",
        "                    link_type = \"Relative\"\n",
        "                    internal_links += 1\n",
        "\n",
        "                print(f\"\\n{i+1}. [{link_type}] '{link_text[:50]}{'...' if len(link_text) > 50 else ''}'\")\n",
        "                print(f\"    URL: {link_url[:80]}{'...' if len(link_url) > 80 else ''}\")\n",
        "\n",
        "            print(f\"\\nüìä Link Classification (first 5):\")\n",
        "            print(f\"   Internal/Relative: {internal_links}\")\n",
        "            print(f\"   External: {external_links}\")\n",
        "            print(f\"   Archive links: {archive_links}\")\n",
        "\n",
        "        else:\n",
        "            print(\"‚ùå No links with href found\")\n",
        "            print(\"üí° This might mean:\")\n",
        "            print(\"   - Page has no links\")\n",
        "            print(\"   - Links use different attributes\")\n",
        "            print(\"   - Page didn't load properly\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error finding links: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VA93z4BHtKdj"
      },
      "source": [
        "**Step 5b: Filter for historical document links**\n",
        "\n",
        "Let's find links that point to historical archives or documents:\n",
        "\n",
        "Copy this code:\n",
        "```python\n",
        "# Define domains that often contain historical documents\n",
        "historical_domains = ['archive.org', 'canadiana.ca', 'gutenberg.org', 'biographi.ca']\n",
        "\n",
        "# Filter links\n",
        "document_links = []\n",
        "for link in all_links:\n",
        "    href = link.get('href', '')\n",
        "    # Check if any historical domain is in the URL\n",
        "    for domain in historical_domains:\n",
        "        if domain in href:\n",
        "            document_links.append({\n",
        "                'text': link.get_text().strip(),\n",
        "                'url': href\n",
        "            })\n",
        "            break  # Don't add the same link twice\n",
        "\n",
        "print(f\"Historical document links found: {len(document_links)}\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azwM8ROAtKdj"
      },
      "outputs": [],
      "source": [
        "# Filter for historical document links with validation\n",
        "try:\n",
        "    if 'all_links' not in locals():\n",
        "        print(\"‚ùå Links not found - run the previous link extraction cell first\")\n",
        "    else:\n",
        "        # Define domains that often contain historical documents\n",
        "        historical_domains = ['archive.org', 'canadiana.ca', 'gutenberg.org', 'biographi.ca', 'bac-lac.gc.ca']\n",
        "\n",
        "        # Filter links\n",
        "        document_links = []\n",
        "        domain_counts = {}\n",
        "\n",
        "        for link in all_links:\n",
        "            href = link.get('href', '')\n",
        "            link_text = link.get_text().strip()\n",
        "\n",
        "            # Check if any historical domain is in the URL\n",
        "            for domain in historical_domains:\n",
        "                if domain in href:\n",
        "                    document_links.append({\n",
        "                        'text': link_text,\n",
        "                        'url': href,\n",
        "                        'domain': domain\n",
        "                    })\n",
        "\n",
        "                    # Count by domain\n",
        "                    domain_counts[domain] = domain_counts.get(domain, 0) + 1\n",
        "                    break  # Don't add the same link twice\n",
        "\n",
        "        print(f\"üèõÔ∏è Historical document analysis:\")\n",
        "        print(f\"   Total links checked: {len(all_links)}\")\n",
        "        print(f\"   Historical document links found: {len(document_links)}\")\n",
        "\n",
        "        if document_links:\n",
        "            print(f\"\\nüìä By domain:\")\n",
        "            for domain, count in domain_counts.items():\n",
        "                print(f\"   {domain}: {count} links\")\n",
        "\n",
        "            print(f\"\\nüìö Historical document links:\")\n",
        "            for i, link in enumerate(document_links[:5], 1):  # Show first 5\n",
        "                print(f\"\\n{i}. {link['text'][:60]}{'...' if len(link['text']) > 60 else ''}\")\n",
        "                print(f\"   Domain: {link['domain']}\")\n",
        "                print(f\"   URL: {link['url'][:80]}{'...' if len(link['url']) > 80 else ''}\")\n",
        "\n",
        "            if len(document_links) > 5:\n",
        "                print(f\"\\n... and {len(document_links) - 5} more historical links\")\n",
        "\n",
        "            # Validate link quality\n",
        "            valid_links = [link for link in document_links if link['text'] and len(link['text']) > 3]\n",
        "            print(f\"\\n‚úÖ Quality check: {len(valid_links)}/{len(document_links)} links have meaningful text\")\n",
        "\n",
        "        else:\n",
        "            print(\"‚ùå No historical document links found\")\n",
        "            print(f\"üí° Searched for these domains: {', '.join(historical_domains)}\")\n",
        "            print(\"   This might mean:\")\n",
        "            print(\"   - This page doesn't link to major historical archives\")\n",
        "            print(\"   - Links use different URL structures\")\n",
        "            print(\"   - Need to add more domain patterns\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error filtering historical links: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyMZNO4ktKdj"
      },
      "source": [
        "**Step 5c: Display the historical links**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWXt7tLQtKdj"
      },
      "outputs": [],
      "source": [
        "# Show the historical document links we found\n",
        "try:\n",
        "    if 'document_links' not in locals():\n",
        "        print(\"‚ùå Historical document links not available - run the filter cell first\")\n",
        "    elif not document_links:\n",
        "        print(\"‚ÑπÔ∏è No historical document links were identified yet.\")\n",
        "    else:\n",
        "        print(\"Historical Document Links:\")\n",
        "        print(\"=\" * 50)\n",
        "        for i, link in enumerate(document_links, 1):\n",
        "            print(f\"{i}. {link['text']}\")\n",
        "            print(f\"   URL: {link['url']}\")\n",
        "            print()\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error displaying historical document links: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jU3km0TtKdj"
      },
      "source": [
        "### üîÑ **Your Turn: Find PDF Links**\n",
        "\n",
        "Many historical documents are available as PDFs. Adapt the link-finding code to:\n",
        "\n",
        "1. Find all links that contain \".pdf\" in their href\n",
        "2. Store them in a list called `pdf_links`\n",
        "3. Print how many PDF links you found\n",
        "4. Display the first 3 PDF links\n",
        "\n",
        "Hint: Use `if '.pdf' in href:` to check for PDF links."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1N-CJWLtKdj"
      },
      "outputs": [],
      "source": [
        "# Your exercise: Find PDF links\n",
        "try:\n",
        "    if 'all_links' not in locals():\n",
        "        print(\"‚ùå Links not found - run the previous link extraction cell first\")\n",
        "    else:\n",
        "        # Find all links that contain \".pdf\" in their href\n",
        "        pdf_links = []\n",
        "\n",
        "        for link in all_links:\n",
        "            href = link.get('href', '')\n",
        "            link_text = link.get_text().strip()\n",
        "\n",
        "            if '.pdf' in href.lower():\n",
        "                pdf_links.append({\n",
        "                    'text': link_text,\n",
        "                    'url': href\n",
        "                })\n",
        "\n",
        "        print(f\"üìÑ PDF Link Analysis:\")\n",
        "        print(f\"   Total links checked: {len(all_links)}\")\n",
        "        print(f\"   PDF links found: {len(pdf_links)}\")\n",
        "\n",
        "        if pdf_links:\n",
        "            print(f\"\\n‚úÖ Found {len(pdf_links)} PDF documents!\")\n",
        "\n",
        "            print(f\"\\nüìã First 3 PDF links:\")\n",
        "            for i, pdf in enumerate(pdf_links[:3], 1):\n",
        "                print(f\"\\n{i}. '{pdf['text'][:60]}{'...' if len(pdf['text']) > 60 else ''}'\")\n",
        "                print(f\"   URL: {pdf['url']}\")\n",
        "\n",
        "                # Analyze URL for file type validation\n",
        "                if pdf['url'].lower().endswith('.pdf'):\n",
        "                    print(\"   ‚úÖ Direct PDF link\")\n",
        "                else:\n",
        "                    print(\"   ‚ö†Ô∏è  URL contains .pdf but doesn't end with .pdf\")\n",
        "\n",
        "            if len(pdf_links) > 3:\n",
        "                print(f\"\\n... and {len(pdf_links) - 3} more PDF links\")\n",
        "\n",
        "            # Quality validation\n",
        "            valid_pdfs = [pdf for pdf in pdf_links if pdf['text'] and len(pdf['text']) > 3]\n",
        "            print(f\"\\nüìä Quality check: {len(valid_pdfs)}/{len(pdf_links)} PDF links have meaningful text\")\n",
        "\n",
        "        else:\n",
        "            print(\"‚ùå No PDF links found on this page\")\n",
        "            print(\"üí° This might mean:\")\n",
        "            print(\"   - Page doesn't link to PDF documents\")\n",
        "            print(\"   - PDFs are embedded differently\")\n",
        "            print(\"   - Links use different file extensions\")\n",
        "            print(\"   - Try looking for links with 'download', 'document', or 'file' in text\")\n",
        "\n",
        "            # Alternative search\n",
        "            print(\"\\nüîç Searching for potential document links...\")\n",
        "            doc_keywords = ['download', 'document', 'file', 'report', 'manuscript']\n",
        "            potential_docs = []\n",
        "\n",
        "            for link in all_links:\n",
        "                link_text = link.get_text().lower()\n",
        "                if any(keyword in link_text for keyword in doc_keywords):\n",
        "                    potential_docs.append(link.get_text().strip())\n",
        "\n",
        "            if potential_docs:\n",
        "                print(f\"   Found {len(potential_docs)} links with document keywords:\")\n",
        "                for doc in potential_docs[:3]:\n",
        "                    print(f\"     - {doc[:60]}{'...' if len(doc) > 60 else ''}\")\n",
        "            else:\n",
        "                print(\"   No obvious document links found\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error finding PDF links: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOlrCF2wtKdo"
      },
      "source": [
        "## Step 6: Internet Archive Metadata\n",
        "\n",
        "Internet Archive documents have rich metadata. Let's learn to extract it systematically.\n",
        "\n",
        "**Step 6a: Get an Internet Archive page**\n",
        "\n",
        "Let's work with our Saskatchewan document:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1Au04aGtKdo"
      },
      "outputs": [],
      "source": [
        "# Get Internet Archive page with validation\n",
        "ia_url = \"https://archive.org/details/saskatchewan00sask\"\n",
        "\n",
        "print(\"üèõÔ∏è Working with Internet Archive metadata...\")\n",
        "print(f\"URL: {ia_url}\")\n",
        "\n",
        "try:\n",
        "    # Use our respectful function with a bit longer delay for IA\n",
        "    ia_response = respectful_get(ia_url, delay=2)\n",
        "\n",
        "    if ia_response:\n",
        "        print(f\"‚úÖ Status Code: {ia_response.status_code}\")\n",
        "        print(\"‚úÖ Internet Archive page loaded successfully!\")\n",
        "\n",
        "        # Basic validation\n",
        "        if 'archive.org' in ia_response.url:\n",
        "            print(\"‚úÖ Confirmed: This is an Internet Archive page\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è  Warning: Response URL doesn't match Internet Archive\")\n",
        "\n",
        "        # Check content size\n",
        "        content_size = len(ia_response.text)\n",
        "        print(f\"üìÑ Content size: {content_size:,} characters\")\n",
        "\n",
        "        if content_size > 10000:\n",
        "            print(\"‚úÖ Substantial content received\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è  Warning: Less content than expected\")\n",
        "\n",
        "    else:\n",
        "        print(\"‚ùå Failed to load Internet Archive page\")\n",
        "        print(\"üí° Possible issues:\")\n",
        "        print(\"   - Internet Archive servers busy\")\n",
        "        print(\"   - Network connectivity issues\")\n",
        "        print(\"   - Document no longer available\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading Internet Archive page: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgP4AVIMtKdp"
      },
      "source": [
        "**Step 6b: Extract the document title**\n",
        "\n",
        "Internet Archive puts document titles in `<h1>` tags:\n",
        "\n",
        "Copy this code:\n",
        "```python\n",
        "title = ia_soup.find('h1')\n",
        "if title:\n",
        "    document_title = title.get_text().strip()\n",
        "    print(f\"Document Title: {document_title}\")\n",
        "else:\n",
        "    print(\"No title found\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_dhCfUFtKdp"
      },
      "outputs": [],
      "source": [
        "# Extract Internet Archive title with validation\n",
        "try:\n",
        "    if 'ia_response' not in locals() or not ia_response:\n",
        "        print(\"‚ùå Internet Archive response not available - run the previous cell first\")\n",
        "    else:\n",
        "        # Create soup for Internet Archive page\n",
        "        ia_soup = BeautifulSoup(ia_response.text, 'html.parser')\n",
        "        print(\"‚úÖ Internet Archive soup created\")\n",
        "\n",
        "        # Find title - IA uses h1 for main document title\n",
        "        title_tag = ia_soup.find('h1')\n",
        "\n",
        "        if title_tag:\n",
        "            document_title = title_tag.get_text().strip()\n",
        "            print(f\"üìö Document Title: {document_title}\")\n",
        "\n",
        "            # Validate title quality\n",
        "            if len(document_title) > 5:\n",
        "                print(\"‚úÖ Title validation: Reasonable length\")\n",
        "            else:\n",
        "                print(\"‚ö†Ô∏è  Warning: Title seems very short\")\n",
        "\n",
        "            # Check for expected keywords\n",
        "            title_lower = document_title.lower()\n",
        "            if 'saskatchewan' in title_lower:\n",
        "                print(\"‚úÖ Content validation: Saskatchewan document confirmed\")\n",
        "            else:\n",
        "                print(\"‚ö†Ô∏è  Content note: Title doesn't contain 'Saskatchewan'\")\n",
        "                print(f\"   This might be normal - could be a more specific title\")\n",
        "\n",
        "            # Check for historical time indicators\n",
        "            historical_indicators = ['history', 'historical', '19', '18', 'century']\n",
        "            found_indicators = [ind for ind in historical_indicators if ind in title_lower]\n",
        "            if found_indicators:\n",
        "                print(f\"üìñ Historical indicators: {', '.join(found_indicators)}\")\n",
        "\n",
        "        else:\n",
        "            print(\"‚ùå No h1 title tag found\")\n",
        "            print(\"üí° Trying alternative title methods...\")\n",
        "\n",
        "            # Try alternative title extraction\n",
        "            title_alternatives = [\n",
        "                ia_soup.find('title'),  # HTML title tag\n",
        "                ia_soup.find('h2'),     # Secondary heading\n",
        "                ia_soup.find('div', class_='item-title')  # IA specific class\n",
        "            ]\n",
        "\n",
        "            for alt_title in title_alternatives:\n",
        "                if alt_title:\n",
        "                    alt_text = alt_title.get_text().strip()\n",
        "                    if alt_text and len(alt_text) > 5:\n",
        "                        print(f\"üìö Alternative title found: {alt_text}\")\n",
        "                        break\n",
        "            else:\n",
        "                print(\"‚ùå No alternative titles found\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error extracting title: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PITEaildtKdp"
      },
      "source": [
        "**Step 6c: Find metadata fields**\n",
        "\n",
        "Internet Archive uses `<dt>` (definition term) and `<dd>` (definition description) tags for metadata:\n",
        "\n",
        "Copy this code:\n",
        "```python\n",
        "# Find metadata terms and values\n",
        "metadata_terms = ia_soup.find_all('dt')\n",
        "metadata_values = ia_soup.find_all('dd')\n",
        "\n",
        "print(f\"Metadata fields found: {len(metadata_terms)}\")\n",
        "print(f\"Metadata values found: {len(metadata_values)}\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68LKehJWtKdp"
      },
      "outputs": [],
      "source": [
        "# Find metadata fields on the Internet Archive page\n",
        "try:\n",
        "    if 'ia_soup' not in locals():\n",
        "        print(\"‚ùå Internet Archive soup not found - run the previous cell first\")\n",
        "    else:\n",
        "        metadata_terms = ia_soup.find_all('dt')\n",
        "        metadata_values = ia_soup.find_all('dd')\n",
        "        print(f\"üóÇÔ∏è Metadata terms found: {len(metadata_terms)}\")\n",
        "        print(f\"üìù Metadata values found: {len(metadata_values)}\")\n",
        "        if metadata_terms and metadata_values:\n",
        "            print(\"‚úÖ Metadata structure detected\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è  Warning: No metadata definition lists found\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error collecting metadata fields: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiawnq17tKdp"
      },
      "source": [
        "**Step 6d: Extract and display metadata**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpDdvSnatKdp"
      },
      "outputs": [],
      "source": [
        "# Create a dictionary to store metadata\n",
        "try:\n",
        "    if 'metadata_terms' not in locals() or 'metadata_values' not in locals():\n",
        "        print(\"‚ùå Metadata fields not found - run the metadata extraction cell first\")\n",
        "    else:\n",
        "        metadata = {}\n",
        "        for term, value in zip(metadata_terms, metadata_values):\n",
        "            term_text = term.get_text().strip()\n",
        "            value_text = value.get_text().strip()\n",
        "            metadata[term_text] = value_text\n",
        "        if not metadata:\n",
        "            print(\"‚ÑπÔ∏è No metadata pairs were captured - the page structure may have changed\")\n",
        "        else:\n",
        "            print(\"Document Metadata:\")\n",
        "            print(\"=\" * 40)\n",
        "            important_fields = ['by', 'Publication date', 'Topics', 'Language']\n",
        "            for field in important_fields:\n",
        "                if field in metadata:\n",
        "                    value = metadata[field]\n",
        "                    if len(value) > 100:\n",
        "                        value = value[:100] + '...'\n",
        "                    print(f\"{field}: {value}\")\n",
        "            extra_fields = [key for key in metadata if key not in important_fields]\n",
        "            if extra_fields:\n",
        "                print()\n",
        "                print(f\"üîç Additional metadata fields captured: {len(extra_fields)}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error building metadata dictionary: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-j-z3Z5RtKdp"
      },
      "source": [
        "### üîÑ **Your Turn: Create a Metadata Function**\n",
        "\n",
        "Now create a reusable function that extracts metadata from any Internet Archive document. Fill in the missing parts:\n",
        "\n",
        "```python\n",
        "def extract_ia_metadata(url):\n",
        "    \"\"\"Extract title and metadata from an Internet Archive document\"\"\"\n",
        "    # 1. Get the page with requests.get()\n",
        "    # 2. Create soup with BeautifulSoup\n",
        "    # 3. Extract title from h1 tag\n",
        "    # 4. Extract metadata from dt/dd tags\n",
        "    # 5. Return a dictionary with title and metadata\n",
        "```\n",
        "\n",
        "Test it with the University of Toronto annual report: `https://archive.org/details/annualreport191920nivuoft`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZrK0u8BtKdp"
      },
      "outputs": [],
      "source": [
        "# Create a reusable Internet Archive metadata function\n",
        "def extract_ia_metadata(url):\n",
        "    \"\"\"Extract title and metadata from an Internet Archive document\"\"\"\n",
        "    try:\n",
        "        print(f\"üîç Analyzing Internet Archive URL: {url}\")\n",
        "\n",
        "        # Extract item ID from URL\n",
        "        if '/details/' in url:\n",
        "            item_id = url.split('/details/')[-1]\n",
        "            print(f\"üìã Item ID extracted: {item_id}\")\n",
        "        else:\n",
        "            return {'error': 'Invalid Internet Archive URL format'}\n",
        "\n",
        "        # Method 1: Try Internet Archive Python library (preferred)\n",
        "        try:\n",
        "            import internetarchive as ia\n",
        "            item = ia.get_item(item_id)\n",
        "\n",
        "            # Extract key metadata\n",
        "            metadata = {\n",
        "                'method': 'IA Python Library',\n",
        "                'title': item.metadata.get('title', 'No title'),\n",
        "                'creator': item.metadata.get('creator', 'No creator'),\n",
        "                'date': item.metadata.get('date', 'No date'),\n",
        "                'subject': item.metadata.get('subject', 'No subject'),\n",
        "                'description': item.metadata.get('description', 'No description')[:200] + '...' if item.metadata.get('description') else 'No description',\n",
        "                'language': item.metadata.get('language', 'No language'),\n",
        "                'files_count': len(list(item.files))\n",
        "            }\n",
        "\n",
        "            print(\"‚úÖ Successfully extracted metadata using IA library\")\n",
        "            return metadata\n",
        "\n",
        "        except ImportError:\n",
        "            print(\"‚ö†Ô∏è  IA library not available, falling back to web scraping...\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  IA library failed ({e}), falling back to web scraping...\")\n",
        "\n",
        "        # Method 2: Fallback to web scraping\n",
        "        response = respectful_get(url, delay=2)\n",
        "        if not response:\n",
        "            return {'error': 'Could not download page'}\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Extract metadata from HTML\n",
        "        title_tag = soup.find('h1')\n",
        "        title = title_tag.get_text().strip() if title_tag else 'No title found'\n",
        "\n",
        "        # Try to find metadata fields\n",
        "        metadata_dict = {'method': 'Web Scraping', 'title': title}\n",
        "\n",
        "        # Look for dt/dd metadata pairs\n",
        "        dt_tags = soup.find_all('dt')\n",
        "        dd_tags = soup.find_all('dd')\n",
        "\n",
        "        for dt, dd in zip(dt_tags, dd_tags):\n",
        "            key = dt.get_text().strip().lower()\n",
        "            value = dd.get_text().strip()\n",
        "\n",
        "            # Map common fields\n",
        "            if 'by' in key or 'creator' in key:\n",
        "                metadata_dict['creator'] = value\n",
        "            elif 'date' in key:\n",
        "                metadata_dict['date'] = value\n",
        "            elif 'topic' in key or 'subject' in key:\n",
        "                metadata_dict['subject'] = value\n",
        "            elif 'language' in key:\n",
        "                metadata_dict['language'] = value\n",
        "\n",
        "        print(\"‚úÖ Successfully extracted metadata using web scraping\")\n",
        "        return metadata_dict\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error extracting metadata: {e}\")\n",
        "        return {'error': str(e)}\n",
        "\n",
        "# Test the function with University of Toronto annual report\n",
        "uoft_url = \"https://archive.org/details/annualreport191920nivuoft\"\n",
        "print(\"üéì Testing with University of Toronto Annual Report 1919-20...\")\n",
        "\n",
        "result = extract_ia_metadata(uoft_url)\n",
        "\n",
        "print(f\"\\nüìä Extracted Metadata:\")\n",
        "print(\"=\" * 50)\n",
        "for key, value in result.items():\n",
        "    print(f\"{key.title()}: {value}\")\n",
        "\n",
        "print(f\"\\nüí° This function demonstrates two approaches:\")\n",
        "print(f\"   1. Internet Archive Python library (preferred)\")\n",
        "print(f\"   2. Web scraping with Beautiful Soup (fallback)\")\n",
        "print(f\"   Real research projects should use both for robustness!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRyhzTwgtKdp"
      },
      "source": [
        "## Step 7: Final Challenge - Your Research Project\n",
        "\n",
        "Time to put it all together! Choose a Canadian historical source and conduct your own analysis.\n",
        "\n",
        "**Available sources:**\n",
        "- Encyclopedia Rupert's Land purchase article: Research the historical quotes and themes\n",
        "- Saskatchewan History: Analyze the metadata and publication context\n",
        "- UofT Annual Report 1919-20: Extract institutional information\n",
        "\n",
        "**Your research steps:**\n",
        "1. Choose a source and research question\n",
        "2. Use the scraping techniques you've learned\n",
        "3. Extract specific information related to your question\n",
        "4. Present your findings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Internet Archive library - much more efficient!\n",
        "print(\"üèõÔ∏è Accessing Internet Archive with the Python library...\")\n",
        "\n",
        "try:\n",
        "    import internetarchive as ia\n",
        "except ImportError:\n",
        "    print(\"‚ùå Internet Archive library not installed yet - run the installation cell below first\")\n",
        "else:\n",
        "    try:\n",
        "        item_id = \"saskatchewan00sask\"  # The ID from the URL\n",
        "        item = ia.get_item(item_id)\n",
        "        print(f\"‚úÖ Successfully retrieved item: {item_id}\")\n",
        "        print(f\"üìö Title: {item.metadata.get('title', 'No title')}\")\n",
        "        print(f\"üìÖ Date: {item.metadata.get('date', 'No date')}\")\n",
        "        print(f\"üë§ Creator: {item.metadata.get('creator', 'No creator')}\")\n",
        "        print(f\"üìñ Subject: {item.metadata.get('subject', 'No subject')}\")\n",
        "        files = list(item.files)\n",
        "        print()\n",
        "        print(f\"üìÅ Available files: {len(files)}\")\n",
        "        for i, file in enumerate(files[:5]):\n",
        "            file_name = file.get('name', 'Unknown')\n",
        "            file_format = file.get('format', 'Unknown')\n",
        "            file_size = file.get('size', 'Unknown')\n",
        "            print(f\"   {i+1}. {file_name} ({file_format}) - {file_size} bytes\")\n",
        "        if len(files) > 5:\n",
        "            print(f\"   ... and {len(files) - 5} more files\")\n",
        "        print()\n",
        "        print(\"üöÄ Much easier than HTML scraping!\")\n",
        "        print(\"üí° The IA library gives us clean, structured data instantly\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error accessing Internet Archive: {e}\")\n",
        "        print(\"üí° This might be due to network issues or an unavailable item\")\n"
      ],
      "metadata": {
        "id": "qaS8JKeztKdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install and import Internet Archive library\n",
        "# First install the library (run this once)\n",
        "!pip install internetarchive --quiet\n",
        "\n",
        "print(\"üìö Installing Internet Archive Python library...\")\n",
        "print(\"‚úÖ Installation complete!\")\n",
        "\n",
        "# Import the library\n",
        "try:\n",
        "    import internetarchive as ia\n",
        "    print(\"‚úÖ Internet Archive library imported successfully!\")\n",
        "    print(\"üîó This library provides direct, efficient access to IA collections\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Error importing Internet Archive library: {e}\")\n",
        "    print(\"üí° Try running: pip install internetarchive\")"
      ],
      "metadata": {
        "id": "ZESwWu-5tKdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Introduction to Internet Archive Python Library\n",
        "\n",
        "While Beautiful Soup is excellent for scraping web pages, the Internet Archive provides a specialized Python library that makes accessing their collections much more efficient. This is perfect for large-scale historical research projects.\n",
        "\n",
        "**Why use the Internet Archive library?**\n",
        "- Direct access to metadata without scraping HTML\n",
        "- Faster downloads and better error handling\n",
        "- Access to full-text search capabilities\n",
        "- Bulk processing of large collections\n",
        "- Respects Internet Archive's preferred access methods"
      ],
      "metadata": {
        "id": "wMHcKPZptKdq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqFMR2KNtKdq"
      },
      "outputs": [],
      "source": [
        "# Final Challenge: Your Historical Research Project\n",
        "\n",
        "# Available Canadian historical sources:\n",
        "sources = {\n",
        "    \"Rupert's Land Purchase (Encyclopedia of Saskatchewan)\": {\n",
        "        \"url\": \"https://esask.uregina.ca/entry/ruperts_land_purchase.html\",\n",
        "        \"type\": \"Online encyclopedia article\",\n",
        "        \"research_questions\": [\n",
        "            \"How is the Rupert's Land purchase narrated for a general audience?\",\n",
        "            \"What Indigenous perspectives are included or missing?\",\n",
        "            \"Which archival collections are linked for further research?\"\n",
        "        ]\n",
        "    },\n",
        "    \"Saskatchewan History\": {\n",
        "        \"url\": \"https://archive.org/details/saskatchewan00sask\",\n",
        "        \"type\": \"Internet Archive document\",\n",
        "        \"research_questions\": [\n",
        "            \"What metadata is available about publication?\",\n",
        "            \"What file formats are provided?\",\n",
        "            \"Who was the creator/publisher?\"\n",
        "        ]\n",
        "    },\n",
        "    \"UofT Annual Report 1919-20\": {\n",
        "        \"url\": \"https://archive.org/details/annualreport191920nivuoft\",\n",
        "        \"type\": \"Institutional document\",\n",
        "        \"research_questions\": [\n",
        "            \"What institutional information is captured?\",\n",
        "            \"How is the document structured?\",\n",
        "            \"What historical context does it provide?\"\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Choose your research project\n",
        "print(\"üî¨ Historical Web Scraping Research Project\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(\"üìö Available sources:\")\n",
        "for i, (name, info) in enumerate(sources.items(), 1):\n",
        "    print(f\"\n",
        "{i}. {name}\")\n",
        "    print(f\"   Type: {info['type']}\")\n",
        "    print(f\"   URL: {info['url'][:60]}...\")\n",
        "    print(f\"   Sample questions:\")\n",
        "    for q in info['research_questions']:\n",
        "        print(f\"     - {q}\")\n",
        "\n",
        "print(f\"\n",
        "üéØ Your research steps:\")\n",
        "print(\"1. Choose a source and research question\")\n",
        "print(\"2. Use appropriate scraping techniques\")\n",
        "print(\"3. Extract and validate data\")\n",
        "print(\"4. Analyze and present findings\")\n",
        "\n",
        "# Example research project - customize this!\n",
        "chosen_source = \"Rupert's Land Purchase (Encyclopedia of Saskatchewan)\"  # Change this to your choice\n",
        "my_url = sources[chosen_source][\"url\"]\n",
        "my_question = \"What historical themes appear in the Rupert's Land purchase article?\"\n",
        "\n",
        "print(f\"\n",
        "üìã Example Research Project:\")\n",
        "print(f\"Source: {chosen_source}\")\n",
        "print(f\"Question: {my_question}\")\n",
        "print(f\"URL: {my_url}\")\n",
        "\n",
        "# Conduct the research\n",
        "print(f\"\n",
        "üîç Conducting research...\")\n",
        "\n",
        "theme_counts = {}\n",
        "\n",
        "try:\n",
        "    response = respectful_get(my_url)\n",
        "    if response:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        print(\"‚úÖ Page successfully scraped\")\n",
        "        if \"themes\" in my_question.lower():\n",
        "            paragraphs = soup.find_all('p')\n",
        "            print(f\"\n",
        "üìä Content Analysis:\")\n",
        "            print(f\"   Paragraphs found: {len(paragraphs)}\")\n",
        "            all_text = soup.get_text().lower()\n",
        "            themes = {\n",
        "                'Economic': ['trade', 'commerce', 'company', 'profit'],\n",
        "                'Indigenous perspectives': ['indigenous', 'cree', 'assiniboine', 'd√©n√©', 'nation'],\n",
        "                'Colonial administration': ['british', 'government', 'hudson', 'london'],\n",
        "                'Geographic': ['rupert', 'saskatchewan', 'manitoba', 'prairies', 'north-west'],\n",
        "                'Temporal': ['1869', '1870', '19th century', 'confederation']\n",
        "            }\n",
        "            for theme_name, keywords in themes.items():\n",
        "                count = sum(all_text.count(keyword) for keyword in keywords)\n",
        "                if count > 0:\n",
        "                    theme_counts[theme_name] = count\n",
        "            if theme_counts:\n",
        "                print(f\"\n",
        "üé≠ Historical Themes Found:\")\n",
        "                for theme, count in sorted(theme_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "                    print(f\"   {theme}: {count} mentions\")\n",
        "            else:\n",
        "                print(\"\n",
        "‚ÑπÔ∏è No theme keywords were detected - try adjusting the keyword lists\")\n",
        "        else:\n",
        "            print(\"‚ÑπÔ∏è Customize the analysis section to match your research question.\")\n",
        "        print(f\"\n",
        "üìù Research Findings:\")\n",
        "        print(\"1. Successfully scraped a Canadian historical source\")\n",
        "        print(\"2. Identified multiple historical themes in source material\" if theme_counts else \"2. Ready to adapt the analysis for your own question\")\n",
        "        print(\"3. Demonstrated effective web scraping for historical research\")\n",
        "        if theme_counts:\n",
        "            most_common = max(theme_counts.items(), key=lambda x: x[1])\n",
        "            print(f\"4. Most prominent theme: {most_common[0]} ({most_common[1]} mentions)\")\n",
        "    else:\n",
        "        print(\"‚ùå Could not complete research - page unavailable\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Research error: {e}\")\n",
        "\n",
        "print(f\"\n",
        "üöÄ Now it's your turn!\")\n",
        "print(\"Customize this code for your own research question:\")\n",
        "print(\"1. Change 'chosen_source' to your preferred source\")\n",
        "print(\"2. Modify 'my_question' to your research interest\")\n",
        "print(\"3. Adapt the analysis code for your specific question\")\n",
        "print(\"4. Add your own themes, keywords, or analysis methods\")\n",
        "\n",
        "print(f\"\n",
        "üí° Remember to:\")\n",
        "print(\"- Validate your scraped data\")\n",
        "print(\"- Handle errors gracefully\")\n",
        "print(\"- Respect website terms of service\")\n",
        "print(\"- Cite your digital sources properly\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PIJs3KgtKdq"
      },
      "source": [
        "## Summary: What You've Learned\n",
        "\n",
        "üéâ **Congratulations!** You've mastered the fundamentals of web scraping for historical research:\n",
        "\n",
        "**Technical Skills:**\n",
        "- ‚úÖ Making web requests with `requests.get()`\n",
        "- ‚úÖ Parsing HTML with Beautiful Soup\n",
        "- ‚úÖ Targeting specific elements (`find`, `find_all`)\n",
        "- ‚úÖ Extracting text, links, and metadata\n",
        "- ‚úÖ Building reusable functions for research\n",
        "\n",
        "**Historical Sources:**\n",
        "- ‚úÖ Blog posts with embedded historical content\n",
        "- ‚úÖ Internet Archive documents with metadata\n",
        "- ‚úÖ Academic indexes and structured data\n",
        "\n",
        "**Research Methods:**\n",
        "- ‚úÖ Systematic content extraction\n",
        "- ‚úÖ Metadata analysis for document context\n",
        "- ‚úÖ Building reproducible research workflows\n",
        "\n",
        "**Next Steps:**\n",
        "In Notebook 3, you'll learn advanced text analysis techniques to find patterns in the historical data you've scraped. We'll also explore the Internet Archive Python library for more efficient access to large collections.\n",
        "\n",
        "**Remember:**\n",
        "- Always respect robots.txt and website terms of service\n",
        "- Cite your digital sources properly\n",
        "- Consider the limitations and context of digitized materials\n",
        "- Use these skills responsibly for legitimate research purposes"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}