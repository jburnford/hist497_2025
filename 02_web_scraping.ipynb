{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Notebook 2: Web Scraping for Historians**\n",
    "\n",
    "Welcome to web scraping! In this notebook, you'll learn to extract historical data from websites using Python. We'll work with real Canadian historical sources and build your skills step by step.\n",
    "\n",
    "**What you'll learn:**\n",
    "- How web pages are structured (HTML basics)\n",
    "- Using Beautiful Soup to extract specific content\n",
    "- Working with Canadian historical archives\n",
    "- Building reusable code for research\n",
    "\n",
    "**Ethics first:** Always respect websites. Check robots.txt files, don't overload servers, and respect copyright."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setting Up Our Tools\n",
    "\n",
    "Before we can scrape websites, we need to install and import the right libraries. Let's do this step by step.\n",
    "\n",
    "**First, install the libraries:**"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Example: How to add delays between requests\nimport time\n\ndef respectful_get(url, delay=1):\n    \"\"\"\n    Make a web request with built-in delay for respectful scraping\n    \"\"\"\n    print(f\"\u23f3 Waiting {delay} second(s) before request...\")\n    time.sleep(delay)  # Wait before making request\n    \n    try:\n        response = requests.get(url, timeout=10)\n        response.raise_for_status()\n        print(f\"\u2705 Request successful\")\n        return response\n    except requests.exceptions.RequestException as e:\n        print(f\"\u274c Request failed: {e}\")\n        return None\n\n# We'll use this function for respectful scraping throughout the notebook\nprint(\"Respectful scraping function defined!\")\nprint(\"\ud83d\udca1 This adds delays between requests to be considerate to servers.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## \ud83e\udd1d Ethical Web Scraping Guidelines\n\nBefore we start scraping, let's understand the ethics and best practices:\n\n**\u2705 Always:**\n- Check robots.txt (add /robots.txt to any website URL)\n- Add delays between requests (don't overwhelm servers)\n- Use reasonable timeouts\n- Respect copyright and terms of service\n- Identify yourself with User-Agent headers when appropriate\n\n**\u274c Never:**\n- Scrape faster than a human could browse\n- Ignore error messages or blocks\n- Scrape personal/private information\n- Violate website terms of service\n- Overload servers with rapid requests\n\n**\ud83d\udcd6 For historical research:**\n- Many archives encourage responsible academic use\n- Always cite your digital sources properly\n- Consider contacting archives for bulk data access\n- Respect cultural sensitivities in historical materials",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This installs the libraries we need\n",
    "!pip install requests beautifulsoup4 --quiet\n",
    "print(\"Libraries installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, import them so we can use them:**\n",
    "\n",
    "Copy this code into the cell below:\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "```\n",
    "If you don't get an error and nothing happens after you click the run button, it worked. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import the libraries we need for web scraping\nimport requests\nfrom bs4 import BeautifulSoup\n\nprint(\"Libraries imported successfully!\")\nprint(\"\u2705 requests: Downloads web pages\")\nprint(\"\u2705 BeautifulSoup: Parses HTML content\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Your First Web Request\n",
    "\n",
    "Let's start by downloading a web page. We'll use a Canadian historical source - a Toronto Public Library blog post about Jesuit Relations.\n",
    "\n",
    "**Step 2a: Define the URL**\n",
    "\n",
    "Copy this code:\n",
    "```python\n",
    "url = \"https://torontopubliclibrary.typepad.com/local-history-genealogy/2020/01/sainte-marie-among-the-hurons-selections-from-the-jesuit-relations-and-allied-documents.html\"\n",
    "print(f\"We're going to scrape: {url}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define the URL for our Canadian historical source\nurl = \"https://torontopubliclibrary.typepad.com/local-history-genealogy/2020/01/sainte-marie-among-the-hurons-selections-from-the-jesuit-relations-and-allied-documents.html\"\nprint(f\"We're going to scrape: {url}\")\nprint(\"\\n\ud83d\udcda This is a Toronto Public Library blog post about Jesuit Relations\")\nprint(\"\ud83c\udde8\ud83c\udde6 Perfect for learning Canadian historical web scraping!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2b: Download the page**\n",
    "\n",
    "Now let's actually download the web page. The `requests.get()` function fetches the page for us.\n",
    "\n",
    "Copy this code:\n",
    "```python\n",
    "response = requests.get(url)\n",
    "print(f\"Status Code: {response.status_code}\")  # 200 means success\n",
    "print(f\"Page downloaded! It contains {len(response.text)} characters.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Download the page with error handling\ntry:\n    response = requests.get(url, timeout=10)  # 10 second timeout\n    response.raise_for_status()  # Raises an exception for bad status codes\n    print(f\"\u2705 Success! Status Code: {response.status_code}\")\n    print(f\"\ud83d\udcc4 Page downloaded! It contains {len(response.text):,} characters.\")\nexcept requests.exceptions.RequestException as e:\n    print(f\"\u274c Error downloading page: {e}\")\n    print(\"\ud83d\udca1 This could be due to:\")\n    print(\"   - No internet connection\")\n    print(\"   - Website is down\")\n    print(\"   - URL has changed\")\n    print(\"   - Server is blocking requests\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2c: Look at the raw HTML**\n",
    "\n",
    "Let's see what we actually downloaded. Warning: it's going to look messy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Your adaptation exercise: Saskatchewan Internet Archive document\nsaskatchewan_url = \"https://archive.org/details/saskatchewan00sask\"\n\nprint(\"\ud83c\udf41 Trying Saskatchewan historical document...\")\nprint(f\"URL: {saskatchewan_url}\")\n\ntry:\n    # Use our respectful function\n    sask_response = respectful_get(saskatchewan_url)\n    \n    if sask_response:\n        print(f\"\u2705 Status Code: {sask_response.status_code}\")\n        print(f\"\ud83d\udcc4 Downloaded {len(sask_response.text):,} characters\")\n        \n        # Show first 500 characters\n        print(\"\\nFirst 500 characters of raw HTML:\")\n        print(\"-\" * 50)\n        print(sask_response.text[:500])\n        print(\"-\" * 50)\n        print(\"\ud83d\udca1 Notice how different this Internet Archive page looks compared to the blog!\")\n    else:\n        print(\"\u274c Failed to download Saskatchewan document\")\n        \nexcept Exception as e:\n    print(f\"\u274c Error: {e}\")\n    print(\"\ud83d\udca1 If this fails, it might be due to network issues or site changes\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udd04 **Your Turn: Adaptation Exercise**\n",
    "\n",
    "Now try the same steps with a different Canadian source. Copy the code from above and modify it to use this Internet Archive document about Saskatchewan:\n",
    "\n",
    "```python\n",
    "url = \"https://archive.org/details/saskatchewan00sask\"\n",
    "```\n",
    "\n",
    "Follow the same steps: define the URL, download the page, check the status code, and look at the first 500 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your adaptation exercise here\n",
    "# 1. Define the Saskatchewan URL\n",
    "# 2. Download the page with requests.get()\n",
    "# 3. Print the status code and character count\n",
    "# 4. Show first 500 characters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Making Sense of HTML with Beautiful Soup\n",
    "\n",
    "Raw HTML is hard to work with. Beautiful Soup parses it and makes it easy to extract what we need.\n",
    "\n",
    "**Step 3a: Create your first \"soup\"**\n",
    "\n",
    "Let's go back to our TPL blog post and parse it properly:\n",
    "\n",
    "Copy this code:\n",
    "```python\n",
    "# Go back to the TPL blog post\n",
    "url = \"https://torontopubliclibrary.typepad.com/local-history-genealogy/2020/01/sainte-marie-among-the-hurons-selections-from-the-jesuit-relations-and-allied-documents.html\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create the soup object\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "print(\"Soup object created! Now we can easily extract content.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create soup object with validation\ntry:\n    # Use our respectful function\n    response = respectful_get(url)\n    \n    if response:\n        # Create the soup object\n        soup = BeautifulSoup(response.text, 'html.parser')\n        print(\"\u2705 Soup object created successfully!\")\n        \n        # Validate we got HTML content\n        if soup.find('html'):\n            print(\"\u2705 Valid HTML structure detected\")\n        else:\n            print(\"\u26a0\ufe0f  Warning: No HTML structure found - might be plain text\")\n            \n        # Check if we got the expected content\n        if len(soup.get_text().strip()) > 100:\n            print(f\"\u2705 Content validation: {len(soup.get_text().strip()):,} characters of text\")\n        else:\n            print(\"\u26a0\ufe0f  Warning: Very little text content found\")\n    else:\n        print(\"\u274c Could not create soup - request failed\")\n        \nexcept Exception as e:\n    print(f\"\u274c Error creating soup: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3b: Extract the page title**\n",
    "\n",
    "Let's start simple by getting the page title. In HTML, the title is in `<title>` tags.\n",
    "\n",
    "Copy this code:\n",
    "```python\n",
    "title = soup.find('title')\n",
    "print(f\"Page title: {title.get_text()}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Extract title with validation\ntry:\n    title_tag = soup.find('title')\n    \n    if title_tag:\n        title_text = title_tag.get_text().strip()\n        print(f\"\u2705 Page title found: {title_text}\")\n        \n        # Validate the title makes sense\n        if len(title_text) > 5:\n            print(\"\u2705 Title validation: Reasonable length\")\n        else:\n            print(\"\u26a0\ufe0f  Warning: Title seems very short\")\n            \n        # Check if it's related to our expected content\n        if any(keyword in title_text.lower() for keyword in ['jesuit', 'sainte-marie', 'huron', 'toronto']):\n            print(\"\u2705 Content validation: Title matches expected historical topic\")\n        else:\n            print(\"\u26a0\ufe0f  Warning: Title doesn't match expected content - check URL\")\n    else:\n        print(\"\u274c No title tag found\")\n        print(\"\ud83d\udca1 This might mean:\")\n        print(\"   - Page structure is different than expected\")\n        print(\"   - We got redirected to a different page\")\n        print(\"   - Page failed to load properly\")\n        \nexcept Exception as e:\n    print(f\"\u274c Error extracting title: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3c: Get clean text (no HTML tags)**\n",
    "\n",
    "The `.get_text()` method removes all HTML tags and gives us just the readable content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all text content without HTML tags\n",
    "try:\n",
    "    if 'soup' not in locals():\n",
    "        print(\"\u274c Soup object not found - run the previous Beautiful Soup cell first\")\n",
    "    else:\n",
    "        clean_text = soup.get_text()\n",
    "        print(f\"Clean text length: {len(clean_text)} characters\")\n",
    "        print()\n",
    "        print(\"First 500 characters of clean text:\")\n",
    "        print(clean_text[:500])\n",
    "        print()\n",
    "        print(\"Much better! Now we can read the actual content.\")\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Error extracting clean text: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udd04 **Your Turn: Practice with Internet Archive**\n",
    "\n",
    "Now apply the same Beautiful Soup steps to the Saskatchewan document. Copy and adapt the code above:\n",
    "\n",
    "1. Create a soup object from the Saskatchewan URL\n",
    "2. Extract and print the title\n",
    "3. Get the clean text and show the first 500 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Your practice: Apply Beautiful Soup to Saskatchewan document\nprint(\"\ud83c\udf41 Analyzing Saskatchewan document with Beautiful Soup...\")\n\ntry:\n    # Create soup object for Saskatchewan document\n    sask_soup = BeautifulSoup(sask_response.text, 'html.parser')\n    print(\"\u2705 Saskatchewan soup created!\")\n    \n    # Extract and print the title\n    sask_title = sask_soup.find('title')\n    if sask_title:\n        title_text = sask_title.get_text().strip()\n        print(f\"\ud83d\udcda Title: {title_text}\")\n        \n        # Validate it's an Internet Archive page\n        if 'archive.org' in title_text.lower() or 'saskatchewan' in title_text.lower():\n            print(\"\u2705 Confirmed: This is the Saskatchewan document\")\n        else:\n            print(\"\u26a0\ufe0f  Title doesn't match expectations\")\n    else:\n        print(\"\u274c No title found\")\n    \n    # Get clean text and show first 500 characters\n    clean_text = sask_soup.get_text()\n    print(f\"\\n\ud83d\udcc4 Clean text length: {len(clean_text):,} characters\")\n    print(\"\\nFirst 500 characters of clean text:\")\n    print(\"-\" * 50)\n    print(clean_text[:500])\n    print(\"-\" * 50)\n    print(\"\ud83d\udca1 Much more readable than raw HTML!\")\n    \nexcept NameError:\n    print(\"\u274c Saskatchewan response not available - run the previous exercise first\")\nexcept Exception as e:\n    print(f\"\u274c Error: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Targeting Specific Content\n",
    "\n",
    "Getting all the text is useful, but often we want specific parts. Let's learn to target particular HTML elements.\n",
    "\n",
    "**Step 4a: Find all paragraphs**\n",
    "\n",
    "Blog posts organize content in paragraphs (`<p>` tags). Let's find them:\n",
    "\n",
    "Copy this code:\n",
    "```python\n",
    "# Go back to our TPL blog soup\n",
    "url = \"https://torontopubliclibrary.typepad.com/local-history-genealogy/2020/01/sainte-marie-among-the-hurons-selections-from-the-jesuit-relations-and-allied-documents.html\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all paragraph tags\n",
    "paragraphs = soup.find_all('p')\n",
    "print(f\"Found {len(paragraphs)} paragraphs\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Find all paragraphs with validation\ntry:\n    # Make sure we have our soup object\n    if 'soup' not in locals():\n        print(\"\u274c Soup object not found - please run the previous Beautiful Soup cells first\")\n    else:\n        # Find all paragraph tags\n        paragraphs = soup.find_all('p')\n        print(f\"\ud83d\udcdd Found {len(paragraphs)} paragraph tags\")\n        \n        # Validate we found reasonable content\n        if len(paragraphs) > 0:\n            print(\"\u2705 Paragraph extraction successful\")\n            \n            # Show some examples\n            print(f\"\\nAnalyzing first 3 paragraphs:\")\n            for i in range(min(3, len(paragraphs))):\n                para_text = paragraphs[i].get_text().strip()\n                if para_text:  # Only show non-empty paragraphs\n                    print(f\"\\nParagraph {i+1} ({len(para_text)} chars):\")\n                    print(f\"'{para_text[:100]}{'...' if len(para_text) > 100 else ''}'\")\n                else:\n                    print(f\"\\nParagraph {i+1}: (empty)\")\n        else:\n            print(\"\u26a0\ufe0f  Warning: No paragraphs found\")\n            print(\"\ud83d\udca1 This might mean:\")\n            print(\"   - Page uses different HTML structure\")\n            print(\"   - Content is in different tags (div, article, etc.)\")\n            print(\"   - Page didn't load properly\")\n            \nexcept Exception as e:\n    print(f\"\u274c Error finding paragraphs: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4b: Look at individual paragraphs**\n",
    "\n",
    "Let's examine a few paragraphs to see what we're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the first few paragraphs with validation\n",
    "try:\n",
    "    if 'paragraphs' not in locals():\n",
    "        print(\"\u274c Paragraphs not found - run the paragraph extraction cell first\")\n",
    "    elif not paragraphs:\n",
    "        print(\"\u26a0\ufe0f  No paragraph tags available to display\")\n",
    "    else:\n",
    "        print(\"First 3 paragraphs:\")\n",
    "        for i in range(min(3, len(paragraphs))):\n",
    "            para_text = paragraphs[i].get_text().strip()\n",
    "            preview = para_text[:100] + ('...' if len(para_text) > 100 else '')\n",
    "            print()\n",
    "            print(f\"Paragraph {i+1}: {preview}\")\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Error previewing paragraphs: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4c: Filter for substantial paragraphs**\n",
    "\n",
    "Many paragraphs are short or empty. Let's filter for substantial ones (more than 50 characters):\n",
    "\n",
    "Copy this code:\n",
    "```python\n",
    "# Filter for paragraphs with meaningful content\n",
    "substantial_paras = []\n",
    "for para in paragraphs:\n",
    "    text = para.get_text().strip()\n",
    "    if len(text) > 50:  # Only paragraphs with more than 50 characters\n",
    "        substantial_paras.append(text)\n",
    "\n",
    "print(f\"Substantial paragraphs: {len(substantial_paras)}\")\n",
    "print(f\"\\nFirst substantial paragraph:\")\n",
    "print(substantial_paras[0])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Filter for substantial paragraphs with validation\ntry:\n    if 'paragraphs' not in locals():\n        print(\"\u274c Paragraphs not found - run the previous cell first\")\n    else:\n        # Filter for paragraphs with meaningful content\n        substantial_paras = []\n        empty_count = 0\n        \n        for para in paragraphs:\n            text = para.get_text().strip()\n            if len(text) > 50:  # Only paragraphs with more than 50 characters\n                substantial_paras.append(text)\n            elif len(text) == 0:\n                empty_count += 1\n        \n        print(f\"\ud83d\udcca Paragraph Analysis:\")\n        print(f\"   Total paragraphs found: {len(paragraphs)}\")\n        print(f\"   Substantial paragraphs (>50 chars): {len(substantial_paras)}\")\n        print(f\"   Empty paragraphs: {empty_count}\")\n        print(f\"   Short paragraphs (<50 chars): {len(paragraphs) - len(substantial_paras) - empty_count}\")\n        \n        if substantial_paras:\n            print(f\"\\n\ud83d\udcd6 First substantial paragraph:\")\n            print(\"-\" * 60)\n            print(substantial_paras[0])\n            print(\"-\" * 60)\n            \n            # Validate content quality\n            first_para = substantial_paras[0].lower()\n            if any(word in first_para for word in ['jesuit', 'huron', 'sainte-marie', 'canada', 'history']):\n                print(\"\u2705 Content validation: Found expected historical keywords\")\n            else:\n                print(\"\u26a0\ufe0f  Content note: No obvious historical keywords found\")\n        else:\n            print(\"\u274c No substantial paragraphs found\")\n            print(\"\ud83d\udca1 This might mean the content is structured differently\")\n            \nexcept Exception as e:\n    print(f\"\u274c Error filtering paragraphs: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udd04 **Your Turn: Find Historical Quotes**\n",
    "\n",
    "Historical blog posts often include quotes in special `<blockquote>` tags. Adapt the code above to:\n",
    "\n",
    "1. Find all blockquote elements using `soup.find_all('blockquote')`\n",
    "2. Extract their text and print them\n",
    "3. Count how many historical quotes you found\n",
    "\n",
    "Use the same TPL blog post soup object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Your exercise: Find historical quotes in blockquotes\ntry:\n    if 'soup' not in locals():\n        print(\"\u274c Soup object not found - run the Beautiful Soup creation cells first\")\n    else:\n        # Find all blockquote elements\n        blockquotes = soup.find_all('blockquote')\n        print(f\"\ud83d\udcdc Found {len(blockquotes)} blockquote elements\")\n        \n        if blockquotes:\n            print(\"\u2705 Historical quotes found!\")\n            print(\"\\n\ud83d\udd0d Analyzing each quote:\")\n            \n            valid_quotes = []\n            for i, quote in enumerate(blockquotes, 1):\n                quote_text = quote.get_text().strip()\n                \n                if quote_text and len(quote_text) > 10:  # Filter out very short quotes\n                    valid_quotes.append(quote_text)\n                    print(f\"\\nQuote {i} ({len(quote_text)} characters):\")\n                    print(\"-\" * 50)\n                    # Show first 200 characters of quote\n                    display_text = quote_text[:200] + \"...\" if len(quote_text) > 200 else quote_text\n                    print(display_text)\n                    print(\"-\" * 50)\n                    \n                    # Check for historical indicators\n                    quote_lower = quote_text.lower()\n                    historical_indicators = ['jesuit', 'huron', 'savage', 'father', 'lord', 'god', '16', '17']\n                    found_indicators = [ind for ind in historical_indicators if ind in quote_lower]\n                    \n                    if found_indicators:\n                        print(f\"\ud83d\udcda Historical indicators found: {', '.join(found_indicators)}\")\n                    else:\n                        print(\"\ud83d\udcdd No obvious historical indicators\")\n                else:\n                    print(f\"Quote {i}: (too short or empty)\")\n            \n            print(f\"\\n\ud83d\udcca Summary:\")\n            print(f\"   Total blockquotes: {len(blockquotes)}\")\n            print(f\"   Valid historical quotes: {len(valid_quotes)}\")\n            \n            if valid_quotes:\n                avg_length = sum(len(q) for q in valid_quotes) / len(valid_quotes)\n                print(f\"   Average quote length: {avg_length:.0f} characters\")\n            \n        else:\n            print(\"\u274c No blockquotes found on this page\")\n            print(\"\ud83d\udca1 This might mean:\")\n            print(\"   - This blog doesn't use blockquotes for quotes\")\n            print(\"   - Quotes might be in other tags (div, p with special classes)\")\n            print(\"   - Page structure is different than expected\")\n            \nexcept Exception as e:\n    print(f\"\u274c Error finding quotes: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Working with Links\n",
    "\n",
    "Historical sources often link to primary documents. Let's learn to extract and analyze links.\n",
    "\n",
    "**Step 5a: Find all links**\n",
    "\n",
    "Links are in `<a>` tags with `href` attributes. Let's find them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Find all links with validation\ntry:\n    if 'soup' not in locals():\n        print(\"\u274c Soup object not found - run the Beautiful Soup creation cells first\")\n    else:\n        # Find all links with href attributes\n        all_links = soup.find_all('a', href=True)\n        print(f\"\ud83d\udd17 Found {len(all_links)} links with href attributes\")\n        \n        if all_links:\n            print(\"\u2705 Link extraction successful\")\n            \n            # Analyze link types\n            internal_links = 0\n            external_links = 0\n            archive_links = 0\n            \n            print(\"\\n\ud83d\udd0d Analyzing first 5 links:\")\n            for i, link in enumerate(all_links[:5]):\n                link_text = link.get_text().strip()\n                link_url = link.get('href')\n                \n                # Classify link type\n                if link_url.startswith('http'):\n                    if 'torontopubliclibrary' in link_url:\n                        link_type = \"Internal\"\n                        internal_links += 1\n                    elif 'archive.org' in link_url:\n                        link_type = \"Archive\"\n                        archive_links += 1\n                    else:\n                        link_type = \"External\"\n                        external_links += 1\n                else:\n                    link_type = \"Relative\"\n                    internal_links += 1\n                \n                print(f\"\\n{i+1}. [{link_type}] '{link_text[:50]}{'...' if len(link_text) > 50 else ''}'\")\n                print(f\"    URL: {link_url[:80]}{'...' if len(link_url) > 80 else ''}\")\n            \n            print(f\"\\n\ud83d\udcca Link Classification (first 5):\")\n            print(f\"   Internal/Relative: {internal_links}\")\n            print(f\"   External: {external_links}\")\n            print(f\"   Archive links: {archive_links}\")\n            \n        else:\n            print(\"\u274c No links with href found\")\n            print(\"\ud83d\udca1 This might mean:\")\n            print(\"   - Page has no links\")\n            print(\"   - Links use different attributes\")\n            print(\"   - Page didn't load properly\")\n            \nexcept Exception as e:\n    print(f\"\u274c Error finding links: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5b: Filter for historical document links**\n",
    "\n",
    "Let's find links that point to historical archives or documents:\n",
    "\n",
    "Copy this code:\n",
    "```python\n",
    "# Define domains that often contain historical documents\n",
    "historical_domains = ['archive.org', 'canadiana.ca', 'gutenberg.org', 'biographi.ca']\n",
    "\n",
    "# Filter links\n",
    "document_links = []\n",
    "for link in all_links:\n",
    "    href = link.get('href', '')\n",
    "    # Check if any historical domain is in the URL\n",
    "    for domain in historical_domains:\n",
    "        if domain in href:\n",
    "            document_links.append({\n",
    "                'text': link.get_text().strip(),\n",
    "                'url': href\n",
    "            })\n",
    "            break  # Don't add the same link twice\n",
    "\n",
    "print(f\"Historical document links found: {len(document_links)}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Filter for historical document links with validation\ntry:\n    if 'all_links' not in locals():\n        print(\"\u274c Links not found - run the previous link extraction cell first\")\n    else:\n        # Define domains that often contain historical documents\n        historical_domains = ['archive.org', 'canadiana.ca', 'gutenberg.org', 'biographi.ca', 'bac-lac.gc.ca']\n        \n        # Filter links\n        document_links = []\n        domain_counts = {}\n        \n        for link in all_links:\n            href = link.get('href', '')\n            link_text = link.get_text().strip()\n            \n            # Check if any historical domain is in the URL\n            for domain in historical_domains:\n                if domain in href:\n                    document_links.append({\n                        'text': link_text,\n                        'url': href,\n                        'domain': domain\n                    })\n                    \n                    # Count by domain\n                    domain_counts[domain] = domain_counts.get(domain, 0) + 1\n                    break  # Don't add the same link twice\n        \n        print(f\"\ud83c\udfdb\ufe0f Historical document analysis:\")\n        print(f\"   Total links checked: {len(all_links)}\")\n        print(f\"   Historical document links found: {len(document_links)}\")\n        \n        if document_links:\n            print(f\"\\n\ud83d\udcca By domain:\")\n            for domain, count in domain_counts.items():\n                print(f\"   {domain}: {count} links\")\n            \n            print(f\"\\n\ud83d\udcda Historical document links:\")\n            for i, link in enumerate(document_links[:5], 1):  # Show first 5\n                print(f\"\\n{i}. {link['text'][:60]}{'...' if len(link['text']) > 60 else ''}\")\n                print(f\"   Domain: {link['domain']}\")\n                print(f\"   URL: {link['url'][:80]}{'...' if len(link['url']) > 80 else ''}\")\n            \n            if len(document_links) > 5:\n                print(f\"\\n... and {len(document_links) - 5} more historical links\")\n                \n            # Validate link quality\n            valid_links = [link for link in document_links if link['text'] and len(link['text']) > 3]\n            print(f\"\\n\u2705 Quality check: {len(valid_links)}/{len(document_links)} links have meaningful text\")\n            \n        else:\n            print(\"\u274c No historical document links found\")\n            print(f\"\ud83d\udca1 Searched for these domains: {', '.join(historical_domains)}\")\n            print(\"   This might mean:\")\n            print(\"   - This page doesn't link to major historical archives\")\n            print(\"   - Links use different URL structures\")\n            print(\"   - Need to add more domain patterns\")\n            \nexcept Exception as e:\n    print(f\"\u274c Error filtering historical links: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5c: Display the historical links**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the historical document links we found\n",
    "try:\n",
    "    if 'document_links' not in locals():\n",
    "        print(\"\u274c Historical document links not available - run the filter cell first\")\n",
    "    elif not document_links:\n",
    "        print(\"\u2139\ufe0f No historical document links were identified yet.\")\n",
    "    else:\n",
    "        print(\"Historical Document Links:\")\n",
    "        print(\"=\" * 50)\n",
    "        for i, link in enumerate(document_links, 1):\n",
    "            print(f\"{i}. {link['text']}\")\n",
    "            print(f\"   URL: {link['url']}\")\n",
    "            print()\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Error displaying historical document links: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udd04 **Your Turn: Find PDF Links**\n",
    "\n",
    "Many historical documents are available as PDFs. Adapt the link-finding code to:\n",
    "\n",
    "1. Find all links that contain \".pdf\" in their href\n",
    "2. Store them in a list called `pdf_links`\n",
    "3. Print how many PDF links you found\n",
    "4. Display the first 3 PDF links\n",
    "\n",
    "Hint: Use `if '.pdf' in href:` to check for PDF links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Your exercise: Find PDF links\ntry:\n    if 'all_links' not in locals():\n        print(\"\u274c Links not found - run the previous link extraction cell first\")\n    else:\n        # Find all links that contain \".pdf\" in their href\n        pdf_links = []\n        \n        for link in all_links:\n            href = link.get('href', '')\n            link_text = link.get_text().strip()\n            \n            if '.pdf' in href.lower():\n                pdf_links.append({\n                    'text': link_text,\n                    'url': href\n                })\n        \n        print(f\"\ud83d\udcc4 PDF Link Analysis:\")\n        print(f\"   Total links checked: {len(all_links)}\")\n        print(f\"   PDF links found: {len(pdf_links)}\")\n        \n        if pdf_links:\n            print(f\"\\n\u2705 Found {len(pdf_links)} PDF documents!\")\n            \n            print(f\"\\n\ud83d\udccb First 3 PDF links:\")\n            for i, pdf in enumerate(pdf_links[:3], 1):\n                print(f\"\\n{i}. '{pdf['text'][:60]}{'...' if len(pdf['text']) > 60 else ''}'\")\n                print(f\"   URL: {pdf['url']}\")\n                \n                # Analyze URL for file type validation\n                if pdf['url'].lower().endswith('.pdf'):\n                    print(\"   \u2705 Direct PDF link\")\n                else:\n                    print(\"   \u26a0\ufe0f  URL contains .pdf but doesn't end with .pdf\")\n            \n            if len(pdf_links) > 3:\n                print(f\"\\n... and {len(pdf_links) - 3} more PDF links\")\n                \n            # Quality validation\n            valid_pdfs = [pdf for pdf in pdf_links if pdf['text'] and len(pdf['text']) > 3]\n            print(f\"\\n\ud83d\udcca Quality check: {len(valid_pdfs)}/{len(pdf_links)} PDF links have meaningful text\")\n            \n        else:\n            print(\"\u274c No PDF links found on this page\")\n            print(\"\ud83d\udca1 This might mean:\")\n            print(\"   - Page doesn't link to PDF documents\")\n            print(\"   - PDFs are embedded differently\")\n            print(\"   - Links use different file extensions\")\n            print(\"   - Try looking for links with 'download', 'document', or 'file' in text\")\n            \n            # Alternative search\n            print(\"\\n\ud83d\udd0d Searching for potential document links...\")\n            doc_keywords = ['download', 'document', 'file', 'report', 'manuscript']\n            potential_docs = []\n            \n            for link in all_links:\n                link_text = link.get_text().lower()\n                if any(keyword in link_text for keyword in doc_keywords):\n                    potential_docs.append(link.get_text().strip())\n            \n            if potential_docs:\n                print(f\"   Found {len(potential_docs)} links with document keywords:\")\n                for doc in potential_docs[:3]:\n                    print(f\"     - {doc[:60]}{'...' if len(doc) > 60 else ''}\")\n            else:\n                print(\"   No obvious document links found\")\n                \nexcept Exception as e:\n    print(f\"\u274c Error finding PDF links: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Internet Archive Metadata\n",
    "\n",
    "Internet Archive documents have rich metadata. Let's learn to extract it systematically.\n",
    "\n",
    "**Step 6a: Get an Internet Archive page**\n",
    "\n",
    "Let's work with our Saskatchewan document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get Internet Archive page with validation\nia_url = \"https://archive.org/details/saskatchewan00sask\"\n\nprint(\"\ud83c\udfdb\ufe0f Working with Internet Archive metadata...\")\nprint(f\"URL: {ia_url}\")\n\ntry:\n    # Use our respectful function with a bit longer delay for IA\n    ia_response = respectful_get(ia_url, delay=2)\n    \n    if ia_response:\n        print(f\"\u2705 Status Code: {ia_response.status_code}\")\n        print(\"\u2705 Internet Archive page loaded successfully!\")\n        \n        # Basic validation\n        if 'archive.org' in ia_response.url:\n            print(\"\u2705 Confirmed: This is an Internet Archive page\")\n        else:\n            print(\"\u26a0\ufe0f  Warning: Response URL doesn't match Internet Archive\")\n            \n        # Check content size\n        content_size = len(ia_response.text)\n        print(f\"\ud83d\udcc4 Content size: {content_size:,} characters\")\n        \n        if content_size > 10000:\n            print(\"\u2705 Substantial content received\")\n        else:\n            print(\"\u26a0\ufe0f  Warning: Less content than expected\")\n            \n    else:\n        print(\"\u274c Failed to load Internet Archive page\")\n        print(\"\ud83d\udca1 Possible issues:\")\n        print(\"   - Internet Archive servers busy\")\n        print(\"   - Network connectivity issues\")\n        print(\"   - Document no longer available\")\n        \nexcept Exception as e:\n    print(f\"\u274c Error loading Internet Archive page: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6b: Extract the document title**\n",
    "\n",
    "Internet Archive puts document titles in `<h1>` tags:\n",
    "\n",
    "Copy this code:\n",
    "```python\n",
    "title = ia_soup.find('h1')\n",
    "if title:\n",
    "    document_title = title.get_text().strip()\n",
    "    print(f\"Document Title: {document_title}\")\n",
    "else:\n",
    "    print(\"No title found\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Extract Internet Archive title with validation\ntry:\n    if 'ia_response' not in locals() or not ia_response:\n        print(\"\u274c Internet Archive response not available - run the previous cell first\")\n    else:\n        # Create soup for Internet Archive page\n        ia_soup = BeautifulSoup(ia_response.text, 'html.parser')\n        print(\"\u2705 Internet Archive soup created\")\n        \n        # Find title - IA uses h1 for main document title\n        title_tag = ia_soup.find('h1')\n        \n        if title_tag:\n            document_title = title_tag.get_text().strip()\n            print(f\"\ud83d\udcda Document Title: {document_title}\")\n            \n            # Validate title quality\n            if len(document_title) > 5:\n                print(\"\u2705 Title validation: Reasonable length\")\n            else:\n                print(\"\u26a0\ufe0f  Warning: Title seems very short\")\n                \n            # Check for expected keywords\n            title_lower = document_title.lower()\n            if 'saskatchewan' in title_lower:\n                print(\"\u2705 Content validation: Saskatchewan document confirmed\")\n            else:\n                print(\"\u26a0\ufe0f  Content note: Title doesn't contain 'Saskatchewan'\")\n                print(f\"   This might be normal - could be a more specific title\")\n                \n            # Check for historical time indicators\n            historical_indicators = ['history', 'historical', '19', '18', 'century']\n            found_indicators = [ind for ind in historical_indicators if ind in title_lower]\n            if found_indicators:\n                print(f\"\ud83d\udcd6 Historical indicators: {', '.join(found_indicators)}\")\n            \n        else:\n            print(\"\u274c No h1 title tag found\")\n            print(\"\ud83d\udca1 Trying alternative title methods...\")\n            \n            # Try alternative title extraction\n            title_alternatives = [\n                ia_soup.find('title'),  # HTML title tag\n                ia_soup.find('h2'),     # Secondary heading\n                ia_soup.find('div', class_='item-title')  # IA specific class\n            ]\n            \n            for alt_title in title_alternatives:\n                if alt_title:\n                    alt_text = alt_title.get_text().strip()\n                    if alt_text and len(alt_text) > 5:\n                        print(f\"\ud83d\udcda Alternative title found: {alt_text}\")\n                        break\n            else:\n                print(\"\u274c No alternative titles found\")\n                \nexcept Exception as e:\n    print(f\"\u274c Error extracting title: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6c: Find metadata fields**\n",
    "\n",
    "Internet Archive uses `<dt>` (definition term) and `<dd>` (definition description) tags for metadata:\n",
    "\n",
    "Copy this code:\n",
    "```python\n",
    "# Find metadata terms and values\n",
    "metadata_terms = ia_soup.find_all('dt')\n",
    "metadata_values = ia_soup.find_all('dd')\n",
    "\n",
    "print(f\"Metadata fields found: {len(metadata_terms)}\")\n",
    "print(f\"Metadata values found: {len(metadata_values)}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find metadata fields on the Internet Archive page\n",
    "try:\n",
    "    if 'ia_soup' not in locals():\n",
    "        print(\"\u274c Internet Archive soup not found - run the previous cell first\")\n",
    "    else:\n",
    "        metadata_terms = ia_soup.find_all('dt')\n",
    "        metadata_values = ia_soup.find_all('dd')\n",
    "        print(f\"\ud83d\uddc2\ufe0f Metadata terms found: {len(metadata_terms)}\")\n",
    "        print(f\"\ud83d\udcdd Metadata values found: {len(metadata_values)}\")\n",
    "        if metadata_terms and metadata_values:\n",
    "            print(\"\u2705 Metadata structure detected\")\n",
    "        else:\n",
    "            print(\"\u26a0\ufe0f  Warning: No metadata definition lists found\")\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Error collecting metadata fields: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6d: Extract and display metadata**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store metadata\n",
    "try:\n",
    "    if 'metadata_terms' not in locals() or 'metadata_values' not in locals():\n",
    "        print(\"\u274c Metadata fields not found - run the metadata extraction cell first\")\n",
    "    else:\n",
    "        metadata = {}\n",
    "        for term, value in zip(metadata_terms, metadata_values):\n",
    "            term_text = term.get_text().strip()\n",
    "            value_text = value.get_text().strip()\n",
    "            metadata[term_text] = value_text\n",
    "        if not metadata:\n",
    "            print(\"\u2139\ufe0f No metadata pairs were captured - the page structure may have changed\")\n",
    "        else:\n",
    "            print(\"Document Metadata:\")\n",
    "            print(\"=\" * 40)\n",
    "            important_fields = ['by', 'Publication date', 'Topics', 'Language']\n",
    "            for field in important_fields:\n",
    "                if field in metadata:\n",
    "                    value = metadata[field]\n",
    "                    if len(value) > 100:\n",
    "                        value = value[:100] + '...'\n",
    "                    print(f\"{field}: {value}\")\n",
    "            extra_fields = [key for key in metadata if key not in important_fields]\n",
    "            if extra_fields:\n",
    "                print()\n",
    "                print(f\"\ud83d\udd0d Additional metadata fields captured: {len(extra_fields)}\")\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Error building metadata dictionary: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udd04 **Your Turn: Create a Metadata Function**\n",
    "\n",
    "Now create a reusable function that extracts metadata from any Internet Archive document. Fill in the missing parts:\n",
    "\n",
    "```python\n",
    "def extract_ia_metadata(url):\n",
    "    \"\"\"Extract title and metadata from an Internet Archive document\"\"\"\n",
    "    # 1. Get the page with requests.get()\n",
    "    # 2. Create soup with BeautifulSoup\n",
    "    # 3. Extract title from h1 tag\n",
    "    # 4. Extract metadata from dt/dd tags\n",
    "    # 5. Return a dictionary with title and metadata\n",
    "```\n",
    "\n",
    "Test it with the University of Toronto annual report: `https://archive.org/details/annualreport191920nivuoft`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create a reusable Internet Archive metadata function\ndef extract_ia_metadata(url):\n    \"\"\"Extract title and metadata from an Internet Archive document\"\"\"\n    try:\n        print(f\"\ud83d\udd0d Analyzing Internet Archive URL: {url}\")\n        \n        # Extract item ID from URL\n        if '/details/' in url:\n            item_id = url.split('/details/')[-1]\n            print(f\"\ud83d\udccb Item ID extracted: {item_id}\")\n        else:\n            return {'error': 'Invalid Internet Archive URL format'}\n        \n        # Method 1: Try Internet Archive Python library (preferred)\n        try:\n            import internetarchive as ia\n            item = ia.get_item(item_id)\n            \n            # Extract key metadata\n            metadata = {\n                'method': 'IA Python Library',\n                'title': item.metadata.get('title', 'No title'),\n                'creator': item.metadata.get('creator', 'No creator'),\n                'date': item.metadata.get('date', 'No date'),\n                'subject': item.metadata.get('subject', 'No subject'),\n                'description': item.metadata.get('description', 'No description')[:200] + '...' if item.metadata.get('description') else 'No description',\n                'language': item.metadata.get('language', 'No language'),\n                'files_count': len(list(item.files))\n            }\n            \n            print(\"\u2705 Successfully extracted metadata using IA library\")\n            return metadata\n            \n        except ImportError:\n            print(\"\u26a0\ufe0f  IA library not available, falling back to web scraping...\")\n        except Exception as e:\n            print(f\"\u26a0\ufe0f  IA library failed ({e}), falling back to web scraping...\")\n        \n        # Method 2: Fallback to web scraping\n        response = respectful_get(url, delay=2)\n        if not response:\n            return {'error': 'Could not download page'}\n            \n        soup = BeautifulSoup(response.text, 'html.parser')\n        \n        # Extract metadata from HTML\n        title_tag = soup.find('h1')\n        title = title_tag.get_text().strip() if title_tag else 'No title found'\n        \n        # Try to find metadata fields\n        metadata_dict = {'method': 'Web Scraping', 'title': title}\n        \n        # Look for dt/dd metadata pairs\n        dt_tags = soup.find_all('dt')\n        dd_tags = soup.find_all('dd')\n        \n        for dt, dd in zip(dt_tags, dd_tags):\n            key = dt.get_text().strip().lower()\n            value = dd.get_text().strip()\n            \n            # Map common fields\n            if 'by' in key or 'creator' in key:\n                metadata_dict['creator'] = value\n            elif 'date' in key:\n                metadata_dict['date'] = value\n            elif 'topic' in key or 'subject' in key:\n                metadata_dict['subject'] = value\n            elif 'language' in key:\n                metadata_dict['language'] = value\n        \n        print(\"\u2705 Successfully extracted metadata using web scraping\")\n        return metadata_dict\n        \n    except Exception as e:\n        print(f\"\u274c Error extracting metadata: {e}\")\n        return {'error': str(e)}\n\n# Test the function with University of Toronto annual report\nuoft_url = \"https://archive.org/details/annualreport191920nivuoft\"\nprint(\"\ud83c\udf93 Testing with University of Toronto Annual Report 1919-20...\")\n\nresult = extract_ia_metadata(uoft_url)\n\nprint(f\"\\n\ud83d\udcca Extracted Metadata:\")\nprint(\"=\" * 50)\nfor key, value in result.items():\n    print(f\"{key.title()}: {value}\")\n\nprint(f\"\\n\ud83d\udca1 This function demonstrates two approaches:\")\nprint(f\"   1. Internet Archive Python library (preferred)\")\nprint(f\"   2. Web scraping with Beautiful Soup (fallback)\")\nprint(f\"   Real research projects should use both for robustness!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Final Challenge - Your Research Project\n",
    "\n",
    "Time to put it all together! Choose a Canadian historical source and conduct your own analysis.\n",
    "\n",
    "**Available sources:**\n",
    "- TPL Jesuit Relations blog: Research the historical quotes and themes\n",
    "- Saskatchewan History: Analyze the metadata and publication context\n",
    "- UofT Annual Report 1919-20: Extract institutional information\n",
    "\n",
    "**Your research steps:**\n",
    "1. Choose a source and research question\n",
    "2. Use the scraping techniques you've learned\n",
    "3. Extract specific information related to your question\n",
    "4. Present your findings"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Using Internet Archive library - much more efficient!\n",
    "print(\"\ud83c\udfdb\ufe0f Accessing Internet Archive with the Python library...\")\n",
    "\n",
    "try:\n",
    "    import internetarchive as ia\n",
    "except ImportError:\n",
    "    print(\"\u274c Internet Archive library not installed yet - run the installation cell below first\")\n",
    "else:\n",
    "    try:\n",
    "        item_id = \"saskatchewan00sask\"  # The ID from the URL\n",
    "        item = ia.get_item(item_id)\n",
    "        print(f\"\u2705 Successfully retrieved item: {item_id}\")\n",
    "        print(f\"\ud83d\udcda Title: {item.metadata.get('title', 'No title')}\")\n",
    "        print(f\"\ud83d\udcc5 Date: {item.metadata.get('date', 'No date')}\")\n",
    "        print(f\"\ud83d\udc64 Creator: {item.metadata.get('creator', 'No creator')}\")\n",
    "        print(f\"\ud83d\udcd6 Subject: {item.metadata.get('subject', 'No subject')}\")\n",
    "        files = list(item.files)\n",
    "        print()\n",
    "        print(f\"\ud83d\udcc1 Available files: {len(files)}\")\n",
    "        for i, file in enumerate(files[:5]):\n",
    "            file_name = file.get('name', 'Unknown')\n",
    "            file_format = file.get('format', 'Unknown')\n",
    "            file_size = file.get('size', 'Unknown')\n",
    "            print(f\"   {i+1}. {file_name} ({file_format}) - {file_size} bytes\")\n",
    "        if len(files) > 5:\n",
    "            print(f\"   ... and {len(files) - 5} more files\")\n",
    "        print()\n",
    "        print(\"\ud83d\ude80 Much easier than HTML scraping!\")\n",
    "        print(\"\ud83d\udca1 The IA library gives us clean, structured data instantly\")\n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Error accessing Internet Archive: {e}\")\n",
    "        print(\"\ud83d\udca1 This might be due to network issues or an unavailable item\")\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Install and import Internet Archive library\n# First install the library (run this once)\n!pip install internetarchive --quiet\n\nprint(\"\ud83d\udcda Installing Internet Archive Python library...\")\nprint(\"\u2705 Installation complete!\")\n\n# Import the library\ntry:\n    import internetarchive as ia\n    print(\"\u2705 Internet Archive library imported successfully!\")\n    print(\"\ud83d\udd17 This library provides direct, efficient access to IA collections\")\nexcept ImportError as e:\n    print(f\"\u274c Error importing Internet Archive library: {e}\")\n    print(\"\ud83d\udca1 Try running: pip install internetarchive\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 8: Introduction to Internet Archive Python Library\n\nWhile Beautiful Soup is excellent for scraping web pages, the Internet Archive provides a specialized Python library that makes accessing their collections much more efficient. This is perfect for large-scale historical research projects.\n\n**Why use the Internet Archive library?**\n- Direct access to metadata without scraping HTML\n- Faster downloads and better error handling\n- Access to full-text search capabilities\n- Bulk processing of large collections\n- Respects Internet Archive's preferred access methods",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Challenge: Your Historical Research Project\n",
    "\n",
    "# Available Canadian historical sources:\n",
    "sources = {\n",
    "    \"TPL Jesuit Relations\": {\n",
    "        \"url\": \"https://torontopubliclibrary.typepad.com/local-history-genealogy/2020/01/sainte-marie-among-the-hurons-selections-from-the-jesuit-relations-and-allied-documents.html\",\n",
    "        \"type\": \"Blog post with historical quotes\",\n",
    "        \"research_questions\": [\n",
    "            \"What themes appear in historical quotes?\",\n",
    "            \"How many external historical links are provided?\",\n",
    "            \"What time periods are mentioned?\"\n",
    "        ]\n",
    "    },\n",
    "    \"Saskatchewan History\": {\n",
    "        \"url\": \"https://archive.org/details/saskatchewan00sask\",\n",
    "        \"type\": \"Internet Archive document\",\n",
    "        \"research_questions\": [\n",
    "            \"What metadata is available about publication?\",\n",
    "            \"What file formats are provided?\",\n",
    "            \"Who was the creator/publisher?\"\n",
    "        ]\n",
    "    },\n",
    "    \"UofT Annual Report 1919-20\": {\n",
    "        \"url\": \"https://archive.org/details/annualreport191920nivuoft\",\n",
    "        \"type\": \"Institutional document\",\n",
    "        \"research_questions\": [\n",
    "            \"What institutional information is captured?\",\n",
    "            \"How is the document structured?\",\n",
    "            \"What historical context does it provide?\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\ud83d\udd2c Historical Web Scraping Research Project\")\n",
    "print(\"=\" * 50)\n",
    "print()\n",
    "print(\"\ud83d\udcda Available sources:\")\n",
    "for i, (name, info) in enumerate(sources.items(), 1):\n",
    "    print()\n",
    "    print(f\"{i}. {name}\")\n",
    "    print(f\"   Type: {info['type']}\")\n",
    "    print(f\"   URL: {info['url'][:60]}...\")\n",
    "    print(\"   Sample questions:\")\n",
    "    for q in info['research_questions']:\n",
    "        print(f\"     - {q}\")\n",
    "\n",
    "print()\n",
    "print(\"\ud83c\udfaf Your research steps:\")\n",
    "print(\"1. Choose a source and research question\")\n",
    "print(\"2. Use appropriate scraping techniques\")\n",
    "print(\"3. Extract and validate data\")\n",
    "print(\"4. Analyze and present findings\")\n",
    "\n",
    "chosen_source = \"TPL Jesuit Relations\"  # Change this to your choice\n",
    "my_url = sources[chosen_source][\"url\"]\n",
    "my_question = \"What historical themes appear in the quoted text?\"\n",
    "\n",
    "print()\n",
    "print(\"\ud83d\udccb Example Research Project:\")\n",
    "print(f\"Source: {chosen_source}\")\n",
    "print(f\"Question: {my_question}\")\n",
    "print(f\"URL: {my_url}\")\n",
    "\n",
    "print()\n",
    "print(\"\ud83d\udd0d Conducting research...\")\n",
    "\n",
    "theme_counts = {}\n",
    "\n",
    "try:\n",
    "    response = respectful_get(my_url)\n",
    "    if response:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        print(\"\u2705 Page successfully scraped\")\n",
    "        if \"themes\" in my_question.lower():\n",
    "            blockquotes = soup.find_all('blockquote')\n",
    "            paragraphs = soup.find_all('p')\n",
    "            print()\n",
    "            print(\"\ud83d\udcca Content Analysis:\")\n",
    "            print(f\"   Blockquotes found: {len(blockquotes)}\")\n",
    "            print(f\"   Paragraphs found: {len(paragraphs)}\")\n",
    "            all_text = soup.get_text().lower()\n",
    "            themes = {\n",
    "                'Religious': ['god', 'lord', 'jesus', 'prayer', 'church', 'faith'],\n",
    "                'Indigenous peoples': ['huron', 'savage', 'indian', 'native', 'tribe'],\n",
    "                'French colonial': ['french', 'france', 'jesuit', 'missionary'],\n",
    "                'Geographic': ['canada', 'new france', 'sainte-marie', 'ontario'],\n",
    "                'Temporal': ['1600', '1640', '1650', '17th century', 'century']\n",
    "            }\n",
    "            for theme_name, keywords in themes.items():\n",
    "                count = sum(all_text.count(keyword) for keyword in keywords)\n",
    "                if count > 0:\n",
    "                    theme_counts[theme_name] = count\n",
    "            if theme_counts:\n",
    "                print()\n",
    "                print(\"\ud83c\udfad Historical Themes Found:\")\n",
    "                for theme, count in sorted(theme_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "                    print(f\"   {theme}: {count} mentions\")\n",
    "            else:\n",
    "                print()\n",
    "                print(\"\u2139\ufe0f No theme keywords were detected - try adjusting the keyword lists\")\n",
    "        else:\n",
    "            print(\"\u2139\ufe0f Customize the analysis section to match your research question.\")\n",
    "        print()\n",
    "        print(\"\ud83d\udcdd Research Findings:\")\n",
    "        print(\"1. Successfully scraped Canadian historical content\")\n",
    "        print(\"2. Identified multiple historical themes in source material\" if theme_counts else \"2. Ready to adapt the analysis for your own question\")\n",
    "        print(\"3. Demonstrated effective web scraping for historical research\")\n",
    "        if theme_counts:\n",
    "            most_common = max(theme_counts.items(), key=lambda x: x[1])\n",
    "            print(f\"4. Most prominent theme: {most_common[0]} ({most_common[1]} mentions)\")\n",
    "    else:\n",
    "        print(\"\u274c Could not complete research - page unavailable\")\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Research error: {e}\")\n",
    "\n",
    "print()\n",
    "print(\"\ud83d\ude80 Now it's your turn!\")\n",
    "print(\"Customize this code for your own research question:\")\n",
    "print(\"1. Change 'chosen_source' to your preferred source\")\n",
    "print(\"2. Modify 'my_question' to your research interest\")\n",
    "print(\"3. Adapt the analysis code for your specific question\")\n",
    "print(\"4. Add your own themes, keywords, or analysis methods\")\n",
    "\n",
    "print()\n",
    "print(\"\ud83d\udca1 Remember to:\")\n",
    "print(\"- Validate your scraped data\")\n",
    "print(\"- Handle errors gracefully\")\n",
    "print(\"- Respect website terms of service\")\n",
    "print(\"- Cite your digital sources properly\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: What You've Learned\n",
    "\n",
    "\ud83c\udf89 **Congratulations!** You've mastered the fundamentals of web scraping for historical research:\n",
    "\n",
    "**Technical Skills:**\n",
    "- \u2705 Making web requests with `requests.get()`\n",
    "- \u2705 Parsing HTML with Beautiful Soup\n",
    "- \u2705 Targeting specific elements (`find`, `find_all`)\n",
    "- \u2705 Extracting text, links, and metadata\n",
    "- \u2705 Building reusable functions for research\n",
    "\n",
    "**Historical Sources:**\n",
    "- \u2705 Blog posts with embedded historical content\n",
    "- \u2705 Internet Archive documents with metadata\n",
    "- \u2705 Academic indexes and structured data\n",
    "\n",
    "**Research Methods:**\n",
    "- \u2705 Systematic content extraction\n",
    "- \u2705 Metadata analysis for document context\n",
    "- \u2705 Building reproducible research workflows\n",
    "\n",
    "**Next Steps:**\n",
    "In Notebook 3, you'll learn advanced text analysis techniques to find patterns in the historical data you've scraped. We'll also explore the Internet Archive Python library for more efficient access to large collections.\n",
    "\n",
    "**Remember:**\n",
    "- Always respect robots.txt and website terms of service\n",
    "- Cite your digital sources properly\n",
    "- Consider the limitations and context of digitized materials\n",
    "- Use these skills responsibly for legitimate research purposes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}