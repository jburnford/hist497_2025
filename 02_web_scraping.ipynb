{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tjp-E2UxtKdX"
      },
      "source": [
        "# **Notebook 2: Web Scraping for Historians**\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jburnford/hist497_2025/blob/main/02_web_scraping.ipynb)\n",
        "\n",
        "Welcome to web scraping! In this notebook, you'll learn to extract historical data from websites using Python. We'll work with real Canadian historical sources and build your skills step by step.\n",
        "\n",
        "**What you'll learn:**\n",
        "- How web pages are structured (HTML basics)\n",
        "- Using Beautiful Soup to extract specific content\n",
        "- Working with Canadian historical archives\n",
        "- Building reusable code for research\n",
        "\n",
        "**Ethics first:** Always respect websites. Check robots.txt files, don't overload servers, and respect copyright.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELhjjAF8tKdZ"
      },
      "source": [
        "## Step 1: Setting Up Our Tools\n",
        "\n",
        "Before we can scrape websites, we need to install and import the right libraries. Let's do this step by step.\n",
        "\n",
        "**First, install the libraries:**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: How to add delays between requests\n",
        "import time\n",
        "\n",
        "# Friendly User-Agent so cultural institutions know who is visiting\n",
        "DEFAULT_HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (compatible; HIST497-Student/1.0; +https://github.com/jburnford/hist497_2025)\"\n",
        "}\n",
        "\n",
        "def respectful_get(url, delay=1, headers=None):\n",
        "    \"\"\"\n",
        "    Make a web request with built-in delay for respectful scraping\n",
        "    \"\"\"\n",
        "    print(f\"‚è≥ Waiting {delay} second(s) before request...\")\n",
        "    time.sleep(delay)  # Wait before making request\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers or DEFAULT_HEADERS, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        print(f\"‚úÖ Request successful\")\n",
        "        return response\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"‚ùå Request failed: {e}\")\n",
        "        return None\n",
        "\n",
        "# We'll use this function for respectful scraping throughout the notebook\n",
        "print(\"Respectful scraping function defined!\")\n",
        "print(\"üí° This adds delays between requests to be considerate to servers and sends a helpful User-Agent.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GE4P4FP8tKdZ",
        "outputId": "65a9fcb9-7c9d-42a2-c27f-8386944bf7c4"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Respectful scraping function defined!\n",
            "üí° This adds delays between requests to be considerate to servers and sends a helpful User-Agent.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ü§ù Ethical Web Scraping Guidelines\n",
        "\n",
        "Before we start scraping, let's understand the ethics and best practices:\n",
        "\n",
        "**‚úÖ Always:**\n",
        "- Check robots.txt (add /robots.txt to any website URL)\n",
        "- Add delays between requests (don't overwhelm servers)\n",
        "- Use reasonable timeouts\n",
        "- Respect copyright and terms of service\n",
        "- Identify yourself with User-Agent headers when appropriate\n",
        "\n",
        "**‚ùå Never:**\n",
        "- Scrape faster than a human could browse\n",
        "- Ignore error messages or blocks\n",
        "- Scrape personal/private information\n",
        "- Violate website terms of service\n",
        "- Overload servers with rapid requests\n",
        "\n",
        "**üìñ For historical research:**\n",
        "- Many archives encourage responsible academic use\n",
        "- Always cite your digital sources properly\n",
        "- Consider contacting archives for bulk data access\n",
        "- Respect cultural sensitivities in historical materials"
      ],
      "metadata": {
        "id": "zmNRUYvatKda"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjIWIdZAtKdb",
        "outputId": "fe566064-3f5e-4090-f825-47c67491bbfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries installed successfully!\n"
          ]
        }
      ],
      "source": [
        "# This installs the libraries we need\n",
        "!pip install requests beautifulsoup4 --quiet\n",
        "print(\"Libraries installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JWFkFH3tKdb"
      },
      "source": [
        "**Now, import them so we can use them:**\n",
        "\n",
        "Copy this code into the cell below:\n",
        "```python\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "```\n",
        "If you don't get an error and nothing happens after you click the run button, it worked."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRS-pfRLtKdc",
        "outputId": "e279808e-9472-4922-ec61-4bb2958d3fa4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported successfully!\n",
            "‚úÖ requests: Downloads web pages\n",
            "‚úÖ BeautifulSoup: Parses HTML content\n"
          ]
        }
      ],
      "source": [
        "# Import the libraries we need for web scraping\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(\"‚úÖ requests: Downloads web pages\")\n",
        "print(\"‚úÖ BeautifulSoup: Parses HTML content\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6N-MBvRtKdc"
      },
      "source": [
        "## Step 2: Your First Web Request\n",
        "\n",
        "Let's start by downloading a web page. We'll use a Saskatchewan encyclopedia article about the Rupert's Land purchase.\n",
        "\n",
        "Libraries sometimes block anonymous scraping, so we send a friendly `User-Agent` string that identifies this class.\n",
        "\n",
        "**Step 2a: Define the URL**\n",
        "\n",
        "Copy this code:\n",
        "```python\n",
        "url = \"https://esask.uregina.ca/entry/ruperts_land_purchase.html\"\n",
        "print(f\"We're going to scrape: {url}\")\n",
        "print(\"üìö This is an Encyclopedia of Saskatchewan article about the Rupert's Land purchase\")\n",
        "print(\"üèîÔ∏è Great practice for prairie-focused historical research!\")\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDmpyX-ItKdd",
        "outputId": "78ea58fd-13ab-447b-88da-d8d1b0db832b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We're going to scrape: https://esask.uregina.ca/entry/ruperts_land_purchase.html\n",
            "üìö This is an Encyclopedia of Saskatchewan article about the Rupert's Land purchase\n",
            "üèîÔ∏è Great practice for prairie-focused historical research!\n"
          ]
        }
      ],
      "source": [
        "# Define the URL for our Canadian historical source\n",
        "url = \"https://esask.uregina.ca/entry/ruperts_land_purchase.html\"\n",
        "print(f\"We're going to scrape: {url}\")\n",
        "print(\"üìö This is an Encyclopedia of Saskatchewan article about the Rupert's Land purchase\")\n",
        "print(\"üèîÔ∏è Great practice for prairie-focused historical research!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2elztToztKde"
      },
      "source": [
        "**Step 2b: Download the page**\n",
        "\n",
        "Now let's actually download the web page. The `requests.get()` function fetches the page for us. We include the same `DEFAULT_HEADERS` so the site recognizes us as a respectful browser.\n",
        "\n",
        "Copy this code:\n",
        "```python\n",
        "response = requests.get(url, headers=DEFAULT_HEADERS)\n",
        "print(f\"Status Code: {response.status_code}\")  # 200 means success\n",
        "print(f\"Page downloaded! It contains {len(response.text)} characters.\")\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzQcDssotKde",
        "outputId": "08c444d7-501d-4441-a531-c056763164e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Success! Status Code: 200\n",
            "üìÑ Page downloaded! It contains 9,457 characters.\n"
          ]
        }
      ],
      "source": [
        "# Download the page with error handling\n",
        "try:\n",
        "    response = requests.get(url, headers=DEFAULT_HEADERS, timeout=10)  # 10 second timeout\n",
        "    response.raise_for_status()  # Raises an exception for bad status codes\n",
        "    print(f\"‚úÖ Success! Status Code: {response.status_code}\")\n",
        "    print(f\"üìÑ Page downloaded! It contains {len(response.text):,} characters.\")\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"‚ùå Error downloading page: {e}\")\n",
        "    print(\"üí° This could be due to:\")\n",
        "    print(\"   - No internet connection\")\n",
        "    print(\"   - Website is down\")\n",
        "    print(\"   - URL has changed\")\n",
        "    print(\"   - Server is blocking requests\")\n",
        "    print(\"   - Try sending headers=DEFAULT_HEADERS to share a friendly User-Agent\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqJRsfnBtKde"
      },
      "source": [
        "**Step 2c: Preview the HTML**\n",
        "\n",
        "Can you find the title and main article content?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUf3R1ittKde",
        "outputId": "b71030d7-05e0-44f0-afbd-8d66c6ae4f72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÑ Previewing Encyclopedia of Saskatchewan HTML...\n",
            "--------------------------------------------------\n",
            "<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n",
            "<html xmlns=\"http://www.w3.org/1999/xhtml\">\n",
            "<head>\n",
            "<title>The Encyclopedia of Saskatchewan | Details</title>\n",
            "<!--<base href=\"http://esask.uregina.ca/\">-->\n",
            "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\">\n",
            "\n",
            "<meta name=\"author\" content=\"Cory Toth - Encyclopedia Of Saskatchewan\">\n",
            "<meta name=\"robots\" content=\"INDEX, FOLLOW\">\n",
            "<meta name=\"document-state\" content=\"Dynamic\">\n",
            "<link href=\"../assets/css/global.css\" rel=\"stylesheet\" type=\"text/css\">\n",
            "<script language=\"JavaScript\" src=\"../assets/js/mercury.js\" type=\"text/javascript\"></script>\n",
            "<script language=\"JavaScript\" src=\"../assets/js/niftycube.js\" type=\"text/javascript\"></script>\n",
            "<script type=\"text/javascript\">window.onload=function(){Nifty(\"div.rounded,div.h1,span.h1,h1,.h2,h2,span.btn\");}</script>\n",
            "</head>\n",
            "<body>\n",
            "<%@include file=\"menu.html\" %>\n",
            "</div>\n",
            "<div class=\"ftv_box rounded\"><a title=\"\" href=\"../about_encyclopedia.html\" target=\"_self\" name=\"About the Encyclopedia\"><img style=\"MARGIN-BOTTOM: 5px\" height=\"17\" alt=\"\" src=\"../assets/img/title_ftv.gif\" width=\"140\" border=\"0\" /></a><br />\n",
            "  <div class=\"ftv_box_inset rounded\">Welcome to the Encyclopedia of Saskatchewan. For assistance in exploring this site, please <a title=\"\" href=\"../about_encyclopedia.html\" target=\"_self\" name=\"About the Encyclopedia\">click here</a>.</div>\n",
            "</div>\n",
            "</div>\n",
            "<div class=\"\">\n",
            "  <div class=\"ftv_box rounded\"> <img style=\"margin-bottom: 4px\" height=\"17\" alt=\"\" src=\"../assets/img/title_rels.gif\" width=\"140\" border=\"0\" /><br />\n",
            "    <ul>\n",
            "      <li><a href=\"../themelist-themeID=8858188B-BCD4-8C82-18ECA6F5C9727ECA.html\">History</a></li>\n",
            "    </ul>\n",
            "  </div>\n",
            "  <div style=\"visibility: hidden\"> <img style=\"margin-bottom: 4px\" height=\"17\" alt=\"\" src=\"../assets/img/title_fdbck.gif\" border=\"0\" /><br />\n",
            "    If you have feedback regarding this entry please fill out our <a href=\"../contactus-A17790EC_1560_95DA_4395D0BAC2E99F14=Rupert%E2%80%99s%20Land%20Purchase.html\" style=\"color:white;\">feedback form</a>. </div>\n",
            "</div>\n",
            "</td>\n",
            "<td id=\"maincol\"><div class=\"submain_col rounded\"> \n",
            "    <script type=\"text/javascript\" src=\"../management/app/assets/js/lightbox/prototype.js\"></script> \n",
            "    <script type=\"text/javascript\" src=\"../management/app/assets/js/lightbox/scriptaculous-load=effects.js\"></script> \n",
            "    <script type=\"text/javascript\" src=\"../management/app/assets/js/lightbox/lightbox.js\"></script>\n",
            "    <link rel=\"stylesheet\" href=\"../management/app/assets/css/lightbox.css\" type=\"text/css\" media=\"screen\" />\n",
            "    <style>\n",
            "#imgBxleft {\n",
            "\t\tborder: 1px solid #3e4526;\n",
            "\t\tbackground-color: #e6f7e1;\n",
            "\t\tmargin-bottom: 10px;\n",
            "\t\tmargin-top: 10px;\n",
            "\t\tfloat: left;\t\n",
            "\t\tmargin-right: 15px;\n",
            "\t\tcolor: black;\n",
            "}\n",
            "\n",
            "#imgBxright {\n",
            "\t\tborder: 1px solid #3e4526;\n",
            "\t\tbackground-color: #e6f7e1;\n",
            "\t\tmargin-bottom: 10px;\n",
            "\t\tmargin-top: 10px;\n",
            "\t\tfloat: right;\t\n",
            "\t\tmargin-left: 15px;\n",
            "\t\tcolor: black;\n",
            "}\n",
            "#caption2 {\n",
            "\tfont-weight: bold;\t\t\n",
            "\tmargin-left: 5px;\n",
            "\tcolor: #3e4526\n",
            "}\n",
            "\n",
            "#Iauthor {\n",
            "\tfont-style: italic;\n",
            "\tmargin-left: 5px;\n",
            "\tmargin-bottom: 5px;\n",
            "}\n",
            "#entry img {\n",
            "\tmargin-bottom: 5px;\n",
            "\tborder-bottom: 1px solid #3e4526;\n",
            "}\n",
            "#entry p {\n",
            "\ttext-align: justify;\t\t\n",
            "}\n",
            "</style>\n",
            "    <div class=\"article_box rounded\">\n",
            "      <h1>Rupert's Land Purchase</h1>\n",
            "      <p> Before Saskatchewan became a province, it was part of the North-West Territories and its geographic and economic future was determined by the sale of Rupert's Land. Rupert's Land, the territory granted by the British Crown to the Hudson's Bay Company (HBC) in 1670, was purchased by the government of Canada in 1870: approximately 3 million hectares (or 7 million acres) were purchased for $1.5 million in Canadian currency (√Ç¬£300,000). The HBC was granted one-twentieth of the best farmland in the region, and the company held on to its most successful fur-trading operations. The Rupert's Land Purchase drastically altered the historic relationships that Saskatchewan M&eacute;tis and First Nations peoples had with the land, the Canadian government, and the social environment in the prairie region. Indian and M&eacute;tis people, who were not consulted about the sale, were seen as a deterrent to successful settlement of the west. The M&eacute;tis, led by <a href=\"riel_louis_david_1844-85.html\" class=\"tag\">Louis Riel</a>, successfully negotiated Manitoba's entry into Confederation in 1870: they were promised title to the lands they farmed and an additional 1.4 million acres for their children. However, the promises made by John A Macdonald and the Liberal government were not kept; the Manitoba Resistance of 1869-70 ended with the dispersal of the M&eacute;tis people to northern Saskatchewan to rebuild their communities. </p>\n",
            "      <p>The M&eacute;tis people settled on the land, establishing their traditional patterns of hunting, trapping, and farming; their customs, languages, beliefs, and community systems became part of Saskatchewan's social landscape. <a href=\"metis_women.html\" class=\"tag\">\n",
            "--------------------------------------------------\n",
            "üí° Notice header navigation, metadata, and the main article content.\n"
          ]
        }
      ],
      "source": [
        "# Show the first 5000 characters so we can inspect the HTML\n",
        "print(\"üìÑ Previewing Encyclopedia of Saskatchewan HTML...\")\n",
        "print(\"-\" * 50)\n",
        "print(response.text[:5000])\n",
        "print(\"-\" * 50)\n",
        "print(\"üí° Notice header navigation, metadata, and the main article content.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4NvbBoQtKdf"
      },
      "source": [
        "### üîÑ **Your Turn: Adaptation Exercise**\n",
        "\n",
        "Now try the same steps with a different Canadian source. Copy the code from above and modify it to use this Internet Archive document about Saskatchewan:\n",
        "\n",
        "```python\n",
        "url = \"https://archive.org/details/saskatchewan00sask\"\n",
        "```\n",
        "\n",
        "Follow the same steps: define the URL, download the page with `headers=DEFAULT_HEADERS`, check the status code, and look at the first 500 characters using `print(response.text[:500])`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7R1rSr7ZtKdf"
      },
      "outputs": [],
      "source": [
        "# Your adaptation exercise here\n",
        "# 1. Define the Saskatchewan URL\n",
        "# 2. Download the page with requests.get()\n",
        "# 3. Print the status code and character count\n",
        "# 4. Show first 500 characters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wyU3Nr4tKdf"
      },
      "source": [
        "## Step 3: Making Sense of HTML with Beautiful Soup\n",
        "\n",
        "Raw HTML is hard to work with. Beautiful Soup parses it and makes it easy to extract what we need.\n",
        "\n",
        "**Step 3a: Create your first \"soup\"**\n",
        "\n",
        "Let's go back to our Encyclopedia article and parse it properly:\n",
        "\n",
        "Copy this code:\n",
        "```python\n",
        "# Go back to the Encyclopedia article\n",
        "url = \"https://esask.uregina.ca/entry/ruperts_land_purchase.html\"\n",
        "response = requests.get(url, headers=DEFAULT_HEADERS)\n",
        "\n",
        "# Create the soup object\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "print(\"Soup object created! Now we can easily extract content.\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEHLKgNNtKdf",
        "outputId": "f269cde5-a619-4f9f-f3ae-f09a49db3d5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Soup object created! Now we can easily extract content.\n"
          ]
        }
      ],
      "source": [
        "# Go back to the Encyclopedia article\n",
        "url = \"https://esask.uregina.ca/entry/ruperts_land_purchase.html\"\n",
        "response = requests.get(url, headers=DEFAULT_HEADERS)\n",
        "\n",
        "# Create the soup object\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "print(\"Soup object created! Now we can easily extract content.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWBiqZgmtKdf"
      },
      "source": [
        "**Step 3b: Extract the page title**\n",
        "\n",
        "Let's start simple by getting the page title. In HTML, the title is in `<title>` tags.\n",
        "\n",
        "Copy this code:\n",
        "```python\n",
        "title = soup.find('title')\n",
        "print(f\"Page title: {title.get_text()}\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyIJGfjstKdg",
        "outputId": "1a0719df-5e67-4c7b-97a5-37698a25041b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page title: The Encyclopedia of Saskatchewan | Details\n"
          ]
        }
      ],
      "source": [
        "title = soup.find('title')\n",
        "print(f\"Page title: {title.get_text()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCqqwEy1tKdg"
      },
      "source": [
        "**Step 3c: Get clean text (no HTML tags)**\n",
        "\n",
        "The `.get_text()` method removes all HTML tags and gives us just the readable content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1raLFnAtKdg",
        "outputId": "d4de5c3c-749b-4110-d60d-37e1c31f2c02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean text length: 3778 characters\n",
            "\n",
            "First 500 characters of clean text:\n",
            "\n",
            "\n",
            "\n",
            "The Encyclopedia of Saskatchewan | Details\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "<%@include file=\"menu.html\" %>\n",
            "\n",
            "\n",
            "Welcome to the Encyclopedia of Saskatchewan. For assistance in exploring this site, please click here.\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "History\n",
            "\n",
            "\n",
            " \n",
            "    If you have feedback regarding this entry please fill out our feedback form. \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Rupert's Land Purchase\n",
            " Before Saskatchewan became a province, it was part of the North-West Territories and its geographic and economic future was determined by the sale of Rupert's Land. Rupert's Land, the territory granted by the British Crown to the Hudson's Bay Company (HBC) in 1670, was purchased by the government of Canada in 1870: approximately 3 million hectares (or 7 million acres) were purchased for $1.5 million in Canadian currency (√Ç¬£300,000). The HBC was granted one-twentieth of the best farmland in the region, and the company held on to its most successful fur-trading operations. The Rupert's Land Purchase drastically altered the historic relationships that Saskatchewan M√©tis and First Nations peoples had with the land, the Canadian government, and the social environment in the prairie region. Indian and M√©tis people, who were not consulted about the sale, were seen as a deterrent to successful settlement of the west. The M√©tis, led by Louis Riel, successfully negotiated Manitoba's entry into Confederation in 1870: they were promised title to the lands they farmed and an additional 1.4 million acres for their children. However, the promises made by John A Macdonald\n",
            "\n",
            "Much better! Now we can read the actual content.\n"
          ]
        }
      ],
      "source": [
        "# Extract all text content without HTML tag\n",
        "clean_text = soup.get_text()\n",
        "print(f\"Clean text length: {len(clean_text)} characters\")\n",
        "print()\n",
        "print(\"First 1500 characters of clean text:\")\n",
        "print(clean_text[:1500])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYTddMDvtKdg"
      },
      "source": [
        "### üîÑ **Your Turn: Practice with Internet Archive**\n",
        "\n",
        "Now apply the same Beautiful Soup steps to the Saskatchewan document. Copy and adapt the code above:\n",
        "\n",
        "1. Create a soup object from the Saskatchewan URL\n",
        "2. Extract and print the title\n",
        "3. Get the clean text and show the first 500 characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "qWZkp0kHtKdg"
      },
      "outputs": [],
      "source": [
        "# Your practice: Apply Beautiful Soup to Saskatchewan document\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dw6u9m8ctKdh"
      },
      "source": [
        "## Step 4: Targeting Specific Content\n",
        "\n",
        "Getting all the text is useful, but often we want specific parts. Let's learn to target particular HTML elements.\n",
        "\n",
        "**Step 4a: Find all paragraphs**\n",
        "\n",
        "Blog posts organize content in paragraphs (`<p>` tags). Let's find them:\n",
        "\n",
        "Copy this code:\n",
        "```python\n",
        "# Go back to our Encyclopedia article soup\n",
        "url = \"https://esask.uregina.ca/entry/ruperts_land_purchase.html\"\n",
        "response = requests.get(url, headers=DEFAULT_HEADERS)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Find all paragraph tags\n",
        "paragraphs = soup.find_all('p')\n",
        "print(f\"Found {len(paragraphs)} paragraphs\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRbl_KsZtKdh",
        "outputId": "39bf8e02-850c-49ba-f3d3-af2723c8b7cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4 paragraphs\n"
          ]
        }
      ],
      "source": [
        "# Go back to our Encyclopedia article soup\n",
        "url = \"https://esask.uregina.ca/entry/ruperts_land_purchase.html\"\n",
        "response = requests.get(url, headers=DEFAULT_HEADERS)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Find all paragraph tags\n",
        "paragraphs = soup.find_all('p')\n",
        "print(f\"Found {len(paragraphs)} paragraphs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJBPWwC_tKdh"
      },
      "source": [
        "**Step 4b: Look at individual paragraphs**\n",
        "\n",
        "Let's examine a few paragraphs to see what we're working with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9HpLhMWtKdh",
        "outputId": "036ba5dc-2477-4849-9d11-8c119a2293d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 3 paragraphs:\n",
            "\n",
            "Paragraph 1: Before Saskatchewan became a province, it was part of the North-West Territories and its geographic ...\n",
            "\n",
            "Paragraph 2: The M√©tis people settled on the land, establishing their traditional patterns of hunting, trapping, ...\n",
            "\n",
            "Paragraph 3: The Rupert's Land Purchase also adversely affected Indian populations in the North-West Territories ...\n"
          ]
        }
      ],
      "source": [
        "# Look at the first few paragraphs with validation\n",
        "try:\n",
        "    if 'paragraphs' not in locals():\n",
        "        print(\"‚ùå Paragraphs not found - run the paragraph extraction cell first\")\n",
        "    elif not paragraphs:\n",
        "        print(\"‚ö†Ô∏è  No paragraph tags available to display\")\n",
        "    else:\n",
        "        print(\"First 3 paragraphs:\")\n",
        "        for i in range(min(3, len(paragraphs))):\n",
        "            para_text = paragraphs[i].get_text().strip()\n",
        "            preview = para_text[:100] + ('...' if len(para_text) > 100 else '')\n",
        "            print()\n",
        "            print(f\"Paragraph {i+1}: {preview}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error previewing paragraphs: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eq-dfTrgtKdh"
      },
      "source": [
        "**Step 4c: Filter for substantial paragraphs**\n",
        "\n",
        "Many paragraphs are short or empty. Let's filter for substantial ones (more than 50 characters):\n",
        "\n",
        "Copy this code:\n",
        "```python\n",
        "# Filter for paragraphs with meaningful content\n",
        "substantial_paras = []\n",
        "for para in paragraphs:\n",
        "    text = para.get_text().strip()\n",
        "    if len(text) > 50:  # Only paragraphs with more than 50 characters\n",
        "        substantial_paras.append(text)\n",
        "\n",
        "print(f\"Substantial paragraphs: {len(substantial_paras)}\")\n",
        "print(f\"\\nFirst substantial paragraph:\")\n",
        "print(substantial_paras[0])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2Gk6T3ttKdi",
        "outputId": "1499962e-59f3-4f10-8b01-a5c196180118"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Substantial paragraphs: 3\n",
            "\n",
            "First substantial paragraph:\n",
            "Before Saskatchewan became a province, it was part of the North-West Territories and its geographic and economic future was determined by the sale of Rupert's Land. Rupert's Land, the territory granted by the British Crown to the Hudson's Bay Company (HBC) in 1670, was purchased by the government of Canada in 1870: approximately 3 million hectares (or 7 million acres) were purchased for $1.5 million in Canadian currency (√Ç¬£300,000). The HBC was granted one-twentieth of the best farmland in the region, and the company held on to its most successful fur-trading operations. The Rupert's Land Purchase drastically altered the historic relationships that Saskatchewan M√©tis and First Nations peoples had with the land, the Canadian government, and the social environment in the prairie region. Indian and M√©tis people, who were not consulted about the sale, were seen as a deterrent to successful settlement of the west. The M√©tis, led by Louis Riel, successfully negotiated Manitoba's entry into Confederation in 1870: they were promised title to the lands they farmed and an additional 1.4 million acres for their children. However, the promises made by John A Macdonald and the Liberal government were not kept; the Manitoba Resistance of 1869-70 ended with the dispersal of the M√©tis people to northern Saskatchewan to rebuild their communities.\n"
          ]
        }
      ],
      "source": [
        "# Filter for paragraphs with meaningful content\n",
        "substantial_paras = []\n",
        "for para in paragraphs:\n",
        "    text = para.get_text().strip()\n",
        "    if len(text) > 50:  # Only paragraphs with more than 50 characters\n",
        "        substantial_paras.append(text)\n",
        "\n",
        "print(f\"Substantial paragraphs: {len(substantial_paras)}\")\n",
        "print(f\"\\nFirst substantial paragraph:\")\n",
        "print(substantial_paras[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K44911JjtKdi"
      },
      "source": [
        "### üîÑ **Your Turn: Find Historical Quotes**\n",
        "There are not block quotes in this article.\n",
        "\n",
        "Historical articles often include quotes in special `<blockquote>` tags. Adapt the code above to:\n",
        "\n",
        "1. Find all blockquote elements using `soup.find_all('blockquote')`\n",
        "2. Extract their text and print them\n",
        "3. Count how many historical quotes you found\n",
        "\n",
        "Use the same Encyclopedia article soup object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "se3t4ukOtKdi",
        "outputId": "ce2733d5-91cd-4f89-8f65-a817c6e74312"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìú Found 0 blockquote elements\n",
            "‚ùå No blockquotes found on this page\n",
            "üí° This might mean:\n",
            "   - This site doesn't use blockquotes for quotes\n",
            "   - Quotes might be in other tags (div, p with special classes)\n",
            "   - Page structure is different than expected\n"
          ]
        }
      ],
      "source": [
        "# Your exercise: Find historical quotes in blockquotes\n",
        "try:\n",
        "    if 'soup' not in locals():\n",
        "        print(\"‚ùå Soup object not found - run the Beautiful Soup creation cells first\")\n",
        "    else:\n",
        "        # Find all blockquote elements\n",
        "        blockquotes = soup.find_all('blockquote')\n",
        "        print(f\"üìú Found {len(blockquotes)} blockquote elements\")\n",
        "\n",
        "        if blockquotes:\n",
        "            print(\"‚úÖ Historical quotes found!\")\n",
        "            print(\"\\nüîç Analyzing each quote:\")\n",
        "\n",
        "            valid_quotes = []\n",
        "            for i, quote in enumerate(blockquotes, 1):\n",
        "                quote_text = quote.get_text().strip()\n",
        "\n",
        "                if quote_text and len(quote_text) > 10:  # Filter out very short quotes\n",
        "                    valid_quotes.append(quote_text)\n",
        "                    print(f\"\\nQuote {i} ({len(quote_text)} characters):\")\n",
        "                    print(\"-\" * 50)\n",
        "                    # Show first 200 characters of quote\n",
        "                    display_text = quote_text[:200] + \"...\" if len(quote_text) > 200 else quote_text\n",
        "                    print(display_text)\n",
        "                    print(\"-\" * 50)\n",
        "\n",
        "                    # Check for historical indicators\n",
        "                    quote_lower = quote_text.lower()\n",
        "                    historical_indicators = ['rupert', 'hudson', 'company', 'indigenous', 'metis', 'cree', 'treaty', '1869', '1870']\n",
        "                    found_indicators = [ind for ind in historical_indicators if ind in quote_lower]\n",
        "\n",
        "                    if found_indicators:\n",
        "                        print(f\"üìö Historical indicators found: {', '.join(found_indicators)}\")\n",
        "                    else:\n",
        "                        print(\"üìù No obvious historical indicators\")\n",
        "                else:\n",
        "                    print(f\"Quote {i}: (too short or empty)\")\n",
        "\n",
        "            print(f\"\\nüìä Summary:\")\n",
        "            print(f\"   Total blockquotes: {len(blockquotes)}\")\n",
        "            print(f\"   Valid historical quotes: {len(valid_quotes)}\")\n",
        "\n",
        "            if valid_quotes:\n",
        "                avg_length = sum(len(q) for q in valid_quotes) / len(valid_quotes)\n",
        "                print(f\"   Average quote length: {avg_length:.0f} characters\")\n",
        "\n",
        "        else:\n",
        "            print(\"‚ùå No blockquotes found on this page\")\n",
        "            print(\"üí° This might mean:\")\n",
        "            print(\"   - This site doesn't use blockquotes for quotes\")\n",
        "            print(\"   - Quotes might be in other tags (div, p with special classes)\")\n",
        "            print(\"   - Page structure is different than expected\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error finding quotes: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoqpO6OotKdi"
      },
      "source": [
        "## Step 5: Working with Links\n",
        "\n",
        "Historical sources often link to primary documents. Let's learn to extract and analyze links.\n",
        "\n",
        "**Step 5a: Find all links**\n",
        "\n",
        "Links are in `<a>` tags with `href` attributes. Let's find them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52XKQVzntKdi",
        "outputId": "13a33656-a07f-4766-9e73-fbbd3898f4b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîó Found 73 links with href attributes\n",
            "‚úÖ Link extraction successful\n",
            "\n",
            "üîç Analyzing first 5 links:\n",
            "\n",
            "1. [External] ''\n",
            "    URL: https://activehistory.ca/\n",
            "\n",
            "2. [External] 'Active History'\n",
            "    URL: https://activehistory.ca/\n",
            "\n",
            "3. [External] 'Home'\n",
            "    URL: http://activehistory.ca/\n",
            "\n",
            "4. [External] 'About'\n",
            "    URL: https://activehistory.ca/about/\n",
            "\n",
            "5. [External] 'Features'\n",
            "    URL: https://activehistory.ca/papers/\n",
            "\n",
            "üìä Link Classification (first 5):\n",
            "   Internal/Relative: 0\n",
            "   External: 5\n",
            "   Archive links: 0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Go back to our Encyclopedia article soup\n",
        "url = \"https://activehistory.ca/blog/2010/07/05/remembering-oka/\"\n",
        "response = requests.get(url, headers=DEFAULT_HEADERS)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "# Find all links with validation\n",
        "try:\n",
        "    if 'soup' not in locals():\n",
        "        print(\"‚ùå Soup object not found - run the Beautiful Soup creation cells first\")\n",
        "    else:\n",
        "        # Find all links with href attributes\n",
        "        all_links = soup.find_all('a', href=True)\n",
        "        print(f\"üîó Found {len(all_links)} links with href attributes\")\n",
        "\n",
        "        if all_links:\n",
        "            print(\"‚úÖ Link extraction successful\")\n",
        "\n",
        "            # Analyze link types\n",
        "            internal_links = 0\n",
        "            external_links = 0\n",
        "            archive_links = 0\n",
        "\n",
        "            print(\"\\nüîç Analyzing first 5 links:\")\n",
        "            for i, link in enumerate(all_links[:5]):\n",
        "                link_text = link.get_text().strip()\n",
        "                link_url = link.get('href')\n",
        "\n",
        "                # Classify link type\n",
        "                if link_url.startswith('http'):\n",
        "                    if 'esask.uregina.ca' in link_url:\n",
        "                        link_type = \"Internal\"\n",
        "                        internal_links += 1\n",
        "                    elif 'archive.org' in link_url:\n",
        "                        link_type = \"Archive\"\n",
        "                        archive_links += 1\n",
        "                    else:\n",
        "                        link_type = \"External\"\n",
        "                        external_links += 1\n",
        "                else:\n",
        "                    link_type = \"Relative\"\n",
        "                    internal_links += 1\n",
        "\n",
        "                print(f\"\\n{i+1}. [{link_type}] '{link_text[:50]}{'...' if len(link_text) > 50 else ''}'\")\n",
        "                print(f\"    URL: {link_url[:80]}{'...' if len(link_url) > 80 else ''}\")\n",
        "\n",
        "            print(f\"\\nüìä Link Classification (first 5):\")\n",
        "            print(f\"   Internal/Relative: {internal_links}\")\n",
        "            print(f\"   External: {external_links}\")\n",
        "            print(f\"   Archive links: {archive_links}\")\n",
        "\n",
        "        else:\n",
        "            print(\"‚ùå No links with href found\")\n",
        "            print(\"üí° This might mean:\")\n",
        "            print(\"   - Page has no links\")\n",
        "            print(\"   - Links use different attributes\")\n",
        "            print(\"   - Page didn't load properly\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error finding links: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VA93z4BHtKdj"
      },
      "source": [
        "**Step 5b: Filter for historical document links**\n",
        "\n",
        "Let's find links that point to historical archives or documents:\n",
        "\n",
        "Copy this code:\n",
        "```python\n",
        "# Define domains that often contain historical documents\n",
        "historical_domains = ['archive.org', 'canadiana.org', 'gutenberg.org', 'biographi.ca']\n",
        "\n",
        "# Filter links\n",
        "document_links = []\n",
        "for link in all_links:\n",
        "    href = link.get('href', '')\n",
        "    # Check if any historical domain is in the URL\n",
        "    for domain in historical_domains:\n",
        "        if domain in href:\n",
        "            document_links.append({\n",
        "                'text': link.get_text().strip(),\n",
        "                'url': href\n",
        "            })\n",
        "            break  # Don't add the same link twice\n",
        "\n",
        "print(f\"Historical document links found: {len(document_links)}\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azwM8ROAtKdj",
        "outputId": "64ded4ec-699f-42f2-d61c-0836a28d5263"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Historical document links found: 1\n"
          ]
        }
      ],
      "source": [
        "# Define domains that often contain historical documents\n",
        "historical_domains = ['archive.org', 'canadiana.org', 'gutenberg.org', 'biographi.ca']\n",
        "\n",
        "# Filter links\n",
        "document_links = []\n",
        "for link in all_links:\n",
        "    href = link.get('href', '')\n",
        "    # Check if any historical domain is in the URL\n",
        "    for domain in historical_domains:\n",
        "        if domain in href:\n",
        "            document_links.append({\n",
        "                'text': link.get_text().strip(),\n",
        "                'url': href\n",
        "            })\n",
        "            break  # Don't add the same link twice\n",
        "\n",
        "print(f\"Historical document links found: {len(document_links)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyMZNO4ktKdj"
      },
      "source": [
        "**Step 5c: Display the historical links**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWXt7tLQtKdj",
        "outputId": "450011f0-cdf5-47e2-f77b-2d2793709b93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Historical Document Links:\n",
            "==================================================\n",
            "1. acknowledged by the British in the 1760s\n",
            "   URL: http://http://www.canadiana.org/view/91943/321\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Show the historical document links we found\n",
        "try:\n",
        "    if 'document_links' not in locals():\n",
        "        print(\"‚ùå Historical document links not available - run the filter cell first\")\n",
        "    elif not document_links:\n",
        "        print(\"‚ÑπÔ∏è No historical document links were identified yet.\")\n",
        "    else:\n",
        "        print(\"Historical Document Links:\")\n",
        "        print(\"=\" * 50)\n",
        "        for i, link in enumerate(document_links, 1):\n",
        "            print(f\"{i}. {link['text']}\")\n",
        "            print(f\"   URL: {link['url']}\")\n",
        "            print()\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error displaying historical document links: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jU3km0TtKdj"
      },
      "source": [
        "### üîÑ **Your Turn: Find PDF Links**\n",
        "\n",
        "Use this blog post: \"https://activehistory.ca/blog/2025/07/30/taking-care-of-the-truth-a-call-for-collaborative-community-engaged-residential-school-research/\"\n",
        "\n",
        "Many historical documents are available as PDFs. Adapt the link-finding code to:\n",
        "\n",
        "1. Find all links that contain \".pdf\" in their href\n",
        "2. Store them in a list called `pdf_links`\n",
        "3. Print how many PDF links you found\n",
        "4. Display the first 3 PDF links\n",
        "\n",
        "Hint: Use `if '.pdf' in href:` to check for PDF links."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1N-CJWLtKdj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOlrCF2wtKdo"
      },
      "source": [
        "## Step 6: Internet Archive Metadata\n",
        "\n",
        "Internet Archive documents have rich metadata. Let's learn to extract it systematically.\n",
        "\n",
        "**Step 6a: Get an Internet Archive page**\n",
        "\n",
        "Let's work with our Saskatchewan document:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1Au04aGtKdo",
        "outputId": "1d295a0c-5f8b-40e3-b1f6-51b1f6d690e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üèõÔ∏è Working with Internet Archive metadata...\n",
            "URL: https://archive.org/details/saskatchewan00sask\n",
            "‚è≥ Waiting 2 second(s) before request...\n",
            "‚úÖ Request successful\n",
            "‚úÖ Status Code: 200\n",
            "‚úÖ Internet Archive page loaded successfully!\n",
            "‚úÖ Confirmed: This is an Internet Archive page\n",
            "üìÑ Content size: 228,042 characters\n",
            "‚úÖ Substantial content received\n"
          ]
        }
      ],
      "source": [
        "# Get Internet Archive page with validation\n",
        "ia_url = \"https://archive.org/details/saskatchewan00sask\"\n",
        "\n",
        "print(\"üèõÔ∏è Working with Internet Archive metadata...\")\n",
        "print(f\"URL: {ia_url}\")\n",
        "\n",
        "try:\n",
        "    # Use our respectful function with a bit longer delay for IA\n",
        "    ia_response = respectful_get(ia_url, delay=2)\n",
        "\n",
        "    if ia_response:\n",
        "        print(f\"‚úÖ Status Code: {ia_response.status_code}\")\n",
        "        print(\"‚úÖ Internet Archive page loaded successfully!\")\n",
        "\n",
        "        # Basic validation\n",
        "        if 'archive.org' in ia_response.url:\n",
        "            print(\"‚úÖ Confirmed: This is an Internet Archive page\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è  Warning: Response URL doesn't match Internet Archive\")\n",
        "\n",
        "        # Check content size\n",
        "        content_size = len(ia_response.text)\n",
        "        print(f\"üìÑ Content size: {content_size:,} characters\")\n",
        "\n",
        "        if content_size > 10000:\n",
        "            print(\"‚úÖ Substantial content received\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è  Warning: Less content than expected\")\n",
        "\n",
        "    else:\n",
        "        print(\"‚ùå Failed to load Internet Archive page\")\n",
        "        print(\"üí° Possible issues:\")\n",
        "        print(\"   - Internet Archive servers busy\")\n",
        "        print(\"   - Network connectivity issues\")\n",
        "        print(\"   - Document no longer available\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading Internet Archive page: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgP4AVIMtKdp"
      },
      "source": [
        "**Step 6b: Extract the document title**\n",
        "\n",
        "Internet Archive puts document titles in `<h1>` tags:\n",
        "\n",
        "Copy this code:\n",
        "```python\n",
        "title = ia_soup.find('h1')\n",
        "if title:\n",
        "    document_title = title.get_text().strip()\n",
        "    print(f\"Document Title: {document_title}\")\n",
        "else:\n",
        "    print(\"No title found\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_dhCfUFtKdp",
        "outputId": "4ef8d8cb-5350-4832-c848-37aa5179bd7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document Title: Saskatchewan.\n"
          ]
        }
      ],
      "source": [
        "title = ia_soup.find('h1')\n",
        "if title:\n",
        "    document_title = title.get_text().strip()\n",
        "    print(f\"Document Title: {document_title}\")\n",
        "else:\n",
        "    print(\"No title found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PITEaildtKdp"
      },
      "source": [
        "**Step 6c: Find metadata fields**\n",
        "\n",
        "Internet Archive uses `<dt>` (definition term) and `<dd>` (definition description) tags for metadata:\n",
        "\n",
        "Copy this code:\n",
        "```python\n",
        "# Find metadata terms and values\n",
        "metadata_terms = ia_soup.find_all('dt')\n",
        "metadata_values = ia_soup.find_all('dd')\n",
        "\n",
        "print(f\"Metadata fields found: {len(metadata_terms)}\")\n",
        "print(f\"Metadata values found: {len(metadata_values)}\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68LKehJWtKdp",
        "outputId": "56c93c83-a0ec-41e9-d609-bfa1da0869d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metadata fields found: 36\n",
            "Metadata values found: 36\n"
          ]
        }
      ],
      "source": [
        "# Find metadata terms and values\n",
        "metadata_terms = ia_soup.find_all('dt')\n",
        "metadata_values = ia_soup.find_all('dd')\n",
        "\n",
        "print(f\"Metadata fields found: {len(metadata_terms)}\")\n",
        "print(f\"Metadata values found: {len(metadata_values)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiawnq17tKdp"
      },
      "source": [
        "**Step 6d: Extract and display metadata**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpDdvSnatKdp",
        "outputId": "f3d41eb3-3233-4093-f7c9-1f767992519f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document Metadata:\n",
            "========================================\n",
            "by: Saskatchewan. Dept. of Agriculture\n",
            "Publication date: 1912\n",
            "Topics: Saskatchewan -- Economic conditions -- History, Saskatchewan -- Agriculture, Saskatchewan -- Resourc...\n",
            "Language: English\n",
            "\n",
            "üîç Additional metadata fields captured: 32\n"
          ]
        }
      ],
      "source": [
        "# Create a dictionary to store metadata\n",
        "try:\n",
        "    if 'metadata_terms' not in locals() or 'metadata_values' not in locals():\n",
        "        print(\"‚ùå Metadata fields not found - run the metadata extraction cell first\")\n",
        "    else:\n",
        "        metadata = {}\n",
        "        for term, value in zip(metadata_terms, metadata_values):\n",
        "            term_text = term.get_text().strip()\n",
        "            value_text = value.get_text().strip()\n",
        "            metadata[term_text] = value_text\n",
        "        if not metadata:\n",
        "            print(\"‚ÑπÔ∏è No metadata pairs were captured - the page structure may have changed\")\n",
        "        else:\n",
        "            print(\"Document Metadata:\")\n",
        "            print(\"=\" * 40)\n",
        "            important_fields = ['by', 'Publication date', 'Topics', 'Language']\n",
        "            for field in important_fields:\n",
        "                if field in metadata:\n",
        "                    value = metadata[field]\n",
        "                    if len(value) > 100:\n",
        "                        value = value[:100] + '...'\n",
        "                    print(f\"{field}: {value}\")\n",
        "            extra_fields = [key for key in metadata if key not in important_fields]\n",
        "            if extra_fields:\n",
        "                print()\n",
        "                print(f\"üîç Additional metadata fields captured: {len(extra_fields)}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error building metadata dictionary: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-j-z3Z5RtKdp"
      },
      "source": [
        "### üîÑ **Your Turn: Create a Metadata Function**\n",
        "\n",
        "Now create a reusable function that extracts metadata from any Internet Archive document. Fill in the missing parts:\n",
        "\n",
        "```python\n",
        "def extract_ia_metadata(url):\n",
        "    \"\"\"Extract title and metadata from an Internet Archive document\"\"\"\n",
        "    # 1. Get the page with requests.get()\n",
        "    # 2. Create soup with BeautifulSoup\n",
        "    # 3. Extract title from h1 tag\n",
        "    # 4. Extract metadata from dt/dd tags\n",
        "    # 5. Return a dictionary with title and metadata\n",
        "```\n",
        "\n",
        "Test it with the University of Toronto annual report: `https://archive.org/details/annualreport191920nivuoft`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZrK0u8BtKdp",
        "outputId": "d44e76ed-45ad-4766-c119-2825e3fd5e72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéì Testing with University of Toronto Annual Report 1919-20...\n",
            "üîç Analyzing Internet Archive URL: https://archive.org/details/annualreport191920nivuoft\n",
            "üìã Item ID extracted: annualreport191920nivuoft\n",
            "‚ö†Ô∏è  IA library not available, falling back to web scraping...\n",
            "‚è≥ Waiting 2 second(s) before request...\n",
            "‚úÖ Request successful\n",
            "‚úÖ Successfully extracted metadata using web scraping\n",
            "\n",
            "üìä Extracted Metadata:\n",
            "==================================================\n",
            "Method: Web Scraping\n",
            "Title: Annual report - University of Saskatchewan\n",
            "Creator: University of Saskatchewan\n",
            "Date: 20080605175845\n",
            "Subject: University of Saskatchewan\n",
            "Language: English\n",
            "\n",
            "üí° This function demonstrates two approaches:\n",
            "   1. Internet Archive Python library (preferred)\n",
            "   2. Web scraping with Beautiful Soup (fallback)\n",
            "   Real research projects should use both for robustness!\n"
          ]
        }
      ],
      "source": [
        "# Create a reusable Internet Archive metadata function\n",
        "def extract_ia_metadata(url):\n",
        "    \"\"\"Extract title and metadata from an Internet Archive document\"\"\"\n",
        "    try:\n",
        "        print(f\"üîç Analyzing Internet Archive URL: {url}\")\n",
        "\n",
        "        # Extract item ID from URL\n",
        "        if '/details/' in url:\n",
        "            item_id = url.split('/details/')[-1]\n",
        "            print(f\"üìã Item ID extracted: {item_id}\")\n",
        "        else:\n",
        "            return {'error': 'Invalid Internet Archive URL format'}\n",
        "\n",
        "        # Method 1: Try Internet Archive Python library (preferred)\n",
        "        try:\n",
        "            import internetarchive as ia\n",
        "            item = ia.get_item(item_id)\n",
        "\n",
        "            # Extract key metadata\n",
        "            metadata = {\n",
        "                'method': 'IA Python Library',\n",
        "                'title': item.metadata.get('title', 'No title'),\n",
        "                'creator': item.metadata.get('creator', 'No creator'),\n",
        "                'date': item.metadata.get('date', 'No date'),\n",
        "                'subject': item.metadata.get('subject', 'No subject'),\n",
        "                'description': item.metadata.get('description', 'No description')[:200] + '...' if item.metadata.get('description') else 'No description',\n",
        "                'language': item.metadata.get('language', 'No language'),\n",
        "                'files_count': len(list(item.files))\n",
        "            }\n",
        "\n",
        "            print(\"‚úÖ Successfully extracted metadata using IA library\")\n",
        "            return metadata\n",
        "\n",
        "        except ImportError:\n",
        "            print(\"‚ö†Ô∏è  IA library not available, falling back to web scraping...\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  IA library failed ({e}), falling back to web scraping...\")\n",
        "\n",
        "        # Method 2: Fallback to web scraping\n",
        "        response = respectful_get(url, delay=2)\n",
        "        if not response:\n",
        "            return {'error': 'Could not download page'}\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Extract metadata from HTML\n",
        "        title_tag = soup.find('h1')\n",
        "        title = title_tag.get_text().strip() if title_tag else 'No title found'\n",
        "\n",
        "        # Try to find metadata fields\n",
        "        metadata_dict = {'method': 'Web Scraping', 'title': title}\n",
        "\n",
        "        # Look for dt/dd metadata pairs\n",
        "        dt_tags = soup.find_all('dt')\n",
        "        dd_tags = soup.find_all('dd')\n",
        "\n",
        "        for dt, dd in zip(dt_tags, dd_tags):\n",
        "            key = dt.get_text().strip().lower()\n",
        "            value = dd.get_text().strip()\n",
        "\n",
        "            # Map common fields\n",
        "            if 'by' in key or 'creator' in key:\n",
        "                metadata_dict['creator'] = value\n",
        "            elif 'date' in key:\n",
        "                metadata_dict['date'] = value\n",
        "            elif 'topic' in key or 'subject' in key:\n",
        "                metadata_dict['subject'] = value\n",
        "            elif 'language' in key:\n",
        "                metadata_dict['language'] = value\n",
        "\n",
        "        print(\"‚úÖ Successfully extracted metadata using web scraping\")\n",
        "        return metadata_dict\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error extracting metadata: {e}\")\n",
        "        return {'error': str(e)}\n",
        "\n",
        "# Test the function with University of Toronto annual report\n",
        "uoft_url = \"https://archive.org/details/annualreport191920nivuoft\"\n",
        "print(\"üéì Testing with University of Toronto Annual Report 1919-20...\")\n",
        "\n",
        "result = extract_ia_metadata(uoft_url)\n",
        "\n",
        "print(f\"\\nüìä Extracted Metadata:\")\n",
        "print(\"=\" * 50)\n",
        "for key, value in result.items():\n",
        "    print(f\"{key.title()}: {value}\")\n",
        "\n",
        "print(f\"\\nüí° This function demonstrates two approaches:\")\n",
        "print(f\"   1. Internet Archive Python library (preferred)\")\n",
        "print(f\"   2. Web scraping with Beautiful Soup (fallback)\")\n",
        "print(f\"   Real research projects should use both for robustness!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRyhzTwgtKdp"
      },
      "source": [
        "## Step 7: Final Challenge - Your Research Project\n",
        "\n",
        "Time to put it all together! Choose a Canadian historical source and conduct your own analysis.\n",
        "\n",
        "**Available sources:**\n",
        "- Encyclopedia Rupert's Land purchase article: Research the historical quotes and themes\n",
        "- Saskatchewan History: Analyze the metadata and publication context\n",
        "- UofT Annual Report 1919-20: Extract institutional information\n",
        "\n",
        "**Your research steps:**\n",
        "1. Choose a source and research question\n",
        "2. Use the scraping techniques you've learned\n",
        "3. Extract specific information related to your question\n",
        "4. Present your findings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Internet Archive library - much more efficient!\n",
        "print(\"üèõÔ∏è Accessing Internet Archive with the Python library...\")\n",
        "\n",
        "try:\n",
        "    import internetarchive as ia\n",
        "except ImportError:\n",
        "    print(\"‚ùå Internet Archive library not installed yet - run the installation cell below first\")\n",
        "else:\n",
        "    try:\n",
        "        item_id = \"saskatchewan00sask\"  # The ID from the URL\n",
        "        item = ia.get_item(item_id)\n",
        "        print(f\"‚úÖ Successfully retrieved item: {item_id}\")\n",
        "        print(f\"üìö Title: {item.metadata.get('title', 'No title')}\")\n",
        "        print(f\"üìÖ Date: {item.metadata.get('date', 'No date')}\")\n",
        "        print(f\"üë§ Creator: {item.metadata.get('creator', 'No creator')}\")\n",
        "        print(f\"üìñ Subject: {item.metadata.get('subject', 'No subject')}\")\n",
        "        files = list(item.files)\n",
        "        print()\n",
        "        print(f\"üìÅ Available files: {len(files)}\")\n",
        "        for i, file in enumerate(files[:5]):\n",
        "            file_name = file.get('name', 'Unknown')\n",
        "            file_format = file.get('format', 'Unknown')\n",
        "            file_size = file.get('size', 'Unknown')\n",
        "            print(f\"   {i+1}. {file_name} ({file_format}) - {file_size} bytes\")\n",
        "        if len(files) > 5:\n",
        "            print(f\"   ... and {len(files) - 5} more files\")\n",
        "        print()\n",
        "        print(\"üöÄ Much easier than HTML scraping!\")\n",
        "        print(\"üí° The IA library gives us clean, structured data instantly\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error accessing Internet Archive: {e}\")\n",
        "        print(\"üí° This might be due to network issues or an unavailable item\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaS8JKeztKdp",
        "outputId": "cca02052-ca44-4448-dcc6-65ba2cbdaead"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üèõÔ∏è Accessing Internet Archive with the Python library...\n",
            "‚ùå Internet Archive library not installed yet - run the installation cell below first\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install and import Internet Archive library\n",
        "# First install the library (run this once)\n",
        "!pip install internetarchive --quiet\n",
        "\n",
        "print(\"üìö Installing Internet Archive Python library...\")\n",
        "print(\"‚úÖ Installation complete!\")\n",
        "\n",
        "# Import the library\n",
        "try:\n",
        "    import internetarchive as ia\n",
        "    print(\"‚úÖ Internet Archive library imported successfully!\")\n",
        "    print(\"üîó This library provides direct, efficient access to IA collections\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Error importing Internet Archive library: {e}\")\n",
        "    print(\"üí° Try running: pip install internetarchive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZESwWu-5tKdq",
        "outputId": "22ba8ffb-9d8c-4a6f-e479-243f264787d1"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/108.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hüìö Installing Internet Archive Python library...\n",
            "‚úÖ Installation complete!\n",
            "‚úÖ Internet Archive library imported successfully!\n",
            "üîó This library provides direct, efficient access to IA collections\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Introduction to Internet Archive Python Library\n",
        "\n",
        "While Beautiful Soup is excellent for scraping web pages, the Internet Archive provides a specialized Python library that makes accessing their collections much more efficient. This is perfect for large-scale historical research projects.\n",
        "\n",
        "**Why use the Internet Archive library?**\n",
        "- Direct access to metadata without scraping HTML\n",
        "- Faster downloads and better error handling\n",
        "- Access to full-text search capabilities\n",
        "- Bulk processing of large collections\n",
        "- Respects Internet Archive's preferred access methods"
      ],
      "metadata": {
        "id": "wMHcKPZptKdq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqFMR2KNtKdq",
        "outputId": "b0f9a6e7-13a1-413f-ae5c-7d1e75002ede"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üî¨ Historical Web Scraping Research Project\n",
            "==================================================\n",
            "üìö Available sources:\n",
            "1. Rupert's Land Purchase (Encyclopedia of Saskatchewan)\n",
            "   Type: Online encyclopedia article\n",
            "   URL: https://esask.uregina.ca/entry/ruperts_land_purchase.html...\n",
            "   Sample questions:\n",
            "     - How is the Rupert's Land purchase narrated for a general audience?\n",
            "     - What Indigenous perspectives are included or missing?\n",
            "     - Which archival collections are linked for further research?\n",
            "2. Saskatchewan History\n",
            "   Type: Internet Archive document\n",
            "   URL: https://archive.org/details/saskatchewan00sask...\n",
            "   Sample questions:\n",
            "     - What metadata is available about publication?\n",
            "     - What file formats are provided?\n",
            "     - Who was the creator/publisher?\n",
            "3. UofT Annual Report 1919-20\n",
            "   Type: Institutional document\n",
            "   URL: https://archive.org/details/annualreport191920nivuoft...\n",
            "   Sample questions:\n",
            "     - What institutional information is captured?\n",
            "     - How is the document structured?\n",
            "     - What historical context does it provide?\n",
            "üéØ Your research steps:\n",
            "1. Choose a source and research question\n",
            "2. Use appropriate scraping techniques\n",
            "3. Extract and validate data\n",
            "4. Analyze and present findings\n",
            "üìã Example Research Project:\n",
            "Source: Rupert's Land Purchase (Encyclopedia of Saskatchewan)\n",
            "Question: What historical themes appear in the Rupert's Land purchase article?\n",
            "URL: https://esask.uregina.ca/entry/ruperts_land_purchase.html\n",
            "üîç Conducting research...\n",
            "‚è≥ Waiting 1 second(s) before request...\n",
            "‚úÖ Request successful\n",
            "‚úÖ Page successfully scraped\n",
            "üìä Content Analysis:\n",
            "   Paragraphs found: 4\n",
            "üé≠ Historical Themes Found:\n",
            "   Geographic: 18 mentions\n",
            "   Colonial administration: 9 mentions\n",
            "   Temporal: 5 mentions\n",
            "   Economic: 2 mentions\n",
            "   Indigenous perspectives: 2 mentions\n",
            "üìù Research Findings:\n",
            "1. Successfully scraped a Canadian historical source\n",
            "2. Identified multiple historical themes in source material\n",
            "3. Demonstrated effective web scraping for historical research\n",
            "4. Most prominent theme: Geographic (18 mentions)\n",
            "üöÄ Now it's your turn!\n",
            "Customize this code for your own research question:\n",
            "1. Change 'chosen_source' to your preferred source\n",
            "2. Modify 'my_question' to your research interest\n",
            "3. Adapt the analysis code for your specific question\n",
            "4. Add your own themes, keywords, or analysis methods\n",
            "üí° Remember to:\n",
            "- Validate your scraped data\n",
            "- Handle errors gracefully\n",
            "- Respect website terms of service\n",
            "- Cite your digital sources properly\n"
          ]
        }
      ],
      "source": [
        "# Final Challenge: Your Historical Research Project\n",
        "\n",
        "# Available Canadian historical sources:\n",
        "sources = {\n",
        "    \"Rupert's Land Purchase (Encyclopedia of Saskatchewan)\": {\n",
        "        \"url\": \"https://esask.uregina.ca/entry/ruperts_land_purchase.html\",\n",
        "        \"type\": \"Online encyclopedia article\",\n",
        "        \"research_questions\": [\n",
        "            \"How is the Rupert's Land purchase narrated for a general audience?\",\n",
        "            \"What Indigenous perspectives are included or missing?\",\n",
        "            \"Which archival collections are linked for further research?\"\n",
        "        ]\n",
        "    },\n",
        "    \"Saskatchewan History\": {\n",
        "        \"url\": \"https://archive.org/details/saskatchewan00sask\",\n",
        "        \"type\": \"Internet Archive document\",\n",
        "        \"research_questions\": [\n",
        "            \"What metadata is available about publication?\",\n",
        "            \"What file formats are provided?\",\n",
        "            \"Who was the creator/publisher?\"\n",
        "        ]\n",
        "    },\n",
        "    \"UofT Annual Report 1919-20\": {\n",
        "        \"url\": \"https://archive.org/details/annualreport191920nivuoft\",\n",
        "        \"type\": \"Institutional document\",\n",
        "        \"research_questions\": [\n",
        "            \"What institutional information is captured?\",\n",
        "            \"How is the document structured?\",\n",
        "            \"What historical context does it provide?\"\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Choose your research project\n",
        "print(\"üî¨ Historical Web Scraping Research Project\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(\"üìö Available sources:\")\n",
        "for i, (name, info) in enumerate(sources.items(), 1):\n",
        "    print(f\"{i}. {name}\")\n",
        "    print(f\"   Type: {info['type']}\")\n",
        "    print(f\"   URL: {info['url'][:60]}...\")\n",
        "    print(f\"   Sample questions:\")\n",
        "    for q in info['research_questions']:\n",
        "        print(f\"     - {q}\")\n",
        "\n",
        "print(f\"üéØ Your research steps:\")\n",
        "print(\"1. Choose a source and research question\")\n",
        "print(\"2. Use appropriate scraping techniques\")\n",
        "print(\"3. Extract and validate data\")\n",
        "print(\"4. Analyze and present findings\")\n",
        "\n",
        "# Example research project - customize this!\n",
        "chosen_source = \"Rupert's Land Purchase (Encyclopedia of Saskatchewan)\"  # Change this to your choice\n",
        "my_url = sources[chosen_source][\"url\"]\n",
        "my_question = \"What historical themes appear in the Rupert's Land purchase article?\"\n",
        "\n",
        "print(f\"üìã Example Research Project:\")\n",
        "print(f\"Source: {chosen_source}\")\n",
        "print(f\"Question: {my_question}\")\n",
        "print(f\"URL: {my_url}\")\n",
        "\n",
        "# Conduct the research\n",
        "print(f\"üîç Conducting research...\")\n",
        "\n",
        "theme_counts = {}\n",
        "\n",
        "try:\n",
        "    response = respectful_get(my_url)\n",
        "    if response:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        print(\"‚úÖ Page successfully scraped\")\n",
        "        if \"themes\" in my_question.lower():\n",
        "            paragraphs = soup.find_all('p')\n",
        "            print(f\"üìä Content Analysis:\")\n",
        "            print(f\"   Paragraphs found: {len(paragraphs)}\")\n",
        "            all_text = soup.get_text().lower()\n",
        "            themes = {\n",
        "                'Economic': ['trade', 'commerce', 'company', 'profit'],\n",
        "                'Indigenous perspectives': ['indigenous', 'cree', 'assiniboine', 'd√©n√©', 'nation'],\n",
        "                'Colonial administration': ['british', 'government', 'hudson', 'london'],\n",
        "                'Geographic': ['rupert', 'saskatchewan', 'manitoba', 'prairies', 'north-west'],\n",
        "                'Temporal': ['1869', '1870', '19th century', 'confederation']\n",
        "            }\n",
        "            for theme_name, keywords in themes.items():\n",
        "                count = sum(all_text.count(keyword) for keyword in keywords)\n",
        "                if count > 0:\n",
        "                    theme_counts[theme_name] = count\n",
        "            if theme_counts:\n",
        "                print(f\"üé≠ Historical Themes Found:\")\n",
        "                for theme, count in sorted(theme_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "                    print(f\"   {theme}: {count} mentions\")\n",
        "            else:\n",
        "                print(\"‚ÑπÔ∏è No theme keywords were detected - try adjusting the keyword lists\")\n",
        "        else:\n",
        "            print(\"‚ÑπÔ∏è Customize the analysis section to match your research question.\")\n",
        "        print(f\"üìù Research Findings:\")\n",
        "        print(\"1. Successfully scraped a Canadian historical source\")\n",
        "        print(\"2. Identified multiple historical themes in source material\" if theme_counts else \"2. Ready to adapt the analysis for your own question\")\n",
        "        print(\"3. Demonstrated effective web scraping for historical research\")\n",
        "        if theme_counts:\n",
        "            most_common = max(theme_counts.items(), key=lambda x: x[1])\n",
        "            print(f\"4. Most prominent theme: {most_common[0]} ({most_common[1]} mentions)\")\n",
        "    else:\n",
        "        print(\"‚ùå Could not complete research - page unavailable\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Research error: {e}\")\n",
        "\n",
        "print(f\"üöÄ Now it's your turn!\")\n",
        "print(\"Customize this code for your own research question:\")\n",
        "print(\"1. Change 'chosen_source' to your preferred source\")\n",
        "print(\"2. Modify 'my_question' to your research interest\")\n",
        "print(\"3. Adapt the analysis code for your specific question\")\n",
        "print(\"4. Add your own themes, keywords, or analysis methods\")\n",
        "\n",
        "print(f\"üí° Remember to:\")\n",
        "print(\"- Validate your scraped data\")\n",
        "print(\"- Handle errors gracefully\")\n",
        "print(\"- Respect website terms of service\")\n",
        "print(\"- Cite your digital sources properly\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PIJs3KgtKdq"
      },
      "source": [
        "## Summary: What You've Learned\n",
        "\n",
        "üéâ **Congratulations!** You've mastered the fundamentals of web scraping for historical research:\n",
        "\n",
        "**Technical Skills:**\n",
        "- ‚úÖ Making web requests with `requests.get()`\n",
        "- ‚úÖ Parsing HTML with Beautiful Soup\n",
        "- ‚úÖ Targeting specific elements (`find`, `find_all`)\n",
        "- ‚úÖ Extracting text, links, and metadata\n",
        "- ‚úÖ Building reusable functions for research\n",
        "\n",
        "**Historical Sources:**\n",
        "- ‚úÖ Blog posts with embedded historical content\n",
        "- ‚úÖ Internet Archive documents with metadata\n",
        "- ‚úÖ Academic indexes and structured data\n",
        "\n",
        "**Research Methods:**\n",
        "- ‚úÖ Systematic content extraction\n",
        "- ‚úÖ Metadata analysis for document context\n",
        "- ‚úÖ Building reproducible research workflows\n",
        "\n",
        "**Next Steps:**\n",
        "In Notebook 3, you'll learn advanced text analysis techniques to find patterns in the historical data you've scraped. We'll also explore the Internet Archive Python library for more efficient access to large collections.\n",
        "\n",
        "**Remember:**\n",
        "- Always respect robots.txt and website terms of service\n",
        "- Cite your digital sources properly\n",
        "- Consider the limitations and context of digitized materials\n",
        "- Use these skills responsibly for legitimate research purposes"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}